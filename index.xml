<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Desh Raj on Desh Raj</title>
    <link>https://desh2608.github.io/</link>
    <description>Recent content in Desh Raj on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 03 Jul 2018 00:00:00 +0530</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Waldo: A system for optical character recognition</title>
      <link>https://desh2608.github.io/project/waldo-ocr/</link>
      <pubDate>Tue, 03 Jul 2018 16:56:46 +0530</pubDate>
      
      <guid>https://desh2608.github.io/project/waldo-ocr/</guid>
      <description>&lt;p&gt;It is an ongoing project under Prof. Daniel Povey to develop an Optical Character Recognition system that is robust on focused as well as incidental text. My contributions are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Experimenting with the ICDAR 2015 Robust Reading Challenge dataset by modifying training script.&lt;/li&gt;
&lt;li&gt;A visualization and compression module for segmentation mask overlayed on images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The system consists of a modified UNet first proposed in &lt;a href=&#34;https://arxiv.org/abs/1505.04597&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Speech Recognition using WFSTs</title>
      <link>https://desh2608.github.io/post/intro-speech-recognition-wfst/</link>
      <pubDate>Mon, 23 Apr 2018 13:41:31 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-speech-recognition-wfst/</guid>
      <description>

&lt;p&gt;Until now, all of my blog posts have been about deep learning methods or their application to NLP. Since the last couple of weeks, however, I have started learning about Automatic Speech Recognition (ASR)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Therefore, I will also include speech-related articles in this publication now.&lt;/p&gt;

&lt;p&gt;The ASR logic is very simple (it’s just Bayes rule, like most other things in machine learning). Essentially, given a speech waveform, the objective is to transcribe it, i.e., identify a text which aligns with the waveform. Suppose $Y$ represents the feature vectors obtained from the waveform (Note: this “feature extraction” itself is an involved procedure, and I will describe it in detail in another post), and $\mathbf{w}$ denotes an arbitrary string of words. Then, we have the following.&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathbf{w}} = \text{arg}\max_{\mathbf{w}} { P(\mathbf{w}|Y)} = \text{arg} \max_{\mathbf{w}} {P(Y|\mathbf{w})P(\mathbf{w}) } $$&lt;/p&gt;

&lt;p&gt;The two likelihoods in the term are trained separately. The first component, known as &lt;em&gt;acoustic modeling&lt;/em&gt;, is trained using a parallel corpus of utterances and speech waveforms. The second component, called &lt;em&gt;language modeling&lt;/em&gt;, is trained in an unsupervised fashion from a large corpus of text.&lt;/p&gt;

&lt;p&gt;Although the ASR training appears simple from this abstract level, the implementation is arguably more complex, and is usually done using Weighted Finite State Transducers (WFSTs). In this post, I’ll describe WFSTs, some of their basic algorithms, and give a brief introduction to how they are used for speech recognition.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;weighted-finite-state-transducers-wfsts&#34;&gt;Weighted Finite State Transducers (WFSTs)&lt;/h4&gt;

&lt;p&gt;If you have taken any Theory of Computation course before, you’d probably already be aware what an &lt;em&gt;automata&lt;/em&gt; is. Essentially, a finite automaton accepts a language (which is a set of strings). They are represented by directed graphs as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/17/dag.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each such automaton has a start state, one or more final states, and labeled edges connecting the states. A string is accepted if it ends in a final state after traversing through some path in the graph. For instance in the above DFA (deterministic finite automata), &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;ac&lt;/em&gt;, and &lt;em&gt;ae&lt;/em&gt; are allowed.&lt;/p&gt;

&lt;p&gt;So an &lt;em&gt;acceptor&lt;/em&gt; maps any input string to a binary class {0,1} depending on whether or not the string is accepted. A &lt;em&gt;transducer&lt;/em&gt;, on the other hand, has 2 labels on each edge — an input label, and an output label. Furthermore, a &lt;em&gt;weighted&lt;/em&gt; finite state transducer has weights corresponding to each edge and every final state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/17/wfst.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, a WFST is a mapping from a pair of strings to a weight sum. The pair is formed from the input/output labels along any path of the WFST. For pairs which are not possible in the graph, the corresponding weight is infinite.&lt;/p&gt;

&lt;p&gt;In practice, there are libraries available in every language to implement WFSTs. For C++, &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/WebHome&#34; target=&#34;_blank&#34;&gt;OpenFST&lt;/a&gt; is a popular library, which is also used in the &lt;a href=&#34;http://kaldi-asr.org/&#34; target=&#34;_blank&#34;&gt;Kaldi speech recognition toolkit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In principle, it is possible to implement speech recognition algorithms without using WFSTs. However, these data structures have &lt;a href=&#34;https://cs.nyu.edu/~mohri/pub/csl01.pdf&#34; target=&#34;_blank&#34;&gt;several proven results&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and algorithms which can directly be used in ASRs without having to worry about correctness and complexity. These advantages have made WFSTs almost omniscient in speech recognition. I’ll now summarize some algorithms on WFSTs.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;some-basic-algorithms-on-wfsts&#34;&gt;Some basic algorithms on WFSTs&lt;/h3&gt;

&lt;h4 id=&#34;composition&#34;&gt;Composition&lt;/h4&gt;

&lt;p&gt;Composition, as the name suggests, refers to the process of combining 2 WFSTs to form a single WFST. If we have transducers for pronunciation and word-level grammar, such an algorithm would enable us to form a phone-to-word level system easily.&lt;/p&gt;

&lt;p&gt;Composition is done using 3 rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initial state in the new WFST are formed by combining the initial states of the old WFSTs into pairs.&lt;/li&gt;
&lt;li&gt;Similarly, final states are combined into pairs.&lt;/li&gt;
&lt;li&gt;For every pair of edges such that the o-label of the first WFST is the i-label of the second, we add an edge from the source pair to the destination pair. The edge weight is summed using the sum rules.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An example of composition is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/17/composition.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, it may be important to define what &amp;ldquo;sum&amp;rdquo; means for edge weights. Formally, the &amp;ldquo;languages&amp;rdquo; accepted by WFSTs are generalized through the notion of &lt;a href=&#34;https://en.wikipedia.org/wiki/Semiring&#34; target=&#34;_blank&#34;&gt;semirings&lt;/a&gt;. Basically, it is a set of elements with 2 operators, namely $\oplus$ and $\otimes$. Depending on the type of semiring, these operators can take on different definitions. For example, in a tropical semiring, $\oplus$ denotes &lt;strong&gt;$\min$&lt;/strong&gt;, and $\otimes$ denotes &lt;strong&gt;sum&lt;/strong&gt;. Furthermore, in any WFST, weights are $\otimes$-multiplied along paths (Note: here “multiplied” would mean summed for a tropical semiring) and $\oplus$-summed over paths with identical symbol sequence.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/ComposeDoc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for OpenFST implementation of composition.&lt;/p&gt;

&lt;h4 id=&#34;determinization&#34;&gt;Determinization&lt;/h4&gt;

&lt;p&gt;A deterministic automaton is one in which there is only one transition for each label in every state. By such a formulation, a deterministic WFST removes all redundancy and greatly reduces the complexity of the underlying grammar. But, are all WFSTs determinizable?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The Twins Property:&lt;/em&gt; Let us consider an automaton A. Two states &lt;em&gt;p&lt;/em&gt; and &lt;em&gt;q&lt;/em&gt; in A are said to be siblings if both can be reached by string &lt;em&gt;x&lt;/em&gt; and both have cycles with label &lt;em&gt;y&lt;/em&gt;. Essentially, siblings are twins if the total weight for the paths until the states, as well as that including the cycle, are equal for both.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A WFST is determinizable if all its siblings are twins.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is an example of what I said earlier regarding WFSTs being an efficient implementation of the algorithms used in ASR. There are several methods to determinize a WFST. One such algorithm is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/17/determinization.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In simpler steps, this algorithm does the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;At each state, for every outgoing label, if there are multiple outgoing edges for that label, replace them with a single edge with weight as the $\otimes$-sum of all edge weights containing that label.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since this is a local algorithm, it can be efficiently implemented in-memory. To see how to perform determinization in OpenFST, see &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/DeterminizeDoc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;minimization&#34;&gt;Minimization&lt;/h4&gt;

&lt;p&gt;Although minimization is not as essential as determinization, it is still a nice optimization technique. It refers to minimizing the number of states and transitions in a deterministic WFST.&lt;/p&gt;

&lt;p&gt;Minimization is carried out in 2 steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Weight pushing: All weights are pushed towards the start state. See the following example.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/17/pushing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;After this is done, we combine those states which have identical paths to any final state. For example in the above WFST, states 1 and 2 have become identical after weight pushing, so they are combined into one state.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In OpenFST, the implementation details for minimization can be found &lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/MinimizeDoc&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; shows the complete pipeline for a WFST reduction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/17/pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;wfsts-in-speech-recognition&#34;&gt;WFSTs in speech recognition&lt;/h4&gt;

&lt;p&gt;Several WFSTs are composed in sequence for use in speech recognition. These are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Grammar (&lt;strong&gt;G&lt;/strong&gt;): This is the language model trained on large text corpus.&lt;/li&gt;
&lt;li&gt;Lexicon (&lt;strong&gt;L&lt;/strong&gt;): This encodes information about the likelihood of phones without context.&lt;/li&gt;
&lt;li&gt;Context-dependent phonetics (&lt;strong&gt;C&lt;/strong&gt; ): This is similar to n-gram language modeling, except that it is for phones.&lt;/li&gt;
&lt;li&gt;HMM structure (&lt;strong&gt;H&lt;/strong&gt;): This is the model for the waveform.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In general, the composed transducer &lt;strong&gt;H&lt;/strong&gt;o&lt;strong&gt;C&lt;/strong&gt;o&lt;strong&gt;L&lt;/strong&gt;o&lt;strong&gt;G&lt;/strong&gt; represents the entire pipeline of speech recognition. Each of the components can individually be improved, so that the entire ASR system gets improved.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;This was just a brief introduction to WFSTs which are an important component in ASR systems. In further posts on speech, I hope to discuss things such as feature extraction, popular GMM-HMM models, and latest deep learning advances. I am also reading papers mentioned &lt;a href=&#34;http://jrmeyer.github.io/asr/2017/04/05/seminal-asr-papers.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to get a good overview of how ASR has progressed over the years.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Gales, Mark, and Steve Young. &amp;ldquo;The application of hidden Markov models in speech recognition.&amp;rdquo; Foundations and Trends in Signal Processing 1.3 (2008): 195–304.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Mohri, Mehryar, Fernando Pereira, and Michael Riley. &amp;ldquo;Weighted finite-state transducers in speech recognition.&amp;rdquo; Computer Speech &amp;amp; Language 16.1 (2002): 69–88.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;&lt;a href=&#34;https://wiki.eecs.yorku.ca/course_archive/2011-12/W/6328/_media/wfst-tutorial.pdf&#34; target=&#34;_blank&#34;&gt;Lecture slides&lt;/a&gt; from Prof. Hui Jiang (York University)
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to Obtain Sentence Vectors</title>
      <link>https://desh2608.github.io/post/how-to-obtain-sentence-vectors/</link>
      <pubDate>Thu, 12 Apr 2018 13:41:14 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/how-to-obtain-sentence-vectors/</guid>
      <description>

&lt;p&gt;In several of my previous posts, I have discussed methods for obtaining word embeddings, such as SVD, word2vec, or GloVe. In this post, I will abstract a level higher and talk about 4 different methods that have been proposed to get embeddings for sentences.&lt;/p&gt;

&lt;p&gt;But first, some of you may ask why do we even need a different method for obtaining sentence vectors. Since sentences are essentially made up of words, it may be reasonable to argue that simply taking the sum or the average of the constituent word vectors should give a decent sentence representation. This is akin to a bag-of-words representation, and hence suffers from the same limitations, i.e.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It ignores the order of words in the sentence.&lt;/li&gt;
&lt;li&gt;It ignores the sentence semantics completely.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Other word vector based approaches are also similarly constrained. For instance, a weighted average technique again loses word order within the sentence. To remedy this issue, &lt;a href=&#34;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&#34; target=&#34;_blank&#34;&gt;Socher et al.&lt;/a&gt; combined the words in the order given by the parse tree of the sentence. While this technique may be suitable for complete sentences, it does not work for phrases or paragraphs.&lt;/p&gt;

&lt;p&gt;In an earlier &lt;a href=&#34;https://desh2608.github.io/post/last-3-years-in-text-classification/&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;, I discussed several ways in which sentence representations are obtained as an intermediate step during text classification. Several approaches are used for this purpose, such as character to sentence level feature encoding, parse trees, regional (two-view) embeddings, and so on. However, the limitation with such an &amp;ldquo;intermediate&amp;rdquo; representation is that the vectors obtained are not generic in that they are closely tied to the classification objective. As such, vectors obtained through training on one objective may not be extrapolated for other tasks.&lt;/p&gt;

&lt;p&gt;In light of this discussion, I will now describe 4 recent methods that have been proposed to obtain general sentence vectors. Note that each of these belongs to either of 2 categories: (i) inter-sentence, wherein the vector of one sentence depends on its surrounding sentences, and (ii) intra-sentence, where a sentence vector only depends on that particular sentence in isolation.&lt;/p&gt;

&lt;h4 id=&#34;paragraph-vectors&#34;&gt;Paragraph Vectors&lt;/h4&gt;

&lt;p&gt;In this &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v32/le14.pdf&#34; target=&#34;_blank&#34;&gt;ICML’14 paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; from Mikolov (who also invented &lt;em&gt;word2vec&lt;/em&gt;), the authors propose the following solution: a sentence vector can be learned simply by assigning an index to each sentence, and then treating the index like any other word. This is shown in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/16/doc2vec.png&#34; alt=&#34;Paragraph vectors model. Figure taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Essentially, every paragraph (or sentence) is mapped to a unique vector, and the combined paragraph and word vectors are used to predict the next word. Through such a training, the paragraph vectors may start storing missing information, thus acting like a memory for the paragraph. For this reason, this method is called the Distributed Memory model (PV-DM).&lt;/p&gt;

&lt;p&gt;To obtain the embeddings for an unknown sentence, an inference step needs to be performed. A new column of randomly initialized values is added to the sentence embedding matrix. The inference step is performed keeping all the other parameters fixed to obtain the required vector.&lt;/p&gt;

&lt;p&gt;The PV-DM model requires a large amount of storage space since the paragraph vectors are concatenated with all the vectors in the context window at every training step. To solve this, the authors propose another model, called the Distributed BOW (PV-DBOW), which predicts random words in the context window. The downside is that this model does not use word order, and hence performs worse than PV-DM.&lt;/p&gt;

&lt;h4 id=&#34;skip-thoughts&#34;&gt;Skip-thoughts&lt;/h4&gt;

&lt;p&gt;While PV was an intra-sentence model, &lt;a href=&#34;https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf&#34; target=&#34;_blank&#34;&gt;skip-thoughts&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; is inter-sentence. The method uses continuity of text to predict the next sentence from the given sentence. This also solves the problem of the inference step that is present in the PV model. If you have read about the skip-gram algorithm in word2vec, skip-thoughts is essentially the same technique abstracted to the sentence level.&lt;/p&gt;

&lt;p&gt;In the paper, the authors propose an encoder-decoder framework for training, with an RNN used for both encoding and decoding. In addition to a sentence embedding matrix, this method also generates vectors for the words in the corpus vocabulary. Finally, the objective function to be maximized is as follows.&lt;/p&gt;

&lt;p&gt;$$ \sum_t \log P(w_{i+1}^t|w_{i+1}^{&amp;lt; t},\mathbf{h}_i) + \sum_t \log P(w_{i-1}^t|w_{i-1}^{&amp;lt; t},\mathbf{h}_i) $$&lt;/p&gt;

&lt;p&gt;Here, the indices $i+1$ and $i-1$ represent the next sentence and the previous sentence, respectively. Overall, the function represents the sum of log probabilities of correctly predicting the next sentence and the previous sentence, given the current sentence.&lt;/p&gt;

&lt;p&gt;Since word vectors are also precited at training time, a problem may arise at the time of inference if the new sentence contains an OOV word. To solve this, the authors present a simple solution for vocabulary expansion. We assume that any word, even if it is OOV, will definitely come from some vector space (say w2v), such that we have its vector representation in that space. As such, every known word has 2 representations, one in the RNN space and another in the w2v space. We can then identify a linear transformation matrix that transforms w2v space vectors into RNN space vectors, and this matrix may be used to obtain the RNN vectors for OOV words.&lt;/p&gt;

&lt;h4 id=&#34;fastsent&#34;&gt;FastSent&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.03483.pdf&#34; target=&#34;_blank&#34;&gt;This model&lt;/a&gt;, proposed by Kyunghun Cho&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, is also an inter-sentence technique, and is conceptually very similar to skip-thoughts. The only difference is that it uses a BOW representation of the sentence to predict the surrounding sentences, which makes it computationally much more efficient than skip-thoughts. The training hypothesis remains the same, i.e., rich sentence semantics can be inferred from the content of adjacent sentences. Since the details of the method are same as skip-thoughts, I will not repeat them here to avoid redundancy.&lt;/p&gt;

&lt;h4 id=&#34;sequential-denoising-autoencoders-sdae&#34;&gt;Sequential Denoising Autoencoders (SDAE)&lt;/h4&gt;

&lt;p&gt;This technique was also proposed in the &lt;a href=&#34;https://arxiv.org/pdf/1602.03483.pdf&#34; target=&#34;_blank&#34;&gt;same paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; as FastSent. However, it is essentially an intra-sentence method wherein the objective is to regenerate a sentence from a noisy version.&lt;/p&gt;

&lt;p&gt;In essence, in an SDAE, a high-dimensional input data is corrupted according to some noise function and the model is trained to recover the original data from the corrputed version.&lt;/p&gt;

&lt;p&gt;In the paper, the noise function $N$ uses 2 parameters as follows.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;For each word $w$ in the sentence $S$, $N$ deletes it according to some probability $p_0$.&lt;/li&gt;
&lt;li&gt;For each non-overlapping bigram in $S$, $N$ swaps the bigram tokens with probability $p_x$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These are inspired from the “word dropout” and “debagging” approaches, respectively, which have earlier been studied in some detail.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In the last paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, the authors have performed detailed empirical evaluations of several sentence vector methods, including all of the above. From this analysis, the following observations can be drawn,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Task-dependency:&lt;/strong&gt; Although the methods intend to produce general sentence representations which work well across different tasks, it is found that some methods are more suitable from some tasks due to the inherent algorithm. For instance, skip-thoughts perform well on textual entailment tasks, whereas SDAEs perform much better on paraphrase detection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inter vs. intra:&lt;/strong&gt; The inter-sentence models generate similar vectors in that their nearest neighbors are those sentences which have shared concepts. In contrast, for the intra-sentence models, these are sentences which have more overlapping words.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dependency on word order:&lt;/strong&gt; Although the widely held view is that word order is critical for sentence vectors, the average score for models which are sensitive to word order was found to be almost equal to those which are not. It was even lower for RNN models in unsupervised objectives, which is indeed surprising. One explanation for this may be that the sentences in the dataset, or the evaluation techniques, are not robust enough so as to sufficiently challenge simple word frequency based techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Le, Quoc, and Tomas Mikolov. “Distributed representations of sentences and documents.” International Conference on Machine Learning. 2014.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Kiros, Ryan, et al. “Skip-thought vectors.” Advances in neural information processing systems. 2015.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Hill, Felix, Kyunghyun Cho, and Anna Korhonen. “Learning distributed representations of sentences from unlabelled data.” arXiv preprint arXiv:1602.03483 (2016).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Irony detection in tweets</title>
      <link>https://desh2608.github.io/project/irony-tweet/</link>
      <pubDate>Tue, 20 Mar 2018 17:00:16 +0530</pubDate>
      
      <guid>https://desh2608.github.io/project/irony-tweet/</guid>
      <description>&lt;p&gt;The task was to recognize whether a tweet has irony or not - binary classification. In essence, we identified 2 aspects that were essential to identify irony in tweets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Semantic interaction between text and hashtags, modeled using &lt;a href=&#34;https://arxiv.org/pdf/1510.04935.pdf&#34; target=&#34;_blank&#34;&gt;holographic embeddings&lt;/a&gt; (or circular cross-correlations).&lt;/li&gt;
&lt;li&gt;World knowledge about irony in text, obtained through transfer learning from &lt;a href=&#34;https://deepmoji.mit.edu/&#34; target=&#34;_blank&#34;&gt;DeepMoji&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We were able to obtain a validation accuracy of 69%, although the model performed poorly in the final test phase. The code for the project is available &lt;a href=&#34;https://github.com/desh2608/tweet-irony-detection&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online Learning of Word Embeddings</title>
      <link>https://desh2608.github.io/post/online-learning-word-embeddings/</link>
      <pubDate>Wed, 14 Mar 2018 13:40:57 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/online-learning-word-embeddings/</guid>
      <description>

&lt;p&gt;Word vectors have become the building blocks for all natural language processing systems. I have earlier written an overview of popular algorithms for learning word embeddings &lt;a href=&#34;https://desh2608.github.io/post/understanding-word-vectors/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. One limitation with all these methods (namely SVD, skip-gram, and GloVe) is that they are all “batch” techniques. In this post, I will discuss two recent papers (which are very similar but were developed independently) which aim to provide an online approximation for the skip-gram algorithm.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But first, what do we mean by a “batch” algorithm?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Simply put, in a batch algorithm, the entire data set needs to be available before we begin the processing. In contrast, an “online” algorithm can process inputs on-the-fly, i.e., in a streaming fashion. Needless to say, such algorithms are also preferable when the available resources are not sufficient to process the entire dataset at once.&lt;/p&gt;

&lt;p&gt;Now that we have some idea about batch algorithms, I’ll explain why the existing methods for word representation learning are of this kind. First, in the case of the standard SVD and Stanford’s GloVe, the entire cooccurence matrix needs to be computed, and only then can the processing be started. If some additional data arrives later, the matrix would have to be recomputed, and training would have to be restarted (if at least one of the updates depends on a changed matrix element). Second, in the case of Mikolov’s &lt;em&gt;word2vec&lt;/em&gt; (skip-gram and CBOW), negative sampling is often used to make the computation more efficient. This sampling depends on the unigram probability distribution of the vocabulary words in the corpus. As such, before learning can happen, we need to compute the vocabulary as well as the unigram distribution.&lt;/p&gt;

&lt;p&gt;Recently, two very similar methods (developed independently) have been proposed to make the skip-gram with negative sampling (SGNS) algorithm learn in a streaming fashion. I’ll quickly review the SGNS algorithm first so that there is some context when we discuss the papers.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;batch-sgns-algorithm&#34;&gt;Batch SGNS algorithm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/15/skipgram.png&#34; alt=&#34;Skip-gram objective. Image taken from [The Morning Paper](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/).&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SGNS is a window-based method with the following training objective: Given the target word, predict all the context words in the window.&lt;/p&gt;

&lt;p&gt;Suppose we have a context window where $w$ is the target word and $c$ is one of the context words. Then, skip-gram’s objective is to compute $P(c|w)$, which is given as&lt;/p&gt;

&lt;p&gt;$$ p(c|w;\theta) = \frac{\exp(v_c \cdot v_w)}{\sum_{c^{\prime}\in C}\exp(v_{c^{\prime}}\cdot v_w)} $$&lt;/p&gt;

&lt;p&gt;Basically, it is just a softmax probability distribution over all the word-context pairs in the corpus, directed by the cosine similarity. However, the denominator term here is very expensive to compute since there may be a very large number of possible context words. To solve this problem, negative sampling is used.&lt;/p&gt;

&lt;p&gt;Goldberg and Levy have explained the derivation for the objective function in SGNS very clearly in their &lt;a href=&#34;https://arxiv.org/pdf/1402.3722.pdf&#34; target=&#34;_blank&#34;&gt;note&lt;/a&gt;. I will try to provide a little intuition here.&lt;/p&gt;

&lt;p&gt;For the word $w$, we are trying to predict the context word $c$. Since we are using softmax, this is essentially like a multi-class classification problem, where we are trying to classify the next word into one of $N$ classes (where $N$ is the number of words in the dictionary). Since $N$ may be quite large, this is a very difficult problem.&lt;/p&gt;

&lt;p&gt;What SGNS does is that it converts this multi-classification problem into binary classification. The new objective is to predict, for any given word-context pair $(w,c)$, whether the pair is in the window or not. For this, we try to increase the probability of a &amp;ldquo;positive&amp;rdquo; pair $(w,c)$, while at the same time reducing the probability of $k$ randomly chosen &amp;ldquo;negative samples&amp;rdquo; $(w,s)$ where $s$ is a word not found in $w$’s context. This leads to the following objective function which we try to maximize in SGNS:&lt;/p&gt;

&lt;p&gt;$$ J = \log \sigma(c\cdot w) + \sum_{i=1}^k \mathbb{E}_{w_i \sim p(w)}[\log \sigma (-w_i \cdot w)]  $$&lt;/p&gt;

&lt;p&gt;In other words, we push the target vector in the direction of the positive context vector, and pull it away from $k$ randomly chosen (w.r.t. the unigram probability distribution) negative vectors. Here &amp;ldquo;negative&amp;rdquo; means that these vectors are not actually present in the target’s context.&lt;/p&gt;

&lt;h4 id=&#34;what-do-we-need-to-make-sgns-online&#34;&gt;What do we need to make SGNS online?&lt;/h4&gt;

&lt;p&gt;As is evident from the above discussion, since SGNS is a window-based approach, the training itself is very much in an online paradigm. However, the constraints are in creating a vocabulary and a unigram distribution for negative sampling, which makes SGNS a two-pass method. Further, if additional data is seen later, the distribution and vocabulary would change, and the model would have to be retrained.&lt;/p&gt;

&lt;p&gt;Essentially, we need online alternatives for 2 aspects of the algorithms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Dynamic vocabulary building&lt;/li&gt;
&lt;li&gt;Adaptive unigram distribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this background, I will now discuss the two proposed methods for online SGNS.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;space-saving-word2vec&#34;&gt;Space-Saving word2vec&lt;/h3&gt;

&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/pdf/1704.07463.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; from researchers at Johns Hopkins, the following solutions were proposed for the two problems mentioned above.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Space-saving algorithm for dynamic vocabulary building.&lt;/li&gt;
&lt;li&gt;Reservoir sampling for adaptive unigram distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Space-saving algorithm:&lt;/strong&gt; It is a popular method to estimate the top-$k$ most frequent items in a streaming data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We declare a structure V containing $k$ pairs of word and their counts, and initialize it to empty pairs.&lt;/li&gt;
&lt;li&gt;As word $w$ arrives, if $w \in V$, we increment its count.&lt;/li&gt;
&lt;li&gt;Otherwise, if $V$ has space, we append the pair $(w,1)$ to $V$.&lt;/li&gt;
&lt;li&gt;If not, the word with the lowest count is replaced by $w$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At any instant, the words in the structure V denote the dynamic vocabulary of the corpus.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reservoir sampling:&lt;/strong&gt; Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of $k$ items from a list S containing $n$ items, where $n$ is either a very large or unknown number. (Wikipedia)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Similar to the SS algorithm, we declare a structure (called the reservoir) of $k$ empty elements (not pairs this time). In addition, we initialize a counter $c$ to 0.&lt;/li&gt;
&lt;li&gt;The first $k$ elements in the stream are filled into the reservoir. $c$ is incremented at every occurence.&lt;/li&gt;
&lt;li&gt;For the remaining items, we draw $j$ from $1,\ldots,c$ randomly. If $j &amp;lt; k$, the $j^{\text{th}}$ element of the reservoir is replaced with the new element.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At any instant, the samples present in the reservoir provide an approximate distribution of items in the entire data stream.&lt;/p&gt;

&lt;p&gt;While the algorithm itslelf is conceptually simple, the authors have mentioned several implementation choices which are important for training SGNS online. I list them here with some observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When a word is ejected from a bin in the dynamic vocabulary, its embeddings are re-initialized. As such, every bin has its own learning rate which is reset when the word in the bin is changed.&lt;/li&gt;
&lt;li&gt;During sentence subsampling, all words not in $W$ are retained. Those in $W$ are retained with a probability which is inversely proportional to the square root of its count in the dictionary.&lt;/li&gt;
&lt;li&gt;Probably the most important deviation from the SGNS algorithm is that the reservoir sampling essentially generates an empirical distribution from which to sample negative context words. In contrast, in the original SGNS algorithm, a &lt;em&gt;smoothed&lt;/em&gt; empirical distribution is used. The authors have themselves allowed that “ smoothing the negative sampling distribution was (sic) shown to increase word embedding quality consistently.”&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;incremental-sgns&#34;&gt;Incremental SGNS&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&#34;http://aclweb.org/anthology/D17-1037&#34; target=&#34;_blank&#34;&gt;EMNLP’17 paper&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; from researchers at Yahoo Japan proposes the following alternative solutions for the aforementioned problems.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Misra-Gries algorithm for dynamic vocabulary building.&lt;/li&gt;
&lt;li&gt;A modified reservoir sampling algorithm for adaptive unigram table.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Misra-Gries algorithm:&lt;/strong&gt; This was developed long before the space-saving algorithm (1982) and was the go-to technique for top-$k$ most frequent itemset estimation in streaming data, before the space-saving algorithm was developed. The method is very similar to SS except for one difference:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When word $w$ is not in $V$ and there is no space to append, every element in $V$ is decremented until some element becomes 0, at which point it is replaced by the new word.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Modified reservoir sampling:&lt;/strong&gt; Here is the pseudocode from the paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/15/reservoir.png&#34; alt=&#34;Modified reservoir sampling. Image taken from original paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This algorithm differs from the conventional Reservoir Sampling in two important ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The counts used here are &lt;em&gt;smoothed&lt;/em&gt; (see line 4 to 6). This has been shown to be important for word vector quality, as discussed above.&lt;/li&gt;
&lt;li&gt;If the reservoir does not have enough space, we iterate over all existing words and replace them with some probability (which is proportional to the smoothed count of $w$). Contrast this with the earlier technique, where a $j$ was randomly sampled and word at that index was replaced. (&lt;strong&gt;Disclaimer&lt;/strong&gt;: &lt;em&gt;I am not sure how exactly this modification helps in learning. If I am allowed to venture a guess, I would say that it is a “soft” equivalent of the hard replacement in the original algorithm. This probably helps in the theoretical analysis of the algorithm.&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In addition, the authors have also provided theoretical justification for their algorithm and proved the following theorem: &lt;em&gt;The loss in case of incremental SGNS converges in probability to that of batch SGNS.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In summary, SGNS is probably the easiest batch word embedding algorithm to “streamify” because of its inherent window-based nature. The constraints of vocabulary and counts are addressed with approximation algorithms. I can think of several possible directions in which this work can be continued.&lt;/p&gt;

&lt;p&gt;First, there are several algorithms for estimating the top-$k$ most frequent items in a data stream. These are divided into count-based and sketch-based methods. The SS algorithm is probably the most efficient count-based technique, but it may be useful to look at other methods to see if they provide some edge. (Although I’m pretty sure the JHU researchers would have been thorough in their
selection of the algorithm.)&lt;/p&gt;

&lt;p&gt;Second, GloVe and SVD are yet to be addressed. In case of GloVe in particular, the problem would be to construct the co-occurence matrix in a online fashion. There should be some related work in statistics which can be leveraged for this, but I haven’t conducted much literature survey in this direction.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;May, Chandler, Kevin Duh, Benjamin Van Durme, and Ashwin Lall. &amp;ldquo;&lt;em&gt;Streaming word embeddings with the space-saving algorithm.&lt;/em&gt;&amp;rdquo; arXiv preprint arXiv:1704.07463 (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Kaji, Nobuhiro, and Hayato Kobayashi. &amp;ldquo;&lt;em&gt;Incremental skip-gram model with negative sampling.&lt;/em&gt;&amp;rdquo; arXiv preprint arXiv:1704.03956 (2017).*
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sparsity in Online Learning with Lasso Regularization</title>
      <link>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</link>
      <pubDate>Sat, 24 Feb 2018 13:40:42 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</guid>
      <description>

&lt;p&gt;Sparse vectors have become popular recently for 2 reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sparse matrices require much less storage since they can be stored using various space-saving methods.&lt;/li&gt;
&lt;li&gt;Sparse vectors are much more interpretable than dense vectors. For instance, the non-zero non-negative components of a sparse word vector may be taken to denote the weights for certain features. In contrast, there is no interpretation for a value like $-0.1347$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sparsity is often induced through the use of L1 (or Lasso) regularization. There are 2 formulations of the Lasso: (i) convex constraint, and (ii) soft regularization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Convex constraint&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the name suggests, a convex constraint is added to the minimization problem so that the parameters do not exceed a certain value.&lt;/p&gt;

&lt;p&gt;$$ \min_{\beta \in \mathbb{R}^p}\lVert y - X\beta \rVert_2^2 \quad \text{s.t.} \quad \lVert \beta \rVert_1 \leq t $$&lt;/p&gt;

&lt;p&gt;The smaller the value of the tuning parameter $t$, fewer is the number of non-zero components in the solution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Soft regularization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is just the Lagrange form the the convex constraint, and is used because it is easier to optimize. Note that it is equivalent to the convex constraint formulation for an appropriately chosen $g$.&lt;/p&gt;

&lt;p&gt;$$ \min_{\beta \in \mathbb{R}^p}\lVert y - X\beta \rVert_2^2 + g\lVert \beta \rVert_1 $$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;There is a great theoretical explanation of sparsity with Lasso regularization by &lt;a href=&#34;http://www.stat.cmu.edu/~ryantibs/&#34; target=&#34;_blank&#34;&gt;Ryan Tibshirani&lt;/a&gt; and &lt;a href=&#34;http://www.stat.cmu.edu/~larry/&#34; target=&#34;_blank&#34;&gt;Larry Wasserman&lt;/a&gt; which you can find &lt;a href=&#34;http://www.stat.cmu.edu/~larry/=sml/sparsity.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I will instead be focusing on some methods that have been introduced recently for inducing sparsity while learning online i.e., when the samples are obtained one at a time. In addition to such a scenario, online learning also comes into the picture when the data set is simply too large to be loaded in memory at once, and there are not sufficient resources for performing batch learning in a parallel fashion.&lt;/p&gt;

&lt;p&gt;In this post, I will summarize 3 such methods:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jmlr.org/papers/volume10/langford09a/langford09a.pdf&#34; target=&#34;_blank&#34;&gt;Stochastic Truncated Gradient&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jmlr.org/papers/volume10/duchi09a/duchi09a.pdf&#34; target=&#34;_blank&#34;&gt;Forward Backward Splitting&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mendeley.com/viewer/?fileId=00e458de-d9ca-a697-5d67-a4c177759778&amp;amp;documentId=0e9eba78-0cbb-3cb2-a8ea-385a2afb64f5&#34; target=&#34;_blank&#34;&gt;Regularized Dual Averaging&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But first, why a simple soft Lasso regularization won’t work? With the soft regularization method, we are essentially summing up 2 floating point values. As such, it is highly improbable that the sum will be zero, since very few pairs of floats add up to zero.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;stochastic-truncated-gradient-stg&#34;&gt;Stochastic Truncated Gradient (STG)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/14/stg.png&#34; alt=&#34;Simple round-off (T0) vs. Truncated Gradient (T1). Image taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;STG combines ideas from 2 simple techniques:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Coefficient rounding&lt;/em&gt;: In this method, the coefficients are rounded to 0 if they are less than a value $\theta$. This is denoted in the figure above (left graph). The rounding is done after every $k$ steps. The problem with this approach is that if $k$ is small, the coefficients do not get an opportunity to reach a value above $\theta$ before they are pulled back to $0$. On the other hand, if $k$ is large, the intermediate steps in the algorithm need to store a large number of non-zero coefficients, which does not solve the storage issue.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sub-gradient method&lt;/em&gt;: In this method, L1-regularization is performed by shifting the update in the opposite direction depending on the sign of the coefficient. The update equation is&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$ f(w_i) = w_i - \eta\nabla_1 L(w_i,z_i) - \eta g \text{sgn}(w_i) $$&lt;/p&gt;

&lt;p&gt;STG combines &lt;em&gt;rounding&lt;/em&gt; from (1) and &lt;em&gt;gravity&lt;/em&gt; from (2) so that (i) sparsity is achieved (unlike the sub-gradient method), and (ii) the rounding off is not too aggressive (unlike the direct rounding approach). The parameter update is then given by the function $T_1$ (shown in the right graph above).&lt;/p&gt;

&lt;p&gt;$$ T_1(v_j,\alpha,\theta) = \begin{cases} \max(0,v_j-\alpha) \quad &amp;amp;\text{if}~ v_j \in [0,\theta] \\\ \min(0,v_j+\alpha) \quad &amp;amp;\text{if}~ v_j \in [-\theta,0] \\\ 0 \quad &amp;amp;\text{otherwise}   \end{cases} $$&lt;/p&gt;

&lt;p&gt;The update rule is given using $T_1$ as&lt;/p&gt;

&lt;p&gt;$$ f(w_i) = T_1 (w_i - \nabla_1 L_1 (w_i,z_i,\eta g_i,\theta)) $$&lt;/p&gt;

&lt;p&gt;Here, $g$ may be called the gravity parameter, and $\theta$ is called the truncation parameter. In general, the larger these parameters are, the more sparsity is incurred. This can be understood easily from the definition of the truncation function.&lt;/p&gt;

&lt;p&gt;Furthermore, note that on setting $\theta = \infty$ in the truncation function yields a special case of the Sub-gradient method wherein &lt;strong&gt;max&lt;/strong&gt; and &lt;strong&gt;min&lt;/strong&gt; operations are performed after applying gravity pull.&lt;/p&gt;

&lt;p&gt;In the remainder of the paper, the authors prove a strong regret bound for the STG method, and also provide an efficient implementation for the same. Furthermore, they show the asymptotic solution of one instance of the algorithm is essentially equivalent to the Lasso regression, thus justifying the algorithm’s ability to produce sparse weight vectors when the number of features is intractably large.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;forward-backward-splitting-fobos&#34;&gt;Forward Backward Splitting (FOBOS)&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Note: The method was named Forward Looking Subgradient (FOLOS) in the first draft and later renamed since it was essentially the same as an earlier proposed technique, the Forward Backward Splitting. The authors abbreviated it to FOBOS instead of FOBAS to avoid confusing readers of the first draft.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;First, a little background. Consider an objective function of the form $f(w) + r(w)$. In the case of a number of machine learning algorithms, the function $f$ denotes the empirical sum of some loss function (such as mean squared error), and the function $r$ is a regularizer (such as Lasso). If we use a simple gradient descent technique to minimize this objective function, the iterates would be of the form&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = w_t - \eta_t g_t^f - \eta_t g_t^r $$&lt;/p&gt;

&lt;p&gt;where the $g$’s are vectors from the subgradient sets of the corresponding functions. From the paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A common problem in subgradient methods is that if $r$ or $f$ is non-differentiable, the iterates of the subgradient method are very rarely at the points of non-differentiability. In the case of the Lasso regularization function, however, these points are often the true minima of the function.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, the subgradient approach will result in neither a true minima nor a sparse solution if $r$ is the L1 regularizer.&lt;/p&gt;

&lt;p&gt;FOBOS, as the name suggests, splits every iteration into 2 steps — a forward step and a backward step, instead of minimizing both $f$ and $r$ simultaneously. The motivation for the method is that for L1 regularization functions, true minima is usually attained at the points of non-differentiability. For example, in the 2-D space, the function resembles a Diamond shape and the minima is obtained at one of the corner points. Each iteration of FOBOS consists of the following 2 steps:&lt;/p&gt;

&lt;p&gt;$$ w_{t+\frac{1}{2}} = w_t - \eta_t g_t^f \\\ w_{t+1} = \text{argmin}_w { \frac{1}{2}(w_t - w_{t+\frac{1}{2}})^2 + \eta_{t+\frac{1}{2}}r(w) } $$&lt;/p&gt;

&lt;p&gt;The first step is a simple unconstrained subgradient step with respect to the function $f$. In the second step, we try to achieve 2 objectives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Stay close to the interim update vector. This is achieved by the first term.&lt;/li&gt;
&lt;li&gt;Attain a low complexity value as expressed by $r$. (Second term)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So the first step is a &lt;em&gt;forward&lt;/em&gt; step, where we update the coefficient in the direction of the subgradient, while the second is a &lt;em&gt;backward&lt;/em&gt; step where we pull the update back a little so as to obtain sparsity by moving in the direction of the non-differentiable points of $r$.&lt;/p&gt;

&lt;p&gt;Using the first equation in the second, taking derivative w.r.t $w$, and equating the derivative to $0$, we obtain the update scheme as&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = w_t - \eta_t g_t^f + \eta_{t+\frac{1}{2}} g_{t+1}^r $$&lt;/p&gt;

&lt;p&gt;(&lt;strong&gt;Note&lt;/strong&gt;: The equation above looks suspiciously similar to the &lt;strong&gt;&lt;em&gt;Nesterov Accelerated Gradient (NAG)&lt;/em&gt;&lt;/strong&gt; method for optimization. The authors have even cited Nesterov’s paper in related work. It might be interesting to  investigate this further.)&lt;/p&gt;

&lt;p&gt;This update scheme has 2 major advantages, according to the author.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;First, from an algorithmic standpoint, it enables sparse solutions at virtually no additional computational cost. Second, the forward-looking gradient allows us to build on existing analyses and show that the resulting framework enjoys the formal convergence properties of many existing gradient-based and online convex programming algorithms.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the paper, the authors also prove convergence of the method and show that on setting the intermediate learning rate properly, low regret bounds can be proved for both online as well as batch settings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;regularized-dual-averaging-rda&#34;&gt;Regularized Dual Averaging (RDA)&lt;/h4&gt;

&lt;p&gt;Both of the above discussed techniques have one limitation — they perform updates depending only on the subgradients at a particular time step. In contrast, the RDA method “exploits the full regularization structure at each iteration.” Also, since the authors derive closed-form solutions for several popular optimization objectives, it follows that the computational complexity of such an approach is not worse than the methods which perform updates only based on current subgradients (both being $\mathcal{O}(n)$).&lt;/p&gt;

&lt;p&gt;RDA comprises of 3 steps in every iteration.&lt;/p&gt;

&lt;p&gt;In the first step, the subgradient is computed for that particular time step. This is the same as every other subgradient-based online optimization method.&lt;/p&gt;

&lt;p&gt;The second step consists of computing a running average of all past subgradients. This is done using the online approach as&lt;/p&gt;

&lt;p&gt;$$ \bar{g}_t = \frac{t-1}{t}\bar{g}_{t-1} + \frac{1}{t}g_t $$&lt;/p&gt;

&lt;p&gt;In the third step, the update is computed as&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = \text{argmin}_w { &amp;lt;\bar{g}_t,w&amp;gt; + \psi(w) + \frac{\beta}{t}h(w) } $$&lt;/p&gt;

&lt;p&gt;Let us try to understand this update scheme. First, the function $h(w)$ is a strongly convex function such that the update vector which minimizes it also minimizes the regularizer. In the case of Lasso regularization, $h(w)$ is chosen as follows.&lt;/p&gt;

&lt;p&gt;$$ h(w) = \frac{1}{2}\lVert w \rVert_2^2 + \rho \lVert w \rVert_1 $$&lt;/p&gt;

&lt;p&gt;where $\rho$ is a parameter called the sparsity enhancing parameter. $\beta$ is a predetermined non-negative and non-decreasing sequence.&lt;/p&gt;

&lt;p&gt;Now to solve the equation, we can just take the derivative of the argument of argmin and equate it to $0$. On solving this equation, we get an update of the form&lt;/p&gt;

&lt;p&gt;$$ w_{t+1} = \frac{t}{\beta_t}(\bar{g}_t + \rho) $$&lt;/p&gt;

&lt;p&gt;So the scheme ensures that the update is in the same convex space as the regularized dual average. Sparsity can further be controlled by tuning the value of the parameter $\rho$. The scaling factor can be regulated using the
non-decreasing sequence selected at the beginning of the algorithm. For the case when it is equal to the time step $t$, the new coefficient is simply the sum of the dual average and the sparsity parameter.&lt;/p&gt;

&lt;p&gt;The above is just my attempt at understanding the update scheme for RDA. I would be happy to discuss it further if you find something wrong with this explanation.&lt;/p&gt;

&lt;p&gt;Now the method itself would become extremely infeasible if this differentiation would have to be performed for every iteration. However, for most commonly used regularizers and loss functions, the update rule can be represented with a closed-form solution. For this reason, the overall algorithm has the same complexity as earlier algorithms which use only the current step subgradient for performing updates.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Langford, John, Lihong Li, and Tong Zhang. “Sparse online learning via truncated gradient.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 10.Mar (2009): 777–801.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Duchi, John, and Yoram Singer. “Efficient online and batch learning using forward backward splitting.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 10.Dec (2009): 2899–2934.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Xiao, Lin. “Dual averaging methods for regularized stochastic learning and online optimization.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 11.Oct (2010): 2543–2596.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Short Note on Stochastic Gradient Descent Algorithms</title>
      <link>https://desh2608.github.io/post/short-note-sgd-algorithms/</link>
      <pubDate>Thu, 08 Feb 2018 13:40:25 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/short-note-sgd-algorithms/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/13/mindmap.png&#34; alt=&#34;Mind Map for algorithms (taken from [this](http://forums.fast.ai/t/how-do-we-decide-the-optimizer-used-for-training/1829/6) forum post)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I just finished reading &lt;a href=&#34;http://ruder.io/&#34; target=&#34;_blank&#34;&gt;Sebastian Ruder&lt;/a&gt;’s amazing &lt;a href=&#34;https://arxiv.org/abs/1609.04747&#34; target=&#34;_blank&#34;&gt;article&lt;/a&gt; providing an overview of the most popular algorithms used for optimizing gradient descent. Here I’ll make very short notes on them primarily for purposes of recall.&lt;/p&gt;

&lt;h4 id=&#34;momentum&#34;&gt;Momentum&lt;/h4&gt;

&lt;p&gt;The update vector consists of another term which has the previous update vector (weighted by $\gamma$). This helps it to move faster downhill — like a ball.&lt;/p&gt;

&lt;p&gt;$$ v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta) $$&lt;/p&gt;

&lt;h4 id=&#34;nesterov-accelerated-gradient-nag&#34;&gt;Nesterov accelerated gradient (NAG)&lt;/h4&gt;

&lt;p&gt;In Momentum optimizer, the ball may go past the minima due to too much momentum, so we want to have a look-ahead term. In NAG, we take gradient of future position instead of current position.&lt;/p&gt;

&lt;p&gt;$$ v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta - \gamma v_{t-1}) $$&lt;/p&gt;

&lt;h4 id=&#34;adagrad&#34;&gt;Adagrad&lt;/h4&gt;

&lt;p&gt;Instead of a common learning rate for all parameters, we want to have separate learning rate for each. So Adagrad keeps sum of squares of parameter-wise gradients and modifies individual learning rates using this. As a result, parameters occuring more often have smaller gradients.&lt;/p&gt;

&lt;p&gt;$$ \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t +\epsilon}} \odot g_t $$&lt;/p&gt;

&lt;h4 id=&#34;rmsprop&#34;&gt;RMSProp&lt;/h4&gt;

&lt;p&gt;In Adagrad, since we keep adding all gradients, gradients become vanishingly small after some time. So in RMSProp, the idea is to add them in a decaying fashion as&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[g^2]_t = \gamma \mathbb{E}[g^2]_{t-1} + (1-\gamma)g_t^2 $$&lt;/p&gt;

&lt;p&gt;Now replace $G_t$ in the denominator of Adagrad equation by this new term. Due to this, the gradients are no more vanishing.&lt;/p&gt;

&lt;h4 id=&#34;adam-adaptive-moment-estimation&#34;&gt;Adam (Adaptive Moment Estimation)&lt;/h4&gt;

&lt;p&gt;Adam combines RMSProp with Momentum. So, in addition to using the decaying average of past squared gradients for parameter-specific learning rate, it uses a decaying average of past gradients in place of the current gradient (similar to Momentum).&lt;/p&gt;

&lt;p&gt;$$ \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t}+\epsilon}}\hat{m}_t $$&lt;/p&gt;

&lt;p&gt;The $\hat{}$ terms are actually bias-corrected averages to ensure that the values are not biased towards 0.&lt;/p&gt;

&lt;h4 id=&#34;nadam&#34;&gt;Nadam&lt;/h4&gt;

&lt;p&gt;Nadam combines RMSProp with NAG (since NAG is usually better for slope adaptation than Momentum. The derivation is simple and can be found in Ruder’s paper.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In summary, SGD suffers from 2 problems: (i) being hesitant at steep slopes, and (ii) having same learning rate for all parameters. So the improved algorithms are categorized as:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Momentum, NAG: address issue (i). Usually NAG &amp;gt; Momentum.&lt;/li&gt;
&lt;li&gt;Adagrad, RMSProp: address issue (ii). RMSProp &amp;gt; Adagrad.&lt;/li&gt;
&lt;li&gt;Adam, Nadam: address both issues, by combining above methods.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: I have skipped a discussion on AdaDelta in this post since it is very similar to RMSProp and the latter is more popular.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Irony Detection in Tweets</title>
      <link>https://desh2608.github.io/post/irony-detection-in-tweets/</link>
      <pubDate>Wed, 07 Feb 2018 13:40:06 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/irony-detection-in-tweets/</guid>
      <description>

&lt;p&gt;There was a &lt;a href=&#34;https://github.com/Cyvhee/SemEval2018-Task3&#34; target=&#34;_blank&#34;&gt;SemEval 2018 Shared Task&lt;/a&gt; on “irony detection in tweets” that ended recently. As a fun personal project, I thought of giving it a shot, just to implement some new ideas. In this post, I will describe my approach for the problem along with some code.&lt;/p&gt;

&lt;h4 id=&#34;problem-description&#34;&gt;Problem description&lt;/h4&gt;

&lt;p&gt;The task itself was divided into two subtasks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Task A: Binary classification&lt;/em&gt;. Given a tweet, detect whether it has irony or not.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Task B: Multi-label classification&lt;/em&gt;. Given a tweet and a set of labels: i) verbal irony realized through a polarity contrast, ii) verbal irony without such a polarity contrast (i.e., other verbal irony), iii) descriptions of situational irony, iv) non-irony, find the correct irony type.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While the task appears to be a simple text classification job, there are several nuances that make it challenging. Irony is often context-dependent or derived from world knowledge. In sentiment analysis, the semantics of the sentences are sufficient to judge whether the sentence has been spoken in a positive or negative manner. However, irony, by definition, almost always exists when the literal meaning of the sentence is dramatically different from what has been implied. Sample this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Just great when you’re (sic) mobile bill arrives by text.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From a sentiment analysis perspective, the presence of the phrase “just great” would adjudge this sentence strongly positive. However, from our world knowledge, we know the nuances of the interplay between a “mobile bill” and “text.” As a human, then, we can judge that the sentence is spoken in irony.&lt;/p&gt;

&lt;p&gt;The problem is: how can we have an automated system understand this?&lt;/p&gt;

&lt;h4 id=&#34;circular-correlation-between-text-and-hashtags&#34;&gt;Circular correlation between text and hashtags&lt;/h4&gt;

&lt;p&gt;The first idea of a solution came from how the dataset was generated in the first place. To mine tweets containing irony, those tweets were selected which contained the hashtag &lt;strong&gt;#not&lt;/strong&gt;. The idea was that a lot of people explicitly declare their intent at irony through hashtags. For instance, consider the following tweet:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Physical therapy at 8 am is just what I want to be doing with my Friday #iwanttosleep&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this example, let us breakdown the tweet into 2 components:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Text&lt;/em&gt;: Physical therapy at 8 am is just what I want to be doing with my Friday.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Hashtag&lt;/em&gt;: I want to sleep&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is obvious from the semantics of the 2 components that they imply very different things. As such, it may help to model the interaction between the “text” and “hashtag” components of the tweet and then use the resulting embedding for classification. In this regard, we are essentially treating the problem as that of relation classification, where the entities are the 2 components and we need to identify whether there exists a relation between them (task A), and if yes, of which type (task B).&lt;/p&gt;

&lt;p&gt;The problem, now, is reduced to the issue of how to model the two components and their interaction. This is where deep learning comes into the picture.&lt;/p&gt;

&lt;h4 id=&#34;modeling-embeddings-and-interaction&#34;&gt;Modeling embeddings and interaction&lt;/h4&gt;

&lt;p&gt;The embeddings to represent the components are obtained simply by passing their pretrained word vectors through a bidirectional LSTM layer. This is fairly simple for the text component.&lt;/p&gt;

&lt;p&gt;However, in the hashtag component, a single hashtag almost always consists of multiple words concatenated into a single string. Therefore, we first perform word segmentation on the hashtag and use the resulting segments to obtain the embedding.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wordsegment as ws
ws.load()
hashtag = “ “.join(ws.segment(temp))
## Here, &#39;temp&#39; is the original hashtag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the embeddings for the two components have been obtained, we use the circular cross-correlation technique (which I have earlier described in &lt;a href=&#34;https://desh2608.github.io/post/beyond-euclidean-embeddings/&#34; target=&#34;_blank&#34;&gt;this blog post&lt;/a&gt; to model their interaction.  Essentially, the operator is defined as&lt;/p&gt;

&lt;p&gt;$$ [a\cdot b]_k = \sum_{i=1}^{d-1}a_i b_{(k+i)\text{mod}d}. $$&lt;/p&gt;

&lt;p&gt;In Tensorflow, this is implemented as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf

def holographic_merge(inp):
    [a, b] = inp
    a_fft = tf.fft(tf.complex(a, 0.0))
    b_fft = tf.fft(tf.complex(b, 0.0))
    ifft = tf.ifft(tf.conj(a_fft) * b_fft)
    return tf.cast(tf.real(ifft), &#39;float32&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of this merge is then passed to an XGBoost classifier (whose implementation was used out-of-the-box from the corresponding Python package).&lt;/p&gt;

&lt;p&gt;This model resulted in a validation accuracy of ~62%, compared to ~59% for a simple LSTM model. Time to analyze where it was failing!&lt;/p&gt;

&lt;h4 id=&#34;world-knowledge-for-irony-detection&#34;&gt;World knowledge for irony detection&lt;/h4&gt;

&lt;p&gt;The problem with this idea was that although it performed well for samples similar to the example given above, such samples constituted only about 20% of the dataset. For a majority of the tweets containing irony, there was no hashtag, and as such, modeling interactions was useless.&lt;/p&gt;

&lt;p&gt;In such cases, we have to solely rely upon the text component to detect hashtag, for e.g.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The fun part about 4 am drives in the winter, is no one has cleaned the snow yet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If an automated system has to understand that the above sentence contains irony, it needs to know that there is nothing fun about driving on a road covered in snow. This knowledge cannot be gained from learning on a few thousand tweets. We now turn to &lt;strong&gt;transfer learning&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;MIT researchers recently built an unsupervised system called &lt;a href=&#34;https://deepmoji.mit.edu/&#34; target=&#34;_blank&#34;&gt;DeepMoji&lt;/a&gt; for emoji prediction in tweets. According to the website, &amp;ldquo;DeepMoji has learned to understand emotions and sarcasm based on millions of emojis. We hypothesize that if we use this pretrained model to extract features from the text component, it may then be used to predict whether the text contains irony. In a way, we are transfering world knowledge to our model (assuming that the million tweets on which DeepMoji was trained is our world!).&lt;/p&gt;

&lt;p&gt;As expected, concatenating the DeepMoji features with the holographic embeddings resulted in a validation accuracy of $\sim69\%$, i.e., a jump of almost 7%. This reinforces our hypothesis that world knowledge is indeed an important ingredient in any kind of irony detection.&lt;/p&gt;

&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;

&lt;p&gt;In essence, we identified 2 aspects that were essential to identify irony in
tweets:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Semantic interaction between text and hashtags, modeled using holographic embeddings&lt;/li&gt;
&lt;li&gt;World knowledge about irony in text, obtained through transfer learning from DeepMoji&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The code for the project is available &lt;a href=&#34;https://github.com/desh2608/tweet-irony-detection&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; In the final test phase, the results were disappointing (~50% for task A) especially given the high performance on validation set. This could likely have been due to some implementation error on the test set, and we are waiting for the gold labels to be released to analyze our mistake.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Uncertain Fuzzy Self-organization based Clustering: Interval Type-2 Approach to Adaptive Resonance Theory</title>
      <link>https://desh2608.github.io/publication/infosc-17-art/</link>
      <pubDate>Mon, 15 Jan 2018 15:02:35 +0530</pubDate>
      
      <guid>https://desh2608.github.io/publication/infosc-17-art/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 2</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-2/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:45 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-2/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;first part&lt;/a&gt; of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used — Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.&lt;/p&gt;

&lt;h4 id=&#34;rademacher-complexity&#34;&gt;Rademacher complexity&lt;/h4&gt;

&lt;p&gt;Given a family of functions, one of the ways to measure its complexity is to see how well it can fit a random assignment of labels. A more complex hypothesis set would be able to fit a random noise better, and vice versa. For this purpose, we define $m$ random variables $\sigma_i$, called Rademacher variables. We then define the &lt;em&gt;empirical&lt;/em&gt; Rademacher complexity as&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathcal{R}_S}(G) = \mathbb{E}_{\sigma}[\text{sup}_{g\in G}\frac{1}{m}\sigma_i g(z_i)] $$&lt;/p&gt;

&lt;p&gt;Here the summation term is essentially the inner product of the vector of noise (Rademacher variables) and the labels with some $g \in G$. Intuitively, this term can be taken to represent the correlation between the actual assignment and the random assignment. On taking the supremum over all $g \in G$, we are computing how well the function class $G$ correlates with random noise on $S$. The expectation of this term over all random noise distributions measures the average correlation.&lt;/p&gt;

&lt;p&gt;Therefore, a higher Rademacher complexity would imply that the function class $G$ is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class $G$ is, higher is the probability that it would have some $g$ which correlates well with random noise.&lt;/p&gt;

&lt;p&gt;However, this is just the empirical R.C. since we are computing the mean on the given sample set. The actual R.C. is obtained by taking the expectation of this value by sampling $S$ from a distribution $D$ consisting of sample sets of size $m$. Having thus defined the R.C., we can obtain an upper bound on the expected value of an error function $g$ taken from a family of functions $G$.&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[g(z)] \leq \frac{1}{m} \sum_{i=1}^m g(z_i) + 2\mathcal{R}_m(G) + \sqrt{\frac{\log \frac{1}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;Note that if we take the first term on RHS to LHS, the LHS becomes the maximum difference between the empirical and general loss (function value if function is binary-valued). We have access to the empirical values, but not the expectation. So we take 2 sample sets A and B which differ at only 1 point, so that we can use the McDiarmid’s inequality.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The McDiarmid’s inequality bounds the probability that the actual mean and expected mean of a function differ by more than a fixed quantity, given that the function does not deviate by a large amount on perturbing a single element.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The actual proof then becomes simply manipulating the expectation and supremum using Jensen’s inequality (function of an expectation is at most expectation of the function, if the function itself is convex). I do not go into the details of the proof here since it is readily available.&lt;/p&gt;

&lt;p&gt;Till now, we have only computed the bounds on the expectation of the set of loss functions $G$. We actually need to compute bounds on the general loss on the hypothesis class $H$, which assigns binary values to given samples. For this, we use the following lemma which is simple to prove.&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathcal{R}_S}(G) = \frac{1}{2}\hat{\mathcal{R}_{S_X}(G) $$&lt;/p&gt;

&lt;p&gt;From this and the earlier result, we easily arrive at an upper bound on the generalization error of the hypothesis class in terms of its Rademacher complexity.&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{R}_m(H) + \sqrt{\frac{\log \frac{1}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;Here, computing the empirical loss is simple, but computing the R.C. for some hypothesis sets may be hard (since it is equivalent to an empirical risk minimization problem). Therefore, we need some complexity measures which are easier to compute.&lt;/p&gt;

&lt;h4 id=&#34;growth-function&#34;&gt;Growth function&lt;/h4&gt;

&lt;p&gt;The growth function of a hypothesis class $H$ for sample size $m$ denotes the number of distinct ways that $H$ can classify the sample. A more complex hypothesis class would be able to have a larger number of possible combinations for any sample size $m$. However, unlike R.C., this measure is purely combinatorial, and independent of the underlying distributions in $H$.&lt;/p&gt;

&lt;p&gt;The Rademacher complexity and the growth function are related by Massart’s lemma as&lt;/p&gt;

&lt;p&gt;$$ \mathcal{R}_m(G) \leq \sqrt{\frac{2\log \prod_G (m) }{m}} $$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Massart’s lemma bounds the expected correlation of a given vector taken from a set with a vector of random noise, in terms of the size of the set, dimensionality of the set, and the maximum L2-norm of the set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As soon as we see “expected correlation,” we should think of the Rademacher complexity. To introduce the growth function, we use the term for the size of the set, since it essentially denotes the size of set containing all possible assignments for a sample.&lt;/p&gt;

&lt;p&gt;Using this relation in the earlier obtained upper bound, we can bound the generalization error in terms of the growth function.&lt;/p&gt;

&lt;p&gt;Although it is a combinatorial quantity, the growth function still depends on the sample size $m$, and thus would require repeated calculations for all values $m&amp;gt;1$. Instead, we turn to the third and most popular complexity measure for hypothesis sets.&lt;/p&gt;

&lt;h4 id=&#34;vc-dimension&#34;&gt;VC-dimension&lt;/h4&gt;

&lt;p&gt;The VC-dimension of a hypothesis class is the size of the largest set that can be fully shattered by it. By shattering, we mean that $H$ can classify the given set in all possible ways. Formally,&lt;/p&gt;

&lt;p&gt;$$ VCdim(H) = \max{ m:\prod_H (m) = 2^m } $$&lt;/p&gt;

&lt;p&gt;It is important to understand 2 things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If $VCdim(H) = d$, then there exists a set of size $d$ that can be fully shattered. This does not mean that all sets of size $d$ or less are fully shattered by $H$.&lt;/li&gt;
&lt;li&gt;Also, in this case, no set of size greater than $d$ can ever be shattered by $H$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To relate VC-dimension with the growth function, we use the Sauer’s lemma:&lt;/p&gt;

&lt;p&gt;$$ \prod_H(m) \leq \sum_{i=0}^m {m\choose i} $$&lt;/p&gt;

&lt;p&gt;Here, the LHS, which is the growth function, represents the number of possible behaviors that $H$ can have on a set of size $m$. The RHS is the number of small subsets that are completely shattered by $H$. For a detailed proof, I highly recommend &lt;a href=&#34;https://www.youtube.com/watch?v=LHIwWeQhhk4&#34; target=&#34;_blank&#34;&gt;this lecture&lt;/a&gt; (Actually, I would highly recommend the entire course).&lt;/p&gt;

&lt;p&gt;Using some manipulations on the combinatorial, we arrive at&lt;/p&gt;

&lt;p&gt;$$ \prod_H(m) \leq  \left( \frac{em}{d} \right)^d = \mathcal{O}(m^d) $$&lt;/p&gt;

&lt;p&gt;Now we can use this relation with the earlier results to bound the generalization error in terms of the VC-dimension of the hypothesis class.&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{O}\left( \sqrt{\frac{\log(m/d)}{m/d}} \right) $$&lt;/p&gt;

&lt;p&gt;where $m$ is the sample size and $d$ is the VC-dimension.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Here is a quick recap:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rademacher complexity — ability to fit random labels (using correlation)&lt;/li&gt;
&lt;li&gt;Growth function — number of distinct behaviors on $m$&lt;/li&gt;
&lt;li&gt;VC-dimension — largest set size that can be fully shattered&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This blog post is loosely based on notes made from Chapter 3 “Rademacher complexity and VC-Dimension” of &lt;em&gt;Foundations of Machine Learning.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 1</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-1/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:43 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-1/</guid>
      <description>

&lt;p&gt;One of the most significant take-aways from NIPS 2017 was the &lt;a href=&#34;https://syncedreview.com/2017/12/12/lecun-vs-rahimi-has-machine-learning-become-alchemy/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;alchemy&amp;rdquo; debate&lt;/a&gt; spearheaded by &lt;a href=&#34;https://www.linkedin.com/in/ali-rahimi-a85104/&#34; target=&#34;_blank&#34;&gt;Ali Rahimi&lt;/a&gt;. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.&lt;/p&gt;

&lt;p&gt;One of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set. For this, we require the hypothesis class $H$ to approximate the concept class $C$ which determines the labels for the distribution $D$. Since both $C$ and $D$ are unknown, we try to model $H$ based on the known sample set $S$ and its labels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generalization error:&lt;/strong&gt; The generalization error of a hypothesis $h$ is the expectation of the error on a sample $x$ picked from the distribution $D$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Empirical error:&lt;/strong&gt; This is the mean of the error of hypothesis $h$ on the sample $S$ of size $m$.&lt;/p&gt;

&lt;p&gt;Having defined the generalization error and empirical error thus, we can state the objective of learning as follows.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The objective of learning is to have the empirical error approximate the generalization error with high probability.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This kind of a learning framework is known as &lt;strong&gt;PAC-learning&lt;/strong&gt; (Probably Approximately Correct). Formally, a concept class $C$ is PAC-learnable if there is some algorithm A for which the generalization error on a sample $S$ derived from the distribution $D$ is very low (less than $\epsilon$) with high probability (greater than $1- \delta$). In other words, we can say that for a PAC-learnable class, the accuracy is high with good confidence.&lt;/p&gt;

&lt;h3 id=&#34;guarantees-for-finite-hypothesis-sets&#34;&gt;Guarantees for finite hypothesis sets&lt;/h3&gt;

&lt;p&gt;The PAC-learning framework provides strong guarantees for finite hypothesis sets (i.e., where the size of $H$ is finite). Again, this falls in two categories — the consistent case, and the inconsistent case. A hypothesis class is said to be &lt;em&gt;consistent&lt;/em&gt; if it admits no error on the training sample, i.e., the training accuracy is 100%.&lt;/p&gt;

&lt;h4 id=&#34;consistent-hypothesis&#34;&gt;Consistent hypothesis&lt;/h4&gt;

&lt;p&gt;Let us consider a finite hypothesis set $H$. We want the generalization error to be less than some $\epsilon$, so we will take a consistent hypothesis $h \in H$, and bound the probability that its error is more than $\epsilon$, i.e., we are calculating the probability that there exists some $h \in H$, such that $h$ is consistent and its generalization error is more than $\epsilon$. This is simply the union of all $h \in H$ such that it follows the said constraints. By the union bound, this
probability will be less than the sum of the individual probabilities i.e.,&lt;/p&gt;

&lt;p&gt;$$ \sum_{h\in H}Pr[\hat{R}(h)=0 \wedge R(h) &amp;gt; \epsilon] $$&lt;/p&gt;

&lt;p&gt;From the definition of conditional probability, we can write&lt;/p&gt;

&lt;p&gt;$$ Pr(A \cap B) = Pr(A|B)Pr(B) \leq Pr(A|B) $$&lt;/p&gt;

&lt;p&gt;which bounds the required probability $P$ as&lt;/p&gt;

&lt;p&gt;$$ P \leq \sum_{h\in H} Pr[\hat{R}(h) =0| R(h) &amp;gt; \epsilon] $$&lt;/p&gt;

&lt;p&gt;The condition says that the expectation of error of $h$ on any sample is at least $\epsilon$, so it would correctly classify a sample with probability at most $1-\epsilon$. Hence, to correctly classify $m$ training samples with $|H|$ hypotheses, the total probability is given as&lt;/p&gt;

&lt;p&gt;$$ P \leq |H|(1-\epsilon)^m \leq |H|\exp(-m\epsilon) $$&lt;/p&gt;

&lt;p&gt;On setting the RHS of the inequality to $\delta$, we obtain the generalization bound of the finite, consistent hypothesis class as&lt;/p&gt;

&lt;p&gt;$$ R(h_S) \leq \frac{1}{m}\left( \log |H| + \log \frac{1}{\delta} \right) $$&lt;/p&gt;

&lt;p&gt;As expected, the generalization error decreases with a larger training set. However, to arrive at a consistent algorithm, we may have to increase the size of the hypothesis class, which results in an increase in generalization error.&lt;/p&gt;

&lt;h4 id=&#34;inconsistent-hypothesis&#34;&gt;Inconsistent hypothesis&lt;/h4&gt;

&lt;p&gt;In practical scenarios, it is very restrictive to always require a consistent hypothesis class to bound the generalization error. In this section, we look at a more general case where empirical error is non-zero. For this derivation, we use the &lt;strong&gt;Hoeffding’s inequality&lt;/strong&gt; which provides an upper bound on the probability that the mean of independent variables in an interval $[0,1]$ deviates from its expected value by more than a certain amount.&lt;/p&gt;

&lt;p&gt;$$ P(\bar{X} - \mathbb{E}\bar{X} \geq t) \leq \exp(-2nt^2) $$&lt;/p&gt;

&lt;p&gt;If we take the errors as the random variable, their mean is the empirical error and the expectation is the generalization error. We can then get an upper bound for the generalization error of a single hypothesis $h$ as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \sqrt{\frac{\log \frac{2}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;However, this is still not the general case since the hypothesis $h$ returned by the learning algorithm is not fixed. Similar to the consistent case, we will try to obtain an upper bound on the generalization error for an inconsistent (but finite) hypothesis, i.e., we need to compute the probability that there exists some hypothesis $h \in H$ such that the generalization error of $h$ differs from its empirical error by a value greater than $\epsilon$. Again, using the union bound, we get&lt;/p&gt;

&lt;p&gt;$$ P \leq \sum_{h \in H}Pr[|\hat{R}(h)-R(h)|&amp;gt;\epsilon] $$&lt;/p&gt;

&lt;p&gt;Using the Hoeffdieng’s inequality, this becomes&lt;/p&gt;

&lt;p&gt;$$ P \leq 2|H|\exp(-2m\epsilon^2) $$&lt;/p&gt;

&lt;p&gt;Now equating the RHS with $\delta$, we can arrive at the result&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \sqrt{\frac{\log |H| + \log \frac{2}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;Here it is interesting to note that for a fixed $|H|$, to attain the same guarantee as in the consistent case, a quadratically larger labeled sample is required. Let us now analyze the role of the size of hypothesis class. If we have a smaller $H$, the second term is reduced but the empirical error may increase, and vice versa. However, for the same empirical error, it is always better to go with the smaller hypothesis class, i.e., the famous &lt;em&gt;Occam’s Razor&lt;/em&gt; principle.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this article, we looked at some generalization bounds in case of a finite hypothesis, using the PAC learning framework. In the next part, I will discuss some measures for infinite hypotheses, namely the Rademacher complexity, growth function, and the VC dimension.&lt;/p&gt;

&lt;p&gt;This blog post is loosely based on notes made from Chapter 2 &amp;ldquo;The PAC Learning Framework&amp;rdquo; of &lt;em&gt;Foundations of Machine Learning&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Approaches for NMT</title>
      <link>https://desh2608.github.io/post/unsupervised-approaches-for-nmt/</link>
      <pubDate>Thu, 14 Dec 2017 13:39:30 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/unsupervised-approaches-for-nmt/</guid>
      <description>

&lt;p&gt;Translation is one of those tasks in language where the arrival of deep learning systems, and in particular sequence-to-sequence, has been something like a boon. In less than 4 years since the first paper on Neural Machine Translation, software giants such as Google and Microsoft have already announced that their translation systems have almost completely shifted from statistical to neural. Gone are the days when researchers mulled over complex word and phrase alignment techniques, and yet fell short on several language combinations. With the latest framework, all you need are a million parallel sentences, and your system can then translate between this pair sufficiently well.&lt;/p&gt;

&lt;p&gt;A million parallel sentences — that’s a little constraining, though! It is often difficult and sometimes even impossible to obtain a bilingual parallel corpus for many pairs of languages. In such cases, using a pivot language for triangulation has been found to be helpful. However, even in such supervised systems, the performance is still constrained by the size of the training corpus.&lt;/p&gt;

&lt;p&gt;Monolingual data, on the other hand, is available in abundance, and a number of semi-supervised systems do use these, but mostly for the language modeling part of translation. For example, a naive system may perform word-by-word substitution and use a language model trained on the target language to obtain the most probable word order.&lt;/p&gt;

&lt;p&gt;Recently, there have been 2 very similar papers (both currently under review at ICLR ’18) which propose to perform completely unsupervised machine translation. In this article, I will discuss both of these papers. A similar blog is available &lt;a href=&#34;http://ankitg.me/blog/2017/11/05/unsupervised-machine-translation.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, but I didn’t know of its existence until I was already halfway through this post.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;unsupervised-neural-machine-translation&#34;&gt;Unsupervised Neural Machine Translation&lt;/h4&gt;

&lt;p&gt;This paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is from Prof. &lt;a href=&#34;http://www.kyunghyuncho.me/&#34; target=&#34;_blank&#34;&gt;Kyunghyu Cho&lt;/a&gt; (NYU), and the authors have used the traditional seq2seq model with a twist. The encoder is shared across all languages, but each language has its own decoder. The intuition is that a shared encoder will transform a sentence to a shared space representation, from where the language-specific decoder will be able to decode it to its own language.&lt;/p&gt;

&lt;p&gt;Both the encoder and decoder are 2 layer bidirectional RNNs with GRU units. Furthermore, the embeddings used in the feature layer are fixed, and are obtained from pre-trained cross-lingual dictionary. This ensures that the shared space representation obtained using the encoder is language-independent.&lt;/p&gt;

&lt;p&gt;The paper uses 2 interesting techniques for the unsupervised training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Denoising:&lt;/strong&gt; The autoencoder (or seq2seq) is used to reconstruct a sentence in a language, since we only have a monolingual corpus on which to train the system. Due to such a setting, an optimal system would essentially learn to copy the input to the output, and the system would reduce to a word-by-word substitution system. To prevent this, “denoising” is used, which introduces random noise in the input sentence so that copying cannot give the best output. This is dones by making $\frac{N}{2}$ random swaps for any sequence of $N$ tokens. There are 2 advantages to this technique:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Since copying is out of the picture, the system needs to learn the internal structure of language to perform well.&lt;/li&gt;
&lt;li&gt;By swapping words randomly, we also account for word order divergence across languages. For instance, &lt;em&gt;Los Angeles International Airport&lt;/em&gt; in English becomes &lt;em&gt;Aéroport international de Los Angeles&lt;/em&gt; in French.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Backtranslation:&lt;/strong&gt; Even with denoising added, the system is still monolingual. To integrate some element of cross-lingual training, the authors use the method of backtranslation. Given a sentence $x$ in language L1, the shared encoder is used to get the latent representation, and the decoder for the other language L2 is used to obtain a noisy translation $y$. This translation $y$ is then used to
predict the original sentence $x$ using the encoder and decoder for L1. This technique creates a pseudo-parallel corpus so that the system can learn cross-lingual translation.&lt;/p&gt;

&lt;p&gt;Denoising forces the system to capture broad word-level equivalences, while backtranslation helps it to learn more subtle relations between the language pairs. Furthermore, using pretrained cross-lingual embeddings ensures that the shared latent space representations for sentences in both the languages are near each other when the sentences have the same sense (or meaning).&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;unsupervised-machine-translation-using-monolingual-corpora-only&#34;&gt;Unsupervised Machine Translation using Monolingual Corpora Only&lt;/h4&gt;

&lt;p&gt;A very similar paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; from researchers at Facebook employs almost the same techniques, but differs slightly in the encoding mechanism. I personally enjoyed reading this paper more than the first one, although they haven’t gone into details of the components they use in their model. The explanation of the loss function for end-to-end training is very lucid, and the overall structuring itself is appealing to a novice researcher like myself.&lt;/p&gt;

&lt;p&gt;Anyway, the model used in this paper consists of a single encoder and a single decoder (bidirectional LSTM with attention in the decoder, similar to the NMT model used in Google Translate) which is shared by both the languages. For the unsupervised training, 3 techniques are employed.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Denoising&lt;/strong&gt;: Similar to the above paper, the autoencoder is denoised so that it does not learn a word-by-word substitution. The noise model in this case consists of: (i) dropping every word with some random probability, and (ii) shuffling the sentence by applying a random permutation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-domain training&lt;/strong&gt;: This is the same as the “backtranslation” technique used in the above paper. However, the authors have explicitly mentioned that to obtain the translation $x$ from the sentence $y$, the model of the previous iteration is used. This requires that the model be initialized with a naive translation strategy, which in this case, is simple word-by-word substitution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adversarial training&lt;/strong&gt;: In the above paper, due to the use of cross-lingual fixed embeddings in the shared encoder, the latent space representations were arguably similar for similar sentences in different languages. This method does not use cross-lingual embeddings, and hence, the representations will be similar only “as long as the two monolingual corpora exhibit strong structure in feature space.” (Full disclosure: This statement is written as a hand-waving argument without a justification, and one of the reviewers has even pointed this out.) In order to overcome this constraint, the authors employ a discriminator whose task is to predict the language of the encoded sentence. In turn, the encoder has an added term in its loss function which ensures that the representation of similar sentences in different languages are nearby in the latent space.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/9/mono.png&#34; alt=&#34;Training objectives for the system. Figure taken from the paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since the training is done iteratively and BLEU scores are computed at every step, we can simply select the hyperparameters corresponding to the best performing iteration. Empirically, the authors found that this selection has good correlation with test-time performance of the system. Furthermore, this unsupervised model was found to perform as good as a comparable supervised model trained on 100,000 parallel sentences, which is definitely an encouraging achievement for further research in unsupervised NMT.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Artetxe, Mikel, et al. “&lt;a href=&#34;https://arxiv.org/abs/1710.11041&#34; target=&#34;_blank&#34;&gt;Unsupervised Neural Machine Translation&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1710.11041&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Lample, Guillaume, Ludovic Denoyer, and Marc’Aurelio Ranzato. “&lt;a href=&#34;https://arxiv.org/abs/1711.00043&#34; target=&#34;_blank&#34;&gt;Unsupervised Machine Translation Using Monolingual Corpora Only&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1711.00043&lt;/em&gt;(2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Beyond Euclidean Embeddings</title>
      <link>https://desh2608.github.io/post/beyond-euclidean-embeddings/</link>
      <pubDate>Wed, 06 Dec 2017 13:39:15 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/beyond-euclidean-embeddings/</guid>
      <description>

&lt;p&gt;Representation learning, as the name suggests, seeks to learn representations for structures such as images, videos, words, sentencences, graphs, etc., which may then be used for several objectives. Arguably the most important representations used nowadays are word embeddings, usually learnt using the distributional semantics methods such as skip-gram or GloVe. I have previously written about these methods &lt;a href=&#34;https://desh2608.github.io/post/understanding-word-vectors/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Two assumptions are inherent while using these methods to learn word vectors:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;That words are best visualized as points in the $n$-dimensional space.&lt;/li&gt;
&lt;li&gt;That the Euclidean distance or the Euclidean dot product are the best measures of similarity between words (or other structures for which the embeddings have been learnt).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Over the last couple years, researchers have sought to challenge both of these assumptions by proposing several new non-Euclidean representations for words and graphs. Especially in the case of learning relational embeddings, the model should be able to learn all combinations of properties, namely reflexivity/irreflexivity, symmetry/anti-symmetry, and transitivity. Euclidean dot products are limited in that they cannot handle anti-symmetry, since dot products are commutative.&lt;/p&gt;

&lt;p&gt;In this post, I will discuss 4 non-Euclidean embeddings: Gaussian, Holographic, Complex, and Poincare.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;word-representations-via-gaussian-embeddings&#34;&gt;Word representations via Gaussian embeddings&lt;/h4&gt;

&lt;p&gt;The key idea in this ICLR ’15 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is to map words to a density instead of a point. Density here is represented by a “potential function,” such as a Gaussian. The authors provide a nice recap of energy functions as a tool for learning word representations.&lt;/p&gt;

&lt;p&gt;Essentially, any representation learning involves an energy function $E(x,y)$ which scores pairs of inputs and outputs. A loss function is then uses this energy function to quantify the difference between actual output and predicted output. In the case of skip-gram models, the energy function used is a dot product, and the loss function is a logistic regression. In this paper, the authors propose 2 kinds of energy functions (for symmetric and asymmetric similarity), and the loss function used is max margin as follows.&lt;/p&gt;

&lt;p&gt;$$ L_m(w,c_p,c_n) = \max(0,m-E(w,c_p)+E(w,c_n)) $$&lt;/p&gt;

&lt;p&gt;For a Gaussian distribution to model any word, a baseline approach may involve using the distribution around the word to compute and mean and variance. If a word $w$ occurs $N$ times in the corpus, the covariance of the distribution around $w$ is given as&lt;/p&gt;

&lt;p&gt;$$ \sum_w = \frac{1}{NW}\sum_i^N \sum_j^W (c(w)_{ij})(c(w)_{ij}-w)^T $$&lt;/p&gt;

&lt;p&gt;where W is the window size, and $w$ is the assumed mean. However, the distributions learned using this empirical approach do not possess some desired properties such as unsupervised entailment represented as inclusion between ellipsoids. To solve this, 2 energy functions are proposed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method 1: Symmetric similarity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This method just computes the inner product between the two distributions. It has been shown that the inner product of two normal distributions is again a normal distribution. Furthermore, we take the log of this value for two reasons. First, since we are dealing with ranking loss, taking the logarithm converts absolute values into relative values, which is easier to interpret. Second, it is numerically easier to deal with.&lt;/p&gt;

&lt;p&gt;Furthermore, the energy function is shown to be of the form &lt;strong&gt;log det A + const&lt;/strong&gt;. We can interpret the constant term as a regularizer that prevents us from decreasing the distance by only increasing joint variance. This combination pushes the means together while encouraging them to have more concentrated, sharply peaked distributions in order to have high energy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method 2: Asymmetric similarity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This method computes the energy function as the negative of the KL-divergence between the 2 distributions (negative because the KL-divergence returns a distance value and hence needs to be minimized to increase similarity). A low KL divergence from $x$ to $y$ indicates that we can encode $y$ easily as $x$, implying that $y$ entails (logically follows from) $x$.&lt;/p&gt;

&lt;p&gt;The authors have further computed the gradients for each of the two energy functions, and they are easily expressible in terms of existing means and covariances.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;poincare-embeddings-for-hierarchical-representations&#34;&gt;Poincare embeddings for hierarchical representations&lt;/h4&gt;

&lt;p&gt;This paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; proposes embeddings in hyperbolic spaces, such as the Poincare sphere. Before we get into the method itself, I think it would be best to give a brief overview of hyperbolic geometry itself.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hyperbolic geometry&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In his book &lt;em&gt;Elements&lt;/em&gt;, Euclid provided a rigourous framework for axioms, theorems and postulates for all geometrical knowledge at the time. He stated 5 axioms which were to be assumed true. The first 4 were quite self-evident, and were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Any two points can be connected by a line.&lt;/li&gt;
&lt;li&gt;Any line segment can be extended indefinitely.&lt;/li&gt;
&lt;li&gt;Given a line segment, a circle can be drawn with center at one of the endpoints and radius equal to the length of the segment.&lt;/li&gt;
&lt;li&gt;Any two right angles are congruent.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, the fifth axiom, also known as Playfair’s axiom, is much less obvious.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Playfair’s axiom&lt;/em&gt;: Given a line L and a point P, there exists at most one line through P that is parallel to L.&lt;/p&gt;

&lt;p&gt;Euclid himself wasn’t very fond of this axiom and his first 28 postulates depended only on the first 4 axioms, which are the “core” of Euclidean geometry. Even 2000 years after his death, mathematicians tried to derive the fifth axiom from the first 4. While using “proof by contradiction” for this purpose, they assumed the negation of the fifth axiom (Given a line L and a point P not on L, there are at least two distinct lines that can be drawn through P that are parallel to L) and tried to arrive at a contradiction. However, while the derived results were strange and very different from those in Euclidean geometry, they were consistent within themselves. This was a turning point in mathematics as such a bifurcation in geometry had never been expected before. The geometry that arose from these explorations is known as &lt;em&gt;hyperbolic geometry&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;With this knowledge, let us now look at how embeddings may be computed in this new model.&lt;/p&gt;

&lt;p&gt;The Poincare sphere model of hyperbolic space is particularly suitable for representing hierarchies. Consider a knowledge base which can be visualized as a tree. For any branching factor &lt;em&gt;b&lt;/em&gt;, the number of leaf nodes increases exponentially as the number of levels increases. If we try to replicate this construction in a Euclidean disk(sphere), it would not be possible since the area(volume) of a disk(sphere) increases only quadratically(cubically) with increase in radius. This requires that we increase the number of dimensions exponentially.&lt;/p&gt;

&lt;p&gt;However, the Poincare sphere embeds such hierarchies easily: nodes that are exactly $l$ levels below the root are placed on a sphere in hyperbolic space with radius $r \propto l$ and nodes that are less than $l$ levels below the root are located within this sphere. This type of construction is possible as hyperbolic disc area and circle length grow exponentially with their radius. In the paper, the authors used a sphere instead of disk since more degrees of freedom implies better representation of latent hierarchies.&lt;/p&gt;

&lt;p&gt;Distances in the hyperbolic space are given as&lt;/p&gt;

&lt;p&gt;$$ d(u,v) = arcosh\left( 1 + 2\frac{\lVert u-v \rVert^2}{(1-\lVert u \rVert)^2(1-\lVert v \rVert)^2} \right) $$&lt;/p&gt;

&lt;p&gt;Here, hierarchy is represented using the norm of the embedding, while similarity is mirrored in the norm of vector difference. Furthermore, the function is differentiable, which is good for gradient descent.&lt;/p&gt;

&lt;p&gt;For optimization, the update term is the learning rate times the Riemannian gradient of the parameter. The Riemannian gradient itself is computed by taking the product of the Poincare ball matrix inverse (which is trivial to compute) with the Euclidean gradient (which depends on the gradients of the distance function). The loss function used in the paper is a softmax with negative sampling.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;holographic-embeddings-for-knowledge-graphs&#34;&gt;Holographic embeddings for knowledge graphs&lt;/h4&gt;

&lt;p&gt;This and the next method seek to learn embeddings for relations within knowledge graphs, and the motivation for both is to have embeddings that allow asymmetric relations to be sufficiently represented. To achieve said objective, this AAAI ’16 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; employs circular correlations, while the next paper from ICML ’16&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; uses complex embeddings.&lt;/p&gt;

&lt;p&gt;Before describing the method, I will first describe the task. Given a set $E$ of entities and a set $P$ of relation types, the objective is to learn a characteristic function for each relation type that determines whether that relation exists between any two elements in $E$. The entities are referred to as the &lt;em&gt;subject&lt;/em&gt; and the &lt;em&gt;object&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The general approach is to approximate the characteristic function using a function that takes as input the relation vector, and the vectors corresponding to the subject and the object. Using a loss function such as log likelihood minimization with negative sampling, we can tune the parameters that describe the entity vectors and the relation type vector. This is similar to our earlier discussion on energy function optimization.&lt;/p&gt;

&lt;p&gt;The catch here is that the characteristic function is supposed to output a scalar score (the probability of the relation), but the inputs to it are vectors. To convert the input to a scalar, the entity vectors are combined using a composition operator &lt;strong&gt;o&lt;/strong&gt;(more on this later), and its dot product is taken with the relation type vector.&lt;/p&gt;

&lt;p&gt;So the problem boils down to the choice of a good compositional operator. In the past, three different approaches have been taken for this problem.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Tensor product&lt;/em&gt;: Take the outer product of the entity vectors. However, the resulting vector contains the square of the initial number of parameters, which may cause problems such as overfitting down the line.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Concatenation, projection, and non-linearity&lt;/em&gt;: The projection matrix is learned during training. However, due to the absence of interaction between features, the representation learnt is not rich enough, even though non-linearity is added.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Non-compositional methods&lt;/em&gt;: In these approaches, the score is computed as the distance of the difference vector with the relation vector (e.g., TransE).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Essentially, we want an operator which has cross-feature interactions without having the number of parameters explode. To this end, the authors propose the circular correlation operator, which is given as&lt;/p&gt;

&lt;p&gt;$$ [a\cdot b]_k = \sum_{i=1}^{d-1}a_i b_{(k+i)\text{mod}d}. $$&lt;/p&gt;

&lt;p&gt;The output contains as many parameters as the input vectors, while also capturing the interaction between the features. The function measures the covariance between embeddings at different dimension shifts, and the asymmetry stems from this circular correlation.&lt;/p&gt;

&lt;p&gt;At this point, you may be wondering why a simple convolutional operator would not suffice. The answer is that convolution is a commutative function, while correlation is not. Again, the key lies in symmetry (or the lack of it)!&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;complex-embeddings-for-link-prediction&#34;&gt;Complex embeddings for link prediction&lt;/h4&gt;

&lt;p&gt;In the objective of predicting relations described earlier, we can think of the characteristic function as a function which takes as input a latent matrix &lt;strong&gt;X&lt;/strong&gt; of scores and outputs the corresponding probability. This latent matrix is an $E \times E$ matrix since it contains the scores for every possible pair of entities. However, since the number of entitites may be very large, the problem we want to solve is that of matrix factorization.&lt;/p&gt;

&lt;p&gt;This is similar to the singular value decomposition method for learning word vectors that I discussed in an earlier blog post. If we assume that an entity has only one unique representation, regardless of whether it occurs as subject or object, the matrix X can be factorized as&lt;/p&gt;

&lt;p&gt;$$ X = EWE^{-1} $$&lt;/p&gt;

&lt;p&gt;Since the entity vectors are complex in nature ($u$ = Re($u$) + $i$Im($u$)), the matrix factorization of $X$ may be either real or complex. But since the characteristic function returns a real output, we define $X$ as the Real part of the factorization. Now, our original objective is to learn $P(Y=1)$ for every $s-o$ pair, and we are trying to approximate this using the latent matrix $X$. In the case of binary relations (yes/no), $Y$ is essentially a sign matrix, and hence it is safe to assume that its “sign-rank” is low.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But what is a “sign-rank”?&lt;/em&gt; It refers to the smallest rank of a real matrix having the same sign pattern as $Y$. The authors showed in an earlier paper that if the sign rank of $Y$ is low, the rank of Re($EWE^T$) is at most twice that of $Y$. While this is a good upper bound, the actual rank is often much lower than the rank of $Y$.&lt;/p&gt;

&lt;p&gt;In the case of multi-relational data, each relation has a representation $w$ associated with it. The characteristic function then takes as input the relation type along with the subject and object, and computes the score based on a novel scoring function. This function has the following property: if $w$ is real, the characteristic function is symmetric, and if $w$ is imaginary, then it is anti-symmetric.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;While Euclidean embeddings are popular, they are in no way sufficient to represent all the complexities and hierarchies in language. These methods suggest that looking at non-Euclidean spaces for representation learning may be the way to go.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Vilnis, Luke, and Andrew McCallum. “&lt;a href=&#34;https://arxiv.org/pdf/1412.6623.pdf&#34; target=&#34;_blank&#34;&gt;Word representations via gaussian embedding&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1412.6623&lt;/em&gt;(2014).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Nickel, Maximilian, and Douwe Kiela. “&lt;a href=&#34;https://arxiv.org/pdf/1705.08039.pdf&#34; target=&#34;_blank&#34;&gt;Poincare Embeddings for Learning Hierarchical Representations&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1705.08039&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Nickel, Maximilian, Lorenzo Rosasco, and Tomaso A. Poggio. “&lt;a href=&#34;https://arxiv.org/pdf/1510.04935.pdf&#34; target=&#34;_blank&#34;&gt;Holographic Embeddings of Knowledge Graphs&lt;/a&gt;.” &lt;em&gt;AAAI&lt;/em&gt;. 2016.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Trouillon, Théo, et al. “&lt;a href=&#34;http://proceedings.mlr.press/v48/trouillon16.pdf&#34; target=&#34;_blank&#34;&gt;Complex embeddings for simple link prediction&lt;/a&gt;.” &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. 2016.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning for Multimodal Systems</title>
      <link>https://desh2608.github.io/post/deep-learning-multimodal-systems/</link>
      <pubDate>Thu, 09 Nov 2017 13:38:58 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-multimodal-systems/</guid>
      <description>

&lt;p&gt;When I was browsing through research groups for my grad school applications, I came across some interesting applications of new deep learning methods in a multimodal setting. ‘Multimodal,’ as the name suggests, refers to any system involving two or more modes of input or output. For example, an image captioning system provides images as input and expects a textual output. Similarly, speech-to-text, descriptive art, video summarization, etc., are all examples of multimodal objectives. In this article, I will discuss 3 recent papers from &lt;a href=&#34;http://www.cs.unc.edu/~mbansal/&#34; target=&#34;_blank&#34;&gt;Mohit Bansal&lt;/a&gt; (who joined UNC last year), based on album summarization, video
captioning, and image captioning (with a twist).&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;creating-a-story-from-an-album&#34;&gt;Creating a story from an album&lt;/h4&gt;

&lt;p&gt;Given an album containing several images (which may or may not be similar), the task of Visual Storytelling is to generate a natural language story describing the album. In this EMNLP ’17 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, the task is decomposed into 3 steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Album encoder&lt;/em&gt;: Encode the individual photos in the album to form photo vectors&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Photo selector&lt;/em&gt;: Select a small number of representative photos.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Story generator&lt;/em&gt;: Compose a coherent story from the selected photo vectors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/storytelling.png&#34; alt=&#34;Architecture of the Visual Storytelling system. Image taken from original paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For each of these three components, the paper uses a hierarchically-attentive RNN. The first component is similar to an embedding layer in a text classification setting, wherein a lookup table assigns some pretrained vectors to each word and then an RNN is applied to add sentence-level information to each word vector. In a similar fashion in this paper, the initial embeddings for each image are obtained using a pretrained ResNet101 layer, and then a bidirectional RNN with GRU cells is used to add information pertaining to the entire album in every image embedding.&lt;/p&gt;

&lt;p&gt;In the Photo Selector stage, the selection is treated as a latent variable since we only have end-to-end ground truth labels. As such, we use soft attention to output $t$ probability distributions over all the images in the album, where $t$ is the number of summary images required, i.e., each image has $t$ probabilities associated with it. For this purpose, a GRU takes the previous $p$ and the previous hidden state &lt;em&gt;h&lt;/em&gt; as input and outputs the next hidden state. We use a multilayer perceptron with sigmoid activation to fuse the hidden state with the photo vector and obtain the soft attention for the particular image.&lt;/p&gt;

&lt;p&gt;$$ h_t = GRU_{select}(p_{t-1},h_{t-1}) \\\ p(y_{a_i}(t)=1) = \sigma(MLP([h_t,v_i])) $$&lt;/p&gt;

&lt;p&gt;Finally, we can obtain $t$ weighted album representations by taking the weighted sum of the photo vectors with the corresponding probability distributions. Each of these vectors is then used to decode a single sentence. For this purpose, a GRU takes the joint input of the album vector at step $t$, the previous word embedding, and the previous hidden state, and outputs the next hidden state. We repeat this for $t$ steps, thus obtaining the required album summary.&lt;/p&gt;

&lt;p&gt;How do we define loss in such a setting? First, since we already know the correct summary sentences, we can define a &lt;em&gt;generation loss&lt;/em&gt; which is simply the sum of negative log likelihoods of the correct words. However, in addition to the words being similar, the story should be temporally coherent, i.e., the sentences themselves should be in a specific order. For this purpose, we apply a max-margin ranking loss as:&lt;/p&gt;

&lt;p&gt;$$ h_t = GRU_{select}(p_{t-1},h_{t-1}) \\\ p(y_{a_i}(t)=1) = \sigma(MLP([h_t,v_i])) $$&lt;/p&gt;

&lt;p&gt;The total loss is just a linear combination of these two losses. This provides a framework for end-to-end training for the system.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;captioning-videos-using-multi-task-learning&#34;&gt;Captioning videos using multi-task learning&lt;/h4&gt;

&lt;p&gt;It seems multitask learning was under the spotlight in ACL ’17. Two semantic parsing papers I discussed in &lt;a href=&#34;https://desh2608.github.io/post/trends-in-semantic-parsing-2/&#34; target=&#34;_blank&#34;&gt;yesterday’s blog&lt;/a&gt; were both based on this paradigm, and so is this one.&lt;/p&gt;

&lt;p&gt;At this point, I would like to clarify the difference between transfer learning and multitask learning by quoting directly from &lt;a href=&#34;https://www.researchgate.net/post/What_is_the_difference_between_Multi-task_Learning_and_Transfer_Learning&#34; target=&#34;_blank&#34;&gt;this answer&lt;/a&gt; on ResearchGate:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Multi-task learning can be seen as one type of transfer learning, where the information to transfer is some inner representation/substructure of the models under consideration, or the relevant features for a prediction, and where all
the target tasks use the same data samples, but predict different target
features for these (e.g. Part Of Speech tagging and Named Entity Recognition for
natural language processing tasks).&lt;/p&gt;

&lt;p&gt;Transfer Learning, on the other hand, would be the very general problem setting, where the “what” to transfer (representation, model substructures, data samples, parameter priors, …), the concurrency of learning (one or multiple target tasks using one or multiple source tasks, or learning several tasks jointly), the differences in domain (same data or different samples, samples from same or different/related distribution, same or partially different input features) and prediction problem (same target feature or different target features/tasks, same conditional or different/related conditional) are characteristics identifying the subclass of transfer learning problem, and maybe the approach taken to address this problem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A more formal definition can be found &lt;a href=&#34;https://stats.stackexchange.com/questions/255025/difference-between-multitask-learning-and-transfer-learning&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Essentially in multitask learning, all the tasks are learnt simultaneously, whereas in transfer learning, the knowledge from one task is used in another. Now that the terminology is clear, let us look at the tasks and the model used.&lt;/p&gt;

&lt;p&gt;The objective in this paper is video captioning, and the co-learnt tasks are video prediction and language entailment generation. It is arguably difficult to obtain large amounts of annotated data for a video prediction task, and hence learning from other tasks is especially relevant in this context.&lt;/p&gt;

&lt;p&gt;Video prediction refers to the task of predicting the next frame in a video given a sequence of frames. Recognizing textual entailment (RTE), means identifying the logical relationship between two sentences, i.e., whether a premise and hypothesis follow entailment, contradiction, or independence. Knowledge transfer from a video prediction setting helps the model learn the temporal flow of information in a video, while learning from an RTE setting helps it in logically infering a caption from the video. This is the rationale behind using these tasks for the multi-task learning framework.&lt;/p&gt;

&lt;p&gt;The overall architecture of the system is given below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/captioning.png&#34; alt=&#34;Architecture of video captioning system. Image taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For each subsystem, the paper uses a simple attention-based bidirectional LSTM for the encoding and decoding purposes. This is a fine example of how a simple sequence-to-sequence block can be leveraged in different settings to perform interesting tasks.&lt;/p&gt;

&lt;h4 id=&#34;puns-in-image-captions&#34;&gt;Puns in image captions&lt;/h4&gt;

&lt;p&gt;Humor is difficult to capture or create in general. Heterographic homophones (words with different spelling but similar sound) are often used by cartoonists to add subtext to illustrations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/puns.jpg&#34; alt=&#34;Heterographic homophone used for humor in a comic. Taken from http://cartoonsbyjim.com&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, the authors have proposed 2 different methods to generate “punny” captions for images, namely a Generation model, and a Retrieval model.&lt;/p&gt;

&lt;p&gt;The Generation model works as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first step is &lt;em&gt;tagging&lt;/em&gt;. We identify the top 5 objects in the given image using an Inception-ResNet-v2 model trained on ImageNet. We also get the words from a simple caption generated for the image using a Show-and-Tell architecture. The objects and the words together are considered as tags for pun generation.&lt;/li&gt;
&lt;li&gt;We then generate a vocabulary of puns by mining the web and selecting all pairs of words with an edit distance of 0 based on articulatory features.&lt;/li&gt;
&lt;li&gt;From this pun vocabulary, we filter those puns where at least one of the homophones is related to the image in question.&lt;/li&gt;
&lt;li&gt;During the caption generation, at specific time steps, the model is forced to produce a phonological counterpart of a pun word associated with the image. The decoder generates next words based on all previously generated words.&lt;/li&gt;
&lt;li&gt;To solve the issue of non-grammatical sentences due to puns later in the sentence, two models are trained to decode the image in both forward and reverse directions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/generation.png&#34; alt=&#34;Architecture of the Generation model. Image taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Retrieval model, on the other hand, tries to find relevant captions from a prebuilt corpus of captions. This is an entirely deterministic model which requires two conditions to be satisfied:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The caption must contain the counterpart of the pun word present in the image so that incongruity is attained.&lt;/li&gt;
&lt;li&gt;The caption must be contextually relevant to the image, i.e., it must contain at least one of the &amp;ldquo;tagged&amp;rdquo; words that we found earlier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, the captions obtained from both models are pooled together and ranked by taking their log-probability score with respect to the original caption generated from the simple image captioning model. Non-maximal suppression is applied to remove captions which are similar to a higher-ranked caption, and the top 3 such obtained are retained.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;From these examples of multimodal systems, we see that simple sequence-to-sequence models work satsifactorily if used in conjuction with intelligent frameworks such as multitask learning or transfer learning, as is the trend in recent days. A cool thing is that reading about the various transfer learning approaches for this and the previous post has helped me come up with a new solution for a project that I have been working on. More on that later!&lt;/p&gt;

&lt;p&gt;Conference on Empirical Methods in Natural Language Processing*. 2017.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Yu, Licheng, Mohit Bansal, and Tamara Berg. “&lt;a href=&#34;https://arxiv.org/pdf/1708.02977.pdf&#34; target=&#34;_blank&#34;&gt;Hierarchically-Attentive RNN for Album Summarization and Storytelling&lt;/a&gt;.” *Proceedings of the 2017
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Chandrasekaran, Arjun, Devi Parikh, and Mohit Bansal. “&lt;a href=&#34;https://arxiv.org/pdf/1704.08224.pdf&#34; target=&#34;_blank&#34;&gt;Punny Captions: Witty Wordplay in Image Descriptions&lt;/a&gt;.”  &lt;em&gt;arXiv preprint arXiv:1704.08224&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Trends in Semantic Parsing - Part 2</title>
      <link>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</link>
      <pubDate>Wed, 08 Nov 2017 13:38:39 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</guid>
      <description>

&lt;p&gt;In &lt;em&gt;&lt;a href=&#34;https://desh2608.github.io/post/trends-in-semantic-parsing-1/&#34; target=&#34;_blank&#34;&gt;Part 1&lt;/a&gt;&lt;/em&gt; of this two-part series, I discussed some supervised approaches for the objective. In this part, we will look at some unsupervised or semi-supervised approaches, namely a Bayesian model, and transfer learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;an-unsupervised-bayesian-model&#34;&gt;An unsupervised Bayesian model&lt;/h4&gt;

&lt;p&gt;This paper was published in ACL 2011&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, back when statistical methods were still being used for NLP tasks. But with the recent forays into generative models, I feel it has again become relevant to understand how such methods worked. The task of frame semantic parsing can be broken down into 3 independent steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Decompose the sentence into lexical items.&lt;/li&gt;
&lt;li&gt;Divide these items into clusters and assign a label to each cluster.&lt;/li&gt;
&lt;li&gt;Predict argument-predicate relations between clusters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Frames essentially refer to a semantic representation of predicates (such as verbs), and their arguments are represented as clusters. For sake of convenience, we refer both of these structures as semantic classes. For example, in the sentences:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[India] &lt;strong&gt;defeated&lt;/strong&gt; [England].&lt;/li&gt;
&lt;li&gt;[The Indian team] &lt;strong&gt;secured a victory&lt;/strong&gt; over the [English cricket team].&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, ‘defeated’ and ‘secured a victory’ both belong to the frame WINNING, while ‘India’ and ‘Indian team’ are grouped into the cluster labeled WINNER.&lt;/p&gt;

&lt;p&gt;The authors proposed a generative algorithm which makes use of statistical processes to model semantic parsing. We can summarize the model as follows, for a particular sentence:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The distribution of semantic classes is given by a hierarchical Pitman-Yor process, i.e.,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \theta_{root} = PY(\alpha_{root},\beta_{root},\gamma). $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We start with obtaining the semantic class for the root of the tree from the probability distribution which is a sample drawn from the above Pitman-Yor process.&lt;/li&gt;
&lt;li&gt;Once the root is obtained, we call the function GenSemClass on this root.&lt;/li&gt;
&lt;li&gt;Since the current root only has a semantic class, we obtain its syntactic realization from a distribution over all possible syntactic realizations, which is given as a Dirichlet Process with the arguments as the base word and a prior.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \phi_c = DP(w^{&amp;copy;},H^{&amp;copy;}) $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Essentially, the base word $w$ is obtained from a geometric distribution, and the subsequent words are obtained by computing the conditional probability of dependency relation $r$ given $w$, and the next word $p$ given $r$.&lt;/li&gt;
&lt;li&gt;For each argument type $t$, if the probability of having at least 1 argument of type $t$ is non-zero, we generate an argument of that type using function GenArgument, until that probability becomes 0.&lt;/li&gt;
&lt;li&gt;The GenArgument function again computes the base argument from the distribution of syntactic realizations, and then obtains the next semantic class again from the hierarchical PY process.&lt;/li&gt;
&lt;li&gt;We then recursively call the GenSemClass function on this new class.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the essence of the algorithm. Basically we get a semantic frame from the PY process, and then generate the corresponding syntax from a Dirichlet process. This is done recursively, hence the need for a hierarchical PY process. For the details of the stochastic processes, you can look at their Wikipedia pages. For the root level parameters, a stick-breaking construction is used, but I am yet to look into the details of this method. However, I suppose this is similar to the broken-stick technique used to estimate the number of eigenvalues to retain in a principal component analysis.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;transfer-learning&#34;&gt;Transfer learning&lt;/h4&gt;

&lt;p&gt;There were two recent papers in ACL 2017&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; which used some kind of multi-task or transfer learning approach in a neural framework for semantic parsing.&lt;/p&gt;

&lt;p&gt;The first of these papers from Markus Dreyer at Amazon uses the popular sequence-to-sequence model developed for machine translation at Google. The sentence is first encoded into an intermediate vector representation using and encoder, and then decoded into an embedding representation for the parse tree. Popular encoders and decoders are stacked bidirectional LSTM layers, usually with some attention mechanism.&lt;/p&gt;

&lt;p&gt;Once the parse tree embedding has been obtained, the task remains to generate the actual parse tree. For this, the authors have described a COPY-WRITE mechanism. While reading the output embedding at each step, the model has 2 options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;COPY: This copies 1 symbol from the input to the output.&lt;/li&gt;
&lt;li&gt;WRITE: This selects one symbol from the vocabulary of all possible outputs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A final softmax layer generates a probability distribution over both of these choices, such that the probability of choosing WRITE at any step is proportional to an exponential over the output vector at that step, and that for choosing COPY is proportional to an exponential over a non-linear function of the intermediate representation and the output vector (i.e., the encoded and decoded vectors). The authors further describe 3 ways to extend this method in a multi-task setting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;One-to-many&lt;/em&gt;: In this, the encoder is shared but each task has its own decoder and attention parameters.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;One-to-one&lt;/em&gt;: The entire sequence is shared, with an added token at the beginning to identify the task.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;One-to-shareMany&lt;/em&gt;: This also has a shared encoder and decoder, but the final layer is independent for each task. In this way, a large number of parameters can be shared among tasks while still keeping them sufficiently distinct. Empirically, this model was found to perform best among the three.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;The second paper is from &lt;a href=&#34;https://homes.cs.washington.edu/~nasmith/&#34; target=&#34;_blank&#34;&gt;Noah Smith&lt;/a&gt;’s group at Washington. As with the previous paper, I will first describe the basic model and then explain how it is extended in a multi-task setting.&lt;/p&gt;

&lt;p&gt;Given a sentence $x$, and a set of all possible semantic graphs for that sentence $Y(x)$, we want to compute&lt;/p&gt;

&lt;p&gt;$$ \hat{y} = \text{arg}\min_{y \in Y(x)} S(x,y),~~~~ \text{where } S(x,y) = \sum_{p\in y}s(p),$$&lt;/p&gt;

&lt;p&gt;i.e., the scoring function $S$ is a sum of local scores, each of which is itself a parametrized function of some local feature. In this paper, these features are taken to be the following 3 constructs (first order logic):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Predicate&lt;/li&gt;
&lt;li&gt;Unlabeled arc&lt;/li&gt;
&lt;li&gt;Labeled arc&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The model is given in the following diagram taken from the paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/6/multitask.png&#34; alt=&#34;Basic architecture. Figure taken from the original paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the 2 input words, we first obtain vectors using a bi-LSTM layer, and these are then fed into multilayer perceptrons (MLPs) corresponding to each of the three local feature constructs mentioned above. Each first-order structure is itself associated with a vector (shown in red). The scoring function $s(p)$ is simply the dot product of the MLPs output and the first-order vector.&lt;/p&gt;

&lt;p&gt;The cost function is a max-margin objective with a regularization parameter and a sum over individual losses given as&lt;/p&gt;

&lt;p&gt;$$ L(x_i,y_i,\theta) = \max_{y\in Y(x_i)} S(x_i,y) + c(y,y_i) - S(x_i,y_i). $$&lt;/p&gt;

&lt;p&gt;Here, $y_i$ is the gold label output and $y$ is the obtained output, while $c$ is the weighted Hamming distance between the two outputs.&lt;/p&gt;

&lt;p&gt;Once this basic architecture is in place, the authors describe 2 method to extend it with transfer learning. The tasks here are 3 different formalisms in semantic dependency parsing (Delph-in MRS, Predicate-Argument Structure, and Prague Semantic Dependencies), so that each of these require a different variation of the output form. In the first method, the representation is shared among all tasks but the scoring is done separately. This further has variants wherein we can either have a single common bi-LSTM for all tasks, or a concatenation of independent and common layers.&lt;/p&gt;

&lt;p&gt;The second method describes a joint technique to perform representation and inference learning across all the tasks simultaneously. The description is mathematically involved but intuitively simple, since we are just expressing the inner product in the scoring function in a higher dimension. You can look at the original paper for details and notation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;With this, we come to the end of this series on semantic parsing. Since a lot of models are common between different objectives, these methods are highly relevant across any NLP task, especially with a shift from supervised to unsupervised techniques. While writing this article, I have been thinking of ways of adapting the generative model from the Bayesian paper to a neural architecture, and I might read up more about this in the coming weeks. Till then, keep “learning”!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Titov, Ivan, and Alexandre Klementiev. “&lt;a href=&#34;http://klementiev.org/publications/acl11.pdf&#34; target=&#34;_blank&#34;&gt;A Bayesian model for unsupervised semantic parsing&lt;/a&gt;.” &lt;em&gt;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1&lt;/em&gt;. Association for Computational Linguistics, 2011.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Fan, Xing, et al. “&lt;a href=&#34;https://arxiv.org/pdf/1706.04326.pdf&#34; target=&#34;_blank&#34;&gt;Transfer Learning for Neural Semantic Parsing&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1706.04326&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Peng, Hao, Sam Thomson, and Noah A. Smith. “&lt;a href=&#34;https://arxiv.org/pdf/1704.06855.pdf&#34; target=&#34;_blank&#34;&gt;Deep Multitask Learning for Semantic Dependency Parsing&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1704.06855&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
