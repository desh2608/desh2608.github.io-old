<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>semantic parsing on Desh Raj</title>
    <link>https://desh2608.github.io/tags/semantic-parsing/</link>
    <description>Recent content in semantic parsing on Desh Raj</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Wed, 08 Nov 2017 13:38:39 +0530</lastBuildDate>
    
	    <atom:link href="https://desh2608.github.io/tags/semantic-parsing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Trends in Semantic Parsing - Part 2</title>
      <link>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</link>
      <pubDate>Wed, 08 Nov 2017 13:38:39 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</guid>
      <description>

&lt;p&gt;In &lt;em&gt;&lt;a href=&#34;https://desh2608.github.io/post/trends-in-semantic-parsing-1/&#34; target=&#34;_blank&#34;&gt;Part 1&lt;/a&gt;&lt;/em&gt; of this two-part series, I discussed some supervised approaches for the objective. In this part, we will look at some unsupervised or semi-supervised approaches, namely a Bayesian model, and transfer learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;an-unsupervised-bayesian-model&#34;&gt;An unsupervised Bayesian model&lt;/h4&gt;

&lt;p&gt;This paper was published in ACL 2011&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, back when statistical methods were still being used for NLP tasks. But with the recent forays into generative models, I feel it has again become relevant to understand how such methods worked. The task of frame semantic parsing can be broken down into 3 independent steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Decompose the sentence into lexical items.&lt;/li&gt;
&lt;li&gt;Divide these items into clusters and assign a label to each cluster.&lt;/li&gt;
&lt;li&gt;Predict argument-predicate relations between clusters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Frames essentially refer to a semantic representation of predicates (such as verbs), and their arguments are represented as clusters. For sake of convenience, we refer both of these structures as semantic classes. For example, in the sentences:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[India] &lt;strong&gt;defeated&lt;/strong&gt; [England].&lt;/li&gt;
&lt;li&gt;[The Indian team] &lt;strong&gt;secured a victory&lt;/strong&gt; over the [English cricket team].&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, ‘defeated’ and ‘secured a victory’ both belong to the frame WINNING, while ‘India’ and ‘Indian team’ are grouped into the cluster labeled WINNER.&lt;/p&gt;

&lt;p&gt;The authors proposed a generative algorithm which makes use of statistical processes to model semantic parsing. We can summarize the model as follows, for a particular sentence:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The distribution of semantic classes is given by a hierarchical Pitman-Yor process, i.e.,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \theta_{root} = PY(\alpha_{root},\beta_{root},\gamma). $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We start with obtaining the semantic class for the root of the tree from the probability distribution which is a sample drawn from the above Pitman-Yor process.&lt;/li&gt;
&lt;li&gt;Once the root is obtained, we call the function GenSemClass on this root.&lt;/li&gt;
&lt;li&gt;Since the current root only has a semantic class, we obtain its syntactic realization from a distribution over all possible syntactic realizations, which is given as a Dirichlet Process with the arguments as the base word and a prior.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \phi_c = DP(w^{&amp;copy;},H^{&amp;copy;}) $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Essentially, the base word $w$ is obtained from a geometric distribution, and the subsequent words are obtained by computing the conditional probability of dependency relation $r$ given $w$, and the next word $p$ given $r$.&lt;/li&gt;
&lt;li&gt;For each argument type $t$, if the probability of having at least 1 argument of type $t$ is non-zero, we generate an argument of that type using function GenArgument, until that probability becomes 0.&lt;/li&gt;
&lt;li&gt;The GenArgument function again computes the base argument from the distribution of syntactic realizations, and then obtains the next semantic class again from the hierarchical PY process.&lt;/li&gt;
&lt;li&gt;We then recursively call the GenSemClass function on this new class.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the essence of the algorithm. Basically we get a semantic frame from the PY process, and then generate the corresponding syntax from a Dirichlet process. This is done recursively, hence the need for a hierarchical PY process. For the details of the stochastic processes, you can look at their Wikipedia pages. For the root level parameters, a stick-breaking construction is used, but I am yet to look into the details of this method. However, I suppose this is similar to the broken-stick technique used to estimate the number of eigenvalues to retain in a principal component analysis.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;transfer-learning&#34;&gt;Transfer learning&lt;/h4&gt;

&lt;p&gt;There were two recent papers in ACL 2017&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;,&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; which used some kind of multi-task or transfer learning approach in a neural framework for semantic parsing.&lt;/p&gt;

&lt;p&gt;The first of these papers from Markus Dreyer at Amazon uses the popular sequence-to-sequence model developed for machine translation at Google. The sentence is first encoded into an intermediate vector representation using and encoder, and then decoded into an embedding representation for the parse tree. Popular encoders and decoders are stacked bidirectional LSTM layers, usually with some attention mechanism.&lt;/p&gt;

&lt;p&gt;Once the parse tree embedding has been obtained, the task remains to generate the actual parse tree. For this, the authors have described a COPY-WRITE mechanism. While reading the output embedding at each step, the model has 2 options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;COPY: This copies 1 symbol from the input to the output.&lt;/li&gt;
&lt;li&gt;WRITE: This selects one symbol from the vocabulary of all possible outputs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A final softmax layer generates a probability distribution over both of these choices, such that the probability of choosing WRITE at any step is proportional to an exponential over the output vector at that step, and that for choosing COPY is proportional to an exponential over a non-linear function of the intermediate representation and the output vector (i.e., the encoded and decoded vectors). The authors further describe 3 ways to extend this method in a multi-task setting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;One-to-many&lt;/em&gt;: In this, the encoder is shared but each task has its own decoder and attention parameters.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;One-to-one&lt;/em&gt;: The entire sequence is shared, with an added token at the beginning to identify the task.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;One-to-shareMany&lt;/em&gt;: This also has a shared encoder and decoder, but the final layer is independent for each task. In this way, a large number of parameters can be shared among tasks while still keeping them sufficiently distinct. Empirically, this model was found to perform best among the three.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;The second paper is from &lt;a href=&#34;https://homes.cs.washington.edu/~nasmith/&#34; target=&#34;_blank&#34;&gt;Noah Smith&lt;/a&gt;’s group at Washington. As with the previous paper, I will first describe the basic model and then explain how it is extended in a multi-task setting.&lt;/p&gt;

&lt;p&gt;Given a sentence $x$, and a set of all possible semantic graphs for that sentence $Y(x)$, we want to compute&lt;/p&gt;

&lt;p&gt;$$ \hat{y} = \text{arg}\min_{y \in Y(x)} S(x,y),~~~~ \text{where } S(x,y) = \sum_{p\in y}s(p),$$&lt;/p&gt;

&lt;p&gt;i.e., the scoring function $S$ is a sum of local scores, each of which is itself a parametrized function of some local feature. In this paper, these features are taken to be the following 3 constructs (first order logic):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Predicate&lt;/li&gt;
&lt;li&gt;Unlabeled arc&lt;/li&gt;
&lt;li&gt;Labeled arc&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The model is given in the following diagram taken from the paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/6/multitask.png&#34; alt=&#34;Basic architecture. Figure taken from the original paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the 2 input words, we first obtain vectors using a bi-LSTM layer, and these are then fed into multilayer perceptrons (MLPs) corresponding to each of the three local feature constructs mentioned above. Each first-order structure is itself associated with a vector (shown in red). The scoring function $s(p)$ is simply the dot product of the MLPs output and the first-order vector.&lt;/p&gt;

&lt;p&gt;The cost function is a max-margin objective with a regularization parameter and a sum over individual losses given as&lt;/p&gt;

&lt;p&gt;$$ L(x_i,y_i,\theta) = \max_{y\in Y(x_i)} S(x_i,y) + c(y,y_i) - S(x_i,y_i). $$&lt;/p&gt;

&lt;p&gt;Here, $y_i$ is the gold label output and $y$ is the obtained output, while $c$ is the weighted Hamming distance between the two outputs.&lt;/p&gt;

&lt;p&gt;Once this basic architecture is in place, the authors describe 2 method to extend it with transfer learning. The tasks here are 3 different formalisms in semantic dependency parsing (Delph-in MRS, Predicate-Argument Structure, and Prague Semantic Dependencies), so that each of these require a different variation of the output form. In the first method, the representation is shared among all tasks but the scoring is done separately. This further has variants wherein we can either have a single common bi-LSTM for all tasks, or a concatenation of independent and common layers.&lt;/p&gt;

&lt;p&gt;The second method describes a joint technique to perform representation and inference learning across all the tasks simultaneously. The description is mathematically involved but intuitively simple, since we are just expressing the inner product in the scoring function in a higher dimension. You can look at the original paper for details and notation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;With this, we come to the end of this series on semantic parsing. Since a lot of models are common between different objectives, these methods are highly relevant across any NLP task, especially with a shift from supervised to unsupervised techniques. While writing this article, I have been thinking of ways of adapting the generative model from the Bayesian paper to a neural architecture, and I might read up more about this in the coming weeks. Till then, keep “learning”!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Titov, Ivan, and Alexandre Klementiev. “&lt;a href=&#34;http://klementiev.org/publications/acl11.pdf&#34; target=&#34;_blank&#34;&gt;A Bayesian model for unsupervised semantic parsing&lt;/a&gt;.” &lt;em&gt;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1&lt;/em&gt;. Association for Computational Linguistics, 2011.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Fan, Xing, et al. “&lt;a href=&#34;https://arxiv.org/pdf/1706.04326.pdf&#34; target=&#34;_blank&#34;&gt;Transfer Learning for Neural Semantic Parsing&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1706.04326&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Peng, Hao, Sam Thomson, and Noah A. Smith. “&lt;a href=&#34;https://arxiv.org/pdf/1704.06855.pdf&#34; target=&#34;_blank&#34;&gt;Deep Multitask Learning for Semantic Dependency Parsing&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1704.06855&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Trends in Semantic Parsing - Part 1</title>
      <link>https://desh2608.github.io/post/trends-in-semantic-parsing-1/</link>
      <pubDate>Wed, 20 Sep 2017 10:03:22 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/trends-in-semantic-parsing-1/</guid>
      <description>

&lt;p&gt;&lt;em&gt;In this article, I will try to round up some (mostly neural) approaches for semantic parsing and semantic role labeling (SRL). This is not an extensive review of these methods, but just a collection of my notes on reading some recent research on the subject. However, I do believe it covers most of the latest trends as well as their limitations.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;But first, &lt;strong&gt;what is semantic parsing?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Semantic” refers to meaning, and “parsing” means resolving a sentence into its component parts. As such, semantic parsing refers to the task of mapping natural language text to formal representations or abstractions of its meaning. A &lt;em&gt;syntactic parser&lt;/em&gt; may generate constituency or dependency trees from a sentence, but a &lt;em&gt;semantic parser&lt;/em&gt; may be built depending upon the task for which inference is required.&lt;/p&gt;

&lt;p&gt;For example, we can build a parser that converts the natural language query “*Who was the first person to walk on the moon?*” to an equivalent (although complex!) SQL query such as “SELECT name FROM Person WHERE moon_walk = true ORDER BY moon_walk_date FETCH first 1 rows only.”&lt;/p&gt;

&lt;p&gt;Semantic parsing is inherently more complicated than syntactic parsing because it requires understanding concepts from different word phrases. For instance, the following sentences (adapted from [4]) should ideally map to the same formal representation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Sentence 1: India defeated Australia.&lt;/p&gt;

&lt;p&gt;Sentence 2: India secured the victory over the Australian team.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For this reason, semantic parsing is more about capturing the meaning of the sentence rather than plain rule-based pattern matching.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Semantic role labeling&lt;/strong&gt; is a sub-task within the former, where the sentence is parsed into a predicate-argument format. The example given on the Wikipedia page for SRL explains this well. Given a sentence like “Mary sold the book to John,” the task would be to recognize the verb “to sell” as representing the predicate, “Mary” as representing the seller (agent), “the book” as representing the goods (theme), and “John” as representing the recipient. In this sense, SRL is sometimes also called shallow semantic parsing because the structure of the target representation is somewhat known.&lt;/p&gt;

&lt;p&gt;In this article, I will describe models for both these tasks without explicit differentiation, mostly since the same models are found to work well on either task.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;learning-sentence-embeddings-using-deep-neural-models&#34;&gt;Learning sentence embeddings using deep neural models&lt;/h4&gt;

&lt;p&gt;Vector semantics have been used extensively in all NLP tasks, especially after word embeddings (Word2Vec, GloVe) were found to represent the synonymy-antonymy relations well in real space.&lt;/p&gt;

&lt;p&gt;Similar to word embeddings, we can try to obtain dense vectors to represent a sentence, and then find some way to obtain the formal representation from it. Ivan Titov (University of Edinburgh) has recently proposed a couple of models which use &lt;strong&gt;LSTMs&lt;/strong&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;strong&gt;Graph CNNs&lt;/strong&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; for dependency-based SRL task.&lt;/p&gt;

&lt;p&gt;I will first explain the task. We work on datasets where the predicates are marked in the sentence, and the objective is to identify and label the arguments corresponding to each predicate. For instance, given the sentence “&lt;em&gt;Mary eats an apple&lt;/em&gt;,” and the predicate marked as EATS, we need to label the words ‘Mary,’ ‘an,’ and ‘apple’ as &lt;em&gt;agent&lt;/em&gt;, NULL, and &lt;em&gt;theme&lt;/em&gt;, respectively. Also, since a single sentence may contain multiple predicates, the same word may get different labels for each predicate. Essentially, if we repeat the process once for each predicate, out task effectively reduces to a sequence labeling problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LSTM-based approach&lt;/strong&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; : LSTMs (which are a type of RNNs that can preserve memory) have been used to model sequences since they were first introduced. In the first model, the sequence labeling is performed as follows.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Vectors are obtained from each word by concatenating pre-trained embeddings (Word2Vec), random embeddings, and randomly initialized POS embeddings.&lt;/li&gt;
&lt;li&gt;The word vector also contains a 1-bit flag to mark whether it is the predicate in that particular training instance. This is done to ensure that the network treats each predicate differently.&lt;/li&gt;
&lt;li&gt;These are fed into a bi-LSTM layer to obtain the word’s context in the sentence.&lt;/li&gt;
&lt;li&gt;Finally, to label any word, we take the dot product of its hidden state with the predicate’s hidden state and obtain a softmax classifier over it as follows.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ p(r|v_i,v_p) \propto \exp(W_r (v_i \cdot v_p)). $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Further, we can have the weight matrix parametrized on the role label $r$ as:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ W_{l,r} = ReLU(U(u_l \cdot v_r)), $$&lt;/p&gt;

&lt;p&gt;where the vectors in the dot product correspond to randomly initialized embeddings for the predicate lemma and the role, respectively.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GCN-based approach &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/strong&gt; In a second model, Graph Convolutional Networks (GCNs) have been used to represent the dependency tree for the sentence. In a very crude sense, a GCN input layer encodes the sentence into an $m X n$ matrix based on its dependency tree, such that each of the $n$ nodes of the tree is
represented as an $m$-dimensional vector. Once such a matrix has been obtained, we can perform convolutions on it.&lt;/p&gt;

&lt;p&gt;It is then evident that a one-layer GCN can capture information only about its immediate neighbor. By stacking GCN layers, one can incorporate higher degree neighborhoods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/2/gcn.png&#34; alt=&#34;Architecture of an LSTM+GCN encoder&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GCNs and LSTMs are complementary.&lt;/strong&gt; &lt;em&gt;Why?&lt;/em&gt; LSTMs capture long-term dependencies well but are not able to represent syntax effectively. On the other hand, GCNs are built directly on top of a syntactic-dependency tree so they capture syntax well, but due to the limitation of fixed-size convolutions, the range of dependency is limited. Therefore, using a GCN layer on top of the hidden states obtained from a bi-LSTM layer would theoretically capture the best of both worlds. This hypothesis has also been corroborated through experimental results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encoder-decoder model&lt;/strong&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;: In this paper, the task is broadened into formal representation rather than SRL. If we consider the formal representation as a different language, this is similar to a machine translation problem, since both the natural as well as formal representations mean the same. As such, it might be interesting to apply models used for MT to semantic parsing. This paper does exactly this.&lt;/p&gt;

&lt;p&gt;An encoder converts the input sequence to a vector representation and a decoder obtains the target sequence from this vector.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The encoder uses a bi-LSTM layer similar to the previous methods to obtain the vector representation of the input sequence.&lt;/li&gt;
&lt;li&gt;The final hidden state is fed into the decoder layer, which is again a bi-LSTM. The hidden states obtained from this layer is used to predict the corresponding output tokens using a softmax function.&lt;/li&gt;
&lt;li&gt;Alternatively, we can have a hierarchical decoder to account for the hierarchical structure of logical forms. For this purpose, we simply introduce a non-terminal token, say &lt;n&gt;, which indicates the start of a sub-tree. Other tokens may be used to represent the start/end of a terminal sequence or a non-terminal sequence.&lt;/li&gt;
&lt;li&gt;To incorporate the tree structure, we concatenate the hidden state of the parent non-terminal with every child.&lt;/li&gt;
&lt;li&gt;Finally in the decoding step, to better utilize relevant information from the input sequence, we use an attention layer where the context vector is a weighted sum over the hidden vectors in the encoder.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;While these models are very inspired and intuitive, they are all supervised. As such, they are constrained due to cost and availability of annotated data, especially since manually labeling semantic parsing output is a time-consuming process. In part 2 of this article, I will talk about some approaches which overcome this issue.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Marcheggiani, Diego, Anton Frolov, and Ivan Titov. “&lt;a href=&#34;https://arxiv.org/pdf/1701.02593.pdf&#34; target=&#34;_blank&#34;&gt;A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1701.02593&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Marcheggiani, Diego, and Ivan Titov. “&lt;a href=&#34;https://arxiv.org/pdf/1703.04826.pdf&#34; target=&#34;_blank&#34;&gt;Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1703.04826&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Dong, Li, and Mirella Lapata. “&lt;a href=&#34;https://arxiv.org/pdf/1601.01280.pdf&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1601.01280.pdf&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1601.01280&lt;/em&gt; (2016).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
