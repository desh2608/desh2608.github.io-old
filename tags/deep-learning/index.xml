<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on Desh Raj</title>
    <link>https://desh2608.github.io/tags/deep-learning/</link>
    <description>Recent content in deep learning on Desh Raj</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Tue, 31 Jul 2018 18:45:15 +0530</lastBuildDate>
    
	    <atom:link href="https://desh2608.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Theory of Deep Learning: An Illustration with Embeddings</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-5/</link>
      <pubDate>Tue, 31 Jul 2018 18:45:15 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-5/</guid>
      <description>

&lt;p&gt;We have discussed several aspects of deep learning theory, ranging from &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;optimization&lt;/a&gt; and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;generalization guarantees&lt;/a&gt; to &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-3/&#34; target=&#34;_blank&#34;&gt;role of depth&lt;/a&gt; and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-4/&#34; target=&#34;_blank&#34;&gt;generative models&lt;/a&gt;. In this final post of this series, I will illustrate how theory can motivate simple solutions to problems, which can then outperform complex techniques. For this, we will consider a field where deep learning has done exceptionally well, namely, word and sentence embeddings.&lt;/p&gt;

&lt;p&gt;If you need a refresher on word embeddings, I have previously explained them, along with the most popular methods, in &lt;a href=&#34;https://desh2608.github.io/post/understanding-word-vectors/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. The &lt;em&gt;distributional hypothesis&lt;/em&gt; forms the basis for all word embedding techniques used at present. Instead of naively taking the co-occurence matrix, though, almost all techniques use some low-rank approximation for the same. This gives rise to low-dimensional ($\sim 300$) dense embeddings for text. An important question, then, is the following: How can low-dimensional embeddings represent the complex linguistic structure in text? We will first look at this question from a theoretical perspective, based on &lt;a href=&#34;http://aclweb.org/anthology/Q16-1028&#34; target=&#34;_blank&#34;&gt;this ACL&amp;rsquo;16 paper&lt;/a&gt; from Arora et al.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;how-do-low-dimensional-embeddings-approximate-co-occurence-matrices&#34;&gt;How do low-dimensional embeddings approximate co-occurence matrices?&lt;/h3&gt;

&lt;p&gt;Formally, we want to see why, for some low-dimensional vector representations $v$, we have&lt;/p&gt;

&lt;p&gt;$$ \langle v_w,v_{w^{\prime}} \rangle \approx \text{PMI}(w,w^{\prime}), $$&lt;/p&gt;

&lt;p&gt;where $\text{PMI}(w,w^{\prime})$ is the pointwise mutual information between $w$ and $w^{\prime}$, defined as $\log \frac{P(w,w^{\prime})}{P(w)P(w^{\prime})}$, where the probabilities are computed empirically from the co-occurence matrix.&lt;/p&gt;

&lt;p&gt;For this, the authors propose a generative model of language, as opposed to the usual discriminative model that is based on predicting the context words given a target word (i.e., multiclass classification). This is based on the random walk of a discourse vector $c_t \in \mathcal{R}^d$, which generates $t$th word in step $t$. Every word has a time-invariant latent vector $v_w \in \mathcal{R}^d$, and the word production model is given as&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted at time} ~ t|c_t] \propto \exp(\langle c_t,v_w \rangle). $$&lt;/p&gt;

&lt;p&gt;Here, &lt;em&gt;random walk&lt;/em&gt; means that $c_{t+1}$ is obtained by adding a small random displacement vector to $c_t$. For a theoretic analysis, we make an isotropy assumption about the word vectors.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Isotropy assumption&lt;/strong&gt;: In the bulk, word vectors are distributed uniformly in the $\mathcal{R}^d$ space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To generate such a dsitribution, we can just sample i.i.d from $v = s \cdot v^{\prime}$, where $s$ is a scalar random variable ($s \leq \kappa$), and $v^{\prime}$ is obtained from a spherical Gaussian distribution. This is a simple Bayesian prior similar to the assumptions commonly used in statistics.&lt;/p&gt;

&lt;p&gt;Let us define $Z_c = \sum_{w}\exp(\langle v_w,c \rangle)$. This is like the normalization factor used with the above equation, but it is very difficult to compute. In the paper, the authors prove that this value is very close to some constant $Z$ for a fixed $c$. This allows us to remove this factor from consideration. Empirically, it has also been seen that some log-linear models have self-normalization properties, and this may be a reason for the observation. Let us now see how to prove this lemma.&lt;/p&gt;

&lt;p&gt;Since $Z_c$ is a sum of random variables, it may be tempting to use concentration inequalities to bound its value. However, we cannot do this since $Z_c$ is neither sub-Gaussian nor sub-exponential. We approach the problem it two parts. First we bound the mean and variance of $Z_c$, and then show that it is concentrated around its mean.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 1:&lt;/strong&gt; Suppose there are $n$ vectors in our space. Since they are identically distributed, we have&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[Z_c] = n\mathbb{E}[\exp(\langle v_w,c \rangle)] \geq n\mathbb{E}[1 + \langle v_w,c \rangle] = n. $$&lt;/p&gt;

&lt;p&gt;Here, we have used $\mathbb{E}[\langle v_w,c \rangle] = 0$, since $v_w$&amp;rsquo;s are drawn from a scaled uniform spherical Gaussian. Now, suppose all the scalar variables $s_w$ are equal in distribution to $s$. Then, we can write&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[Z_c] = n\mathbb{E}[\exp(\langle v_w,c \rangle)] = n\mathbb{E}\left[ \mathbb{E} [\exp(\langle v_w,c \rangle)|s]\right]. $$&lt;/p&gt;

&lt;p&gt;We can compute the conditional expectation as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E} [\exp(\langle v_w,c \rangle)|s] &amp;amp;= \int_x \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{x^2}{2\sigma^2} \right)\exp(x) dx \\\ &amp;amp;= \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{(x-\sigma^2)^2}{2\sigma^2} + \frac{\sigma^2}{2}\right) dx \\\ &amp;amp;= \exp(\frac{\sigma^2}{2}). \end{align} $$&lt;/p&gt;

&lt;p&gt;Here, the standard deviation is equal to the scaling factor $s$, and so $\sigma^2 = s^2$. It follows that&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}(Z_c) = n\exp(\frac{s^2}{2}). $$&lt;/p&gt;

&lt;p&gt;Similarly, we can show that the variance&lt;/p&gt;

&lt;p&gt;$$ \mathbb{V}(Z_c) \leq n\mathbb{E}[\exp(2s^2)]. $$&lt;/p&gt;

&lt;p&gt;Since $\langle v_w,c \rangle|s$ has a Gaussian distribution with variance $s^2 \leq \kappa^2$, we have using Chernoff bounds that&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[|\langle v_w,c \rangle| \geq \eta \log n |s] \leq \exp \left( - \frac{\eta^2 \log^2 n}{2\kappa^2} \right) = \exp (-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;Here we have removed $\eta$ and $\kappa$ since they are constants. We can now write the converse of this inequality, by taking expectation over all $s_w$, as&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[|\langle v_w,c \rangle| \leq \frac{1}{2}\log n] \geq 1 - \exp(-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;This means that, with high probability, $|\langle v_w,c \rangle| \leq \frac{1}{2}\log n$, or equivalently, $\exp(\langle v_w,c \rangle) \leq \sqrt{n}$. Now, let the random variable $X_w$ have the same distribution as $\exp(\langle v_w,c \rangle)$ when the above holds.&lt;/p&gt;

&lt;p&gt;Let us take a minute to understand what we are doing here. We do not know how to bound the original $Z_c$, since $\exp(\langle v_w,c \rangle)$ has no known concentration bounds. So we approximate it by a new random variable with high probability, so that we can compute bounds on the sum. Now, let $Z_{c}^{\prime} = \sum_{w}X_w$. We will now try to bound the mean and variance for this random variable.&lt;/p&gt;

&lt;p&gt;Computing the lower bound for the mean is simple since the mean of $\exp(\langle v_w,c \rangle)$ is zero, and so $\mathbb{E}[Z_c^{\prime}] \leq n$. We can similarly bound the variance as $\mathbb{V}[Z_c^{\prime}] \leq 1.1 \Lambda n$, where $\Lambda$ is a constant. Now, using Bernstein&amp;rsquo;s inequality, we get&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}\left[ | Z_c^{\prime} - \mathbb{E}[Z_c^{\prime}] | \geq \epsilon n \right] \leq \exp(-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;Since $Z_c$ has the same distribution as $Z_c^{\prime}$, the above inequality also holds for the former. This means that the probability of $Z_c$ deviating from its mean is very low, and so we can say with high probability that&lt;/p&gt;

&lt;p&gt;$$ (1-\epsilon_z)Z \leq Z_c \leq (1+\epsilon_z)Z. $$&lt;/p&gt;

&lt;p&gt;The above proof was just to remove the normalization factor as a constant from the original problem, so that analysis becomes easier. We now come to the main result itself. Suppose $c$ and $c^{\prime}$ are consecutive discourse vectors and $w$ and $w^{\prime}$ are words generated from them. We have&lt;/p&gt;

&lt;p&gt;$$ \begin{align} p(w,w^{\prime}) &amp;amp;= \mathbb{E}_{c,c^{\prime}}[\text{Pr}[w,w^{\prime}|c,c^{\prime}]] \\\ &amp;amp;= \mathbb{E}_{c,c^{\prime}}[p(w|c)p(w^{\prime}|c^{\prime})] \\\ &amp;amp;= \mathbb{E}_{c,c^{\prime}}\left[ \frac{\exp(\langle v_w,c \rangle)}{Z_c}\right] \frac{\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)}{Z_{c^{\prime}}}. \end{align} $$&lt;/p&gt;

&lt;p&gt;As proved above, we can approximate the denominators to $Z$ and take them out of the expectation. This gives&lt;/p&gt;

&lt;p&gt;$$ \begin{align} p(w,w^{\prime}) &amp;amp;= \frac{1}{Z^2}\mathbb{E}_{c,c^{\prime}}[\exp(\langle v_w,c \rangle)\exp(\langle v_{w^{\prime}},c^{\prime} \rangle))] \\\ &amp;amp;= \frac{1}{Z^2}\mathbb{E}_c [\exp(\langle v_w,c \rangle)\mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)]]. \end{align}. $$&lt;/p&gt;

&lt;p&gt;We can compute the internal expectation term as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)] &amp;amp;= \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} - c + c \rangle)] \\\ &amp;amp;= \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} -c \rangle)]\exp(\langle v_{w^{\prime}},c \rangle) \\\ &amp;amp;\approx \exp(\langle v_{w^{\prime}},c \rangle). \end{align}$$&lt;/p&gt;

&lt;p&gt;Here, the last approximation can be done because we have assumed that our random walk has small steps, i.e., $|c^{\prime} - c|$ is small. Using this in above, we get&lt;/p&gt;

&lt;p&gt;$$ p(w,w^{\prime}) = \frac{1}{Z^2}\mathbb{E}[\exp(\langle v_w + v_{w^{\prime}},c \rangle)]. $$&lt;/p&gt;

&lt;p&gt;Since $c$ has uniform distribution over the sphere, the above resembles a Gaussian centered at 0 and variance $\frac{\lVert  v_w + v_{w^{\prime}} \rVert^2}{d}$. Since $\mathbb{E}[\exp(X)] = \exp(\frac{\sigma^2}{2})$ for $X \sim \mathcal{N}(0,\sigma^2)$, we get the closed form expression as&lt;/p&gt;

&lt;p&gt;$$ p(w,w^{\prime}) = \frac{1}{Z^2}\exp\left( \frac{\lVert  v_w + v_{w^{\prime}} \rVert^2}{2d} \right), $$&lt;/p&gt;

&lt;p&gt;which is the desired result. Note that I have ignored some technicalities for error bounds in this proof. We have now shown the original result that we wanted, but how did dimensionality help?&lt;/p&gt;

&lt;p&gt;The answer lies in the &lt;em&gt;isotropy assumption&lt;/em&gt; that we made at the very beginning. Having $n$ vectors be isotropic in $d$ dimensions requires $d &amp;lt;&amp;lt; n$, which is indeed what is observed empirically. Hence, theory justifies experimental findings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;an-algorithm-for-sentence-embeddings&#34;&gt;An algorithm for sentence embeddings&lt;/h3&gt;

&lt;p&gt;In a previous part of this series, I echoed Prof. Arora&amp;rsquo;s concern that theoretical analysis at present is like a postmortem analysis, where we try to find properties of the model that can explain certain empirical findings. The ideal scenario would be where we can use this understanding to guide future learning models. In this section, I will look at &lt;a href=&#34;https://openreview.net/pdf?id=SyK00v5xx&#34; target=&#34;_blank&#34;&gt;this paper from ICLR&amp;rsquo;17&lt;/a&gt; which uses the understanding from the previous section to build simple but strong word embeddings.&lt;/p&gt;

&lt;p&gt;Suppose we want to obtain the vector for a piece of text, say, a sentence. From our generative model defined in the previous section, it would be reasonable to say that this can be approximated by a &lt;em&gt;max a priori&lt;/em&gt; (MAP) estimate of the discourse vector that generated the sentence, i.e.,&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted in sentence} ~ s | c_s] = \frac{\exp(\langle c_s,v_w \rangle)}{Z_{c_s}}, $$&lt;/p&gt;

&lt;p&gt;where $c_s$ is the discourse vector that remains approximately constant for the sentence. However, we need to modify this slightly to account for two real situations.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Some words often appear out of context, and some stop words appear regardless of discourse. To approximate this, we add a term $\alpha p(w)$ to the log-linear model, where $p(w)$ is the unigram probability of the word. This makes probability of appearance of some words high even if they have low correlation with the discourse vector.&lt;/li&gt;
&lt;li&gt;Generation of words depends not just on current sentence, but on entire history of discourse. To model this, we use discourse vector $\tilde{c}_s = \beta c_0 + (1-\beta)c_s$, where $c_0$ is the common discourse vector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, the modified log-linear objective is as follows.&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted in sentence} ~ s | c_s] = \alpha p(w) + (1-\alpha) \frac{\exp(\langle \tilde{c}_s,v_w \rangle)}{Z_{\tilde{c}_s}} $$&lt;/p&gt;

&lt;p&gt;After the word embeddings have been trained using this objective, we can model the likelihood for obtaining sentence $s$ given discourse vector $c_s$ as&lt;/p&gt;

&lt;p&gt;$$ p[s|c_s] = \prod_{w\in s}p(w|c_s) = \prod_{w\in s}\left[ \alpha p(w) + (1-\alpha) \frac{\exp(\langle \tilde{c}_s,v_w \rangle)}{Z} \right]. $$&lt;/p&gt;

&lt;p&gt;Here, we have taken $Z_{\tilde{c}_s} = Z$, in accordance with the result we proved earlier. To maximize this expression, we just need to maximize the term inside the product. Taking $f_w(\tilde{c}_s)$ to denote the term inside the product, we can easily compute its derivative, and then use Taylor expansion, $f_w(\tilde{c}_s) = f_w(0) + \nabla f_w(\tilde{c}_s)^T \tilde{c}_s$, to get an expression for $f_w(\tilde{c}_s)$. Finally, we have&lt;/p&gt;

&lt;p&gt;$$ \text{arg}\max\sum_{w\in s}f_w(\tilde{c}_s) \propto \sum_{w\in s}\frac{a}{p(w)+a}v_w, $$&lt;/p&gt;

&lt;p&gt;where $a = \frac{1-\alpha}{\alpha Z}$. If we analyze this expression, this is simply a weighted sum of the word vectors in the sentence, which is one of the most common bag-of-words technique to obtain sentence embeddings. Furthermore, the weight is low if the unigram frequency of the word is high. This is similar to Tf-idf weighting of words. Now, this theory gives rise to the following algorithm, taken from the original paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/25/sif.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is a striking illustration of how rigorously developed theoretical results can guide construction of simple algorithms in deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Final note:&lt;/strong&gt; This series was based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, which is why the discussion revolved mostly around the work done by his group. The papers themselves are not very trivial to understand, but the &lt;a href=&#34;www.offconvex.org&#34; target=&#34;_blank&#34;&gt;blog posts&lt;/a&gt; are more beginner friendly, and highly recommended. Several people criticize deep learning for being purely intuition-based, but I believe that will change soon, given that so much good research is being done to develop a theory for it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Generative Models</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-4/</link>
      <pubDate>Mon, 30 Jul 2018 23:39:37 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-4/</guid>
      <description>

&lt;p&gt;Till now, in this series based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, we have limited our discussion to the theory of supervised discriminative neural models, i.e., those models which learn the conditional probability $P(y|x)$ from a set of given $(x_i,y_i)$ samples. In particular, we saw &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;how deep networks find good solutions&lt;/a&gt;, &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;why they generalize well&lt;/a&gt; despite being overparametrized, and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-3/&#34; target=&#34;_blank&#34;&gt;what role depth plays&lt;/a&gt; in all of this.&lt;/p&gt;

&lt;p&gt;We now turn our attention towards the theory of unsupervised learning and generative models, with special emphasis on variational autoencoders and generative adversarial networks (GANs). But first, &lt;em&gt;what is unsupervised learning&lt;/em&gt;?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Evidently, unsupervised learning is much more abstract than its supervised counterpart. In the latter, our objective was essentially to find a function that approximates the original mapping of the distribution $\mathcal{X}\times\mathcal{Y}$. In the unsupervised domain, there is no such objective. We are given input data, and we want to learn &amp;ldquo;structure&amp;rdquo;. The most obvious way to understand why this is more difficult is to realize that &lt;em&gt;drawing a picture of a lion is much more difficult than identifying a lion in a picture&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Why is learning structures important? Creating large annotated datasets is an expensive task, and may even be infeasible for some problems such as parsing, which require significant domain knowledge. Let&amp;rsquo;s consider the simplest problem of image classification. The largest dataset for this problem, ImageNet, contains 14 million images, with 20000 distinct output labels. However, the number of images freely available online far exceeds 14 million, which means that we can probably learn something from them. This kind of &lt;strong&gt;transfer learning&lt;/strong&gt; is the most important motivation for unsupervised learning.&lt;/p&gt;

&lt;p&gt;For instance, while training a machine translation model, obtaining a parallel corpus may be difficult, but we always have access to unilateral text corpora in different languages. If we then try to learn some underlying structure present in these languages, it can assist the downstream translation task. In fact, recent advances in &lt;a href=&#34;https://desh2608.github.io/post/transfer-learning-nlp/&#34; target=&#34;_blank&#34;&gt;transfer learning for NLP&lt;/a&gt; have empirically proven that huge performance gains are possible using such a technique.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1206.5538.pdf&#34; target=&#34;_blank&#34;&gt;Representation learning&lt;/a&gt;&lt;/strong&gt; is perhaps the most widely studied aspect of unsupervised learning. A &amp;ldquo;good representation&amp;rdquo; often means one which disentangles factors of variation, i.e, each coordinate in the representation corresponds to one meaningful factor of variation. For example, if we consider word embeddings, an ideal vector representing a word would depict different features of the word along each dimension. However, this is easier said than done, since learning representations require an objective function, and it is still unknown how to translate these notions of &amp;ldquo;good representation&amp;rdquo; into training criteria. For this reason, representation learning is often criticized for getting too much attention for transfer learning. The essence of the criticism, taken from &lt;a href=&#34;https://www.inference.vc/goals-and-principles-of-representation-learning/&#34; target=&#34;_blank&#34;&gt;this post by Ferenc Huszár&lt;/a&gt; is this:&lt;/p&gt;

&lt;p&gt;If we identified transfer learning as the primary task representation learning is supposed to solve, are we actually sure that representation learning is the way to solve it? One can argue that there may be many ways to transfer information from some dataset over to a novel task. Learning a representation and transferring that is just one approach. Meta-learning, for example, might provide another approach.&lt;/p&gt;

&lt;p&gt;In the discussion so far, we have blindly assumed that the data indeed contains structures that can be learnt. This is not an oversight; it is actually based on the &lt;strong&gt;manifold assumption&lt;/strong&gt; which we will discuss next.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;the-manifold-assumption&#34;&gt;The manifold assumption&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;A manifold is a topological space that locally resembles Euclidean space near each point.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that globally, a manifold may not be a Euclidean space. The only requirement for an $n$-manifold, i.e., a manifold in $n$ dimensions, is that each point of the manifold must have a neighborhood that is homeomorphic to the Euclidean space of $n$ dimensions. There are three technicalities in this definition.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;neighborhood&lt;/em&gt; of a point $p$ in $X$ is a $V \subset X$ which contains an open set $U$ containing $p$, i.e., $p$ must be in the interior of $V$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A function $f: X \rightarrow Y$ between two topological spaces $X$ and $Y$ is called a &lt;em&gt;homeomorphism&lt;/em&gt; if it has the following properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$f$ is a bijection,&lt;/li&gt;
&lt;li&gt;$f$ is continuous,&lt;/li&gt;
&lt;li&gt;$f^{-1}$ is continuous.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Euclidean space&lt;/em&gt; is a topological space such that&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;it is in 2 or 3 dimensions and obeys Euclidean postulates, or&lt;/li&gt;
&lt;li&gt;it is in any dimension such that points are given by coordinates and satisfy Euclidean distance.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that the dimension of a manifold may not always be the same as the dimension of the space in which the manifold is embedded. Dimension here simply means the degree of freedom of the underlying process that generated the manifold. As such, lines and curves, even if embedded in $\mathbb{R}^3$, are one-dimensional manifolds.&lt;/p&gt;

&lt;p&gt;With this definition in place, we can now state the manifold assumption. It hypothesizes that the intrinsic dimensionality of the data is much smaller than the ambient space in which the data is embedded. This means that if we have some data in $N$ dimensions, there must be an underlying manifold $\mathcal{M}$ of dimension $n &amp;lt;&amp;lt; N$, from which the data is drawn based on some probability distribution $f$. The goal of unsupervised learning in most cases, is to identify such a manifold.&lt;/p&gt;

&lt;p&gt;It is easy to see that the manifold assumption is, as the name suggests, just an assumption, and does not hold universally. Otherwise, applying the assumption consecutively, we would be able to represent any high-dimensional data using a one-dimensional manifold, which, of course, is not possible.&lt;/p&gt;

&lt;p&gt;The task of manifold learning is modeled as approximating the joint probability density $p(x,z)$, where $x$ is the data point and $z$ is its underlying &amp;ldquo;code&amp;rdquo; on the manifold. Deep generative models have come to be accepted as the standard for estimating this probability, because of two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Deep models promote reuse of features. We have already seen in the previous post that depth is analogous to composition whereas width is analogous to addition. Composition offers more representation capability than addition using the same number of parameters.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep models are conjectured to lead to progressively more abstract features at higher levels of representation. An example of this is the commonly known phenomenon in training deep convolutional networks on image data, where it is found that the first few layers learn lines, blobs, and other local features, and higher level layers learn more abstract features. This is done explicitly using the pooling mechanism.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;theory-of-variational-autoencoders&#34;&gt;Theory of Variational Autoencoders&lt;/h3&gt;

&lt;p&gt;Deep learning models often face some flak for being purely intution-based. &lt;a href=&#34;https://arxiv.org/pdf/1606.05908.pdf&#34; target=&#34;_blank&#34;&gt;Variational autoencoders (VAEs)&lt;/a&gt; are the practitioner&amp;rsquo;s answer to such criticisms, since they are rooted in the theory of Bayesian inference, and also perform well empirically. In this section, we will look at the theory that forms VAEs.&lt;/p&gt;

&lt;p&gt;First, we formalize the notion of the &amp;ldquo;code&amp;rdquo; that we mentioned earlier using the concept of a &lt;strong&gt;latent variable&lt;/strong&gt;. These are those variables that are not directly observed but are inferred from the observable variables. For instance, if the model is drawing a picture of an MNIST digit, it would make sense to first have a variable choose a digit from $[0,\ldots,9]$, and then draw the strokes corresponding to the digit.&lt;/p&gt;

&lt;p&gt;Formally, suppose we have a vector of latent variables $z$ in a high-dimensional space $\mathcal{Z}$ which can be sampled using a probability distribution $P(z)$. Then, suppose we have a family of deterministic functions $f(z;\theta)$ parametrized by $\theta \in \Theta$, such that $f:\mathcal{Z}\times \Theta \rightarrow \mathcal{X}$. The task, then, is to optimize $\theta$ such that we can sample $z$ from $P(z)$ and with high probability, $f(z;\theta)$ will be like the $X$&amp;rsquo;s in our dataset. As such, we can write the expression for the generated data as&lt;/p&gt;

&lt;p&gt;$$ X^{\prime} = f(z;\theta). $$&lt;/p&gt;

&lt;p&gt;Now, since we have no idea how to check if randomly generated images are &amp;ldquo;like&amp;rdquo; our dataset, we use the notion of &amp;ldquo;maximum likelihood&amp;rdquo;, i.e., if the model is likely to produce training set samples, then it is also likely to produce similar samples and unlikely to produce dissimilar ones. With this assumption, we want to maximize the probability of each $X$ in the training process. We can now replace $f(z;\theta)$ by the conditional probability $P(X|z;\theta)$, and we get&lt;/p&gt;

&lt;p&gt;$$ P(X) = \int P(X|z;\theta)P(z)dz. $$&lt;/p&gt;

&lt;p&gt;In VAEs, we usually have $P(X|z;\theta) = \mathcal{N}(X|f(z;\theta),\sigma^2 I)$, which is a Gaussian. Using this formalism, we can use gradient descent to increase $P(X)$ by making $f(z;\theta)$ approach $X$ for some $z$. So essentially, VAEs do the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sample $z$ from some known distribution.&lt;/li&gt;
&lt;li&gt;Feed $z$ into some parametrized function to get $X$.&lt;/li&gt;
&lt;li&gt;Tune the parameters of the function such that generated $X$ resemble those in dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this process, two questions arise:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we define $z$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;VAEs simply sample $z$ from $\mathcal{N}(0,I)$, where $I$ is the identity matrix. The motivation for this choice is that any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function. I do not prove this here, but the proof is based on taking the composition of the inverse cumulative distribution function (CDF) of the desired distribution with the CDF of a Gaussian.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we deal with $\int dz$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We need to understand that the space $\mathcal{Z}$ is very large, and there are only few $z$ which generate realistic $X$, which makes it very difficult to sample &amp;ldquo;good&amp;rdquo; values of $z$ from $P(z)$ . Suppose we have a function $Q(z|X)$ which, given some $X$, gives a distribution over $z$ values that are likely to produce $X$. Now to compute $P(X)$, we need to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;relate $P(X)$ with $\mathbb{E}_{z\sim Q}P(X|z)$, and&lt;/li&gt;
&lt;li&gt;estimate $\mathbb{E}_{z\sim Q}P(X|z)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the first, we use KL-divergence (that we saw in the previous post) between the probability distribution estimated by $Q$ to the actual conditional probability distribution as follows.&lt;/p&gt;

&lt;p&gt;$$ \begin{align} &amp;amp; \mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q}[\log Q(z|X) - \log P(z|X)] \\\ &amp;amp;= \mathbb{E}_{z\sim Q}\left[ \log Q(z|X) - \log \frac{P(X|z)P(z)}{P(X)} \right] \\\ &amp;amp;= \mathbb{E}_{z\sim Q} [ \log Q(z|X) - \log P(X|z) - \log P(z) ] + \log P(X) \\\ \Rightarrow &amp;amp; \log P(X) - \mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q}[\log P(X|z)] - \mathcal{D}_{KL}[Q(z|X)||P(z)] \end{align} $$&lt;/p&gt;

&lt;p&gt;In the LHS of the above equation, we have an expression that we want to maximize, since we want $P(X)$ to be large and we want $Q$ to approximate the conditional probability distribution (this was our objective of using KL-divergence). If we use a sufficiently high-capacity model for $Q$, the $\mathcal{D}_{KL}$ term will approximate $0$, in which case we will directly be optimizing $P(X)$.&lt;/p&gt;

&lt;p&gt;Now we are just left with finding some way to optimize the RHS in the equation. For this, we will have to choose some model for $Q$. An obvious (and usual) choice is to take the multivariate Gaussian, i.e., $Q(z|X) = \mathcal{N}(z|\mu(X),\Sigma(X))$. Since $P(z) = \mathcal{N}(0,I)$, the KL-divergence term on the RHS can now be written as&lt;/p&gt;

&lt;p&gt;$$ \mathcal{D}_{KL}[\mathcal{N}(\mu(X),\sum(X))||\mathcal{N}(0,I)] = \frac{1}{2}\left( \text{tr}(\Sigma(X)) + (\mu(X))^T (\mu(X)) - k - \log \text{det}(\Sigma(X)) \right). $$&lt;/p&gt;

&lt;p&gt;To estimate the first term on the RHS, we just compute the term for one sample of $z$, instead of iterating over several samples. This is because during stochastic gradient descent, different values of $X$ will automatically require us to sample $z$ several times. With this approximation, the optimization objective for a single sample $X$ becomes&lt;/p&gt;

&lt;p&gt;$$ J = \log P(X|z) - \mathcal{D}_{KL}[Q(z|X)||P(z)]. $$&lt;/p&gt;

&lt;p&gt;This can be represented in the form of a feedforward network by the figure on the left below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/24/vae.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There is, however, a caveat. The network is not trainable using backpropagation because the red box is a stochastic step, which means that it is not differentiable. To solve this problem, we use the &lt;strong&gt;reparametrization trick&lt;/strong&gt; as follows.&lt;/p&gt;

&lt;p&gt;$$ z = \mu(X) + \Sigma^{\frac{1}{2}}(X)  \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0,I) $$&lt;/p&gt;

&lt;p&gt;After this trick, we get the final network as shown in the right in the above figure. Furthermore, we must have $\mathcal{D}_{KL}[Q(z|X)||P(z|X)]$ approximately equal $0$ in the LHS. Since we have taken $Q$ to be a Gaussian, this means that the original density function $f$ should be such that $P(z|X)$ is a Gaussian. It turns out that such a function, which maximizes $P(X)$ and satisfies the said criteria, provably exists.&lt;/p&gt;

&lt;p&gt;Although VAEs have strong theoretical support, they do not work very well in practice, especially in problems such as face generation. This is because the loss function used for training is log-likelihood, which ultimately leads to fuzzy face images which have high match with several $X$. Instead of using likelihood, we use the power of discriminative deep learning, which is where GANs come into the picture.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;generative-adversarial-networks-new-insights&#34;&gt;Generative adversarial networks: new insights&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.00160&#34; target=&#34;_blank&#34;&gt;GANs&lt;/a&gt; were proposed in 2014, and have become immensely popular in computer vision ever since. They are basically motivated from game theory, and I will not get into the details here since the tutorial by Ian Goodfellow is a excellent resource for the same.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/24/gan.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since the prior learnt by the generator depends upon the discriminative process, an important issue with GANs is that of &lt;strong&gt;mode collapse&lt;/strong&gt;. The problem is that since the discriminator only learns from a few samples, it may be unable to teach the generator to produce $\mathcal{P}_{synth}$ with sufficiently large diversity. In the context of what we have already seen, this can be taken as the problem of generalization for GANs.&lt;/p&gt;

&lt;p&gt;In this section, I will discuss three results from two important papers from Arora et al. which deal with mode collapse in GANs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.00573.pdf&#34; target=&#34;_blank&#34;&gt;Generalization and equilibrium in generative adversarial nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=BJehNfW0-&#34; target=&#34;_blank&#34;&gt;Do GANs learn the distribution? Some theory and empirics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For all our discussions in this section, we will consider the Wasserstein GAN objective instead of the usual minimax objective, which is as follows (and arguably more intuitive)&lt;/p&gt;

&lt;p&gt;$$ J = \lvert \mathbb{E}_{x\in \mathcal{P}_{real}}[D(x)] - \mathbb{E}_{x\in \mathcal{P}_{synth}}[D(x)] \rvert, $$&lt;/p&gt;

&lt;p&gt;where $D$ is the discriminator.&lt;/p&gt;

&lt;h4 id=&#34;1-generalization-depends-on-discriminator-size&#34;&gt;1. Generalization depends on discriminator size&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;If the discriminator size is $n$, then there exists a generator supported on $\mathcal{O}(n\log n)$ images, which wins against all possible discriminators.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that if we have a discriminator of size $n$, then the best possible generator training is possible using $Cn/\epsilon^2 \log n$ images from the full training set. Any more images will improve the training objective by at most $\epsilon$. I will now give the proof (simplified from the actual proof in the paper).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Suppose $\mu$ denotes the actual distribution learnt by the generator and $\nu$ denotes the actual distribution of real images that the discriminator has access to. Let $\tilde{\mu}$ and $\tilde{\nu}$ be the empirical versions of the above distributions, i.e., the distributions that we actually use for training. Let $d(p,q)$ be some distance measure between the two distributions.&lt;/p&gt;

&lt;p&gt;In the paper, the authors have defined an $\mathcal{F}$-distance that has good generalization properties, but I will not get into the details of that here for sake of simplicity. For this discussion, just assume that the distance measure is $d$. From my earlier post on generalization error in supervised learning, we say that a model generalizes well when, for some $\epsilon$,&lt;/p&gt;

&lt;p&gt;$$ |\text{True error} - \text{Empirical error}| \leq \epsilon. $$&lt;/p&gt;

&lt;p&gt;Here, we don&amp;rsquo;t really know the error, but we can use our distance measure to the same effect. If the size of discriminator is $p$, we want to compute the sample complexity $m$ in terms of $p$ and $\epsilon$ such that the GAN generalizes. For that, we need a few approximations.&lt;/p&gt;

&lt;p&gt;First we approximate the parameter space $\mathcal{V}$ using its $\frac{\epsilon}{8}$-net $\mathcal{X}$. This means that for every $\nu \in \mathcal{V}$, we can find a $\nu^{\prime}\in \mathcal{X}$ which is at a distance of at most $\frac{\epsilon}{8}$ from it. Assuming that the function computed by the discriminator $D$ is 1-Lipschitz, we can then say that $\lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \mu}[D_{\nu^{\prime}}(x)]  \rvert \leq \frac{\epsilon}{8}$.&lt;/p&gt;

&lt;p&gt;The $\epsilon$-net is taken so that we can apply concentration inequalities in this continuous finite space. You can read more about them &lt;a href=&#34;https://www.ti.inf.ethz.ch/ew/lehre/CG12/lecture/Chapter%2015.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Now, we can use Hoeffding&amp;rsquo;s inequality to bound the difference between true and empirical errors on this space as&lt;/p&gt;

&lt;p&gt;$$ P\left[ $\lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \tilde{\mu}}[D_{\nu}(x)]  \rvert \geq \frac{\epsilon}{4} \right] \leq 2\exp \left( -\frac{\epsilon^2 m}{2} \right). $$&lt;/p&gt;

&lt;p&gt;Taking union bound over all $p$ parameters, we get that when $m \geq \frac{Cp\log (p/\epsilon)}{\epsilon^2}$, then the bound holds with high probability. Note that this sample complexity is $m = \mathcal{p\log p}$, which is what we wanted. Now we just need to show that this bound implies that the generalization error is bounded. Since we have taken the $\frac{\epsilon}{8}$-net approximation, we translate both the parameters in $\mathcal{X}$ back to $\mathcal{V}$, paying a cost of $\frac{\epsilon}{8}$ for each. Finally, we get, for every $D$,&lt;/p&gt;

&lt;p&gt;$$ \lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \tilde{\mu}}[D_{\nu}(x)]  \rvert \leq \frac{\epsilon}{2}. $$&lt;/p&gt;

&lt;p&gt;We can prove a similar upper bound for $\nu$. Finally, with similar approximation arguments, and from the definition of our distance function, we get the desired result.&lt;/p&gt;

&lt;h4 id=&#34;2-existence-of-equilibrium&#34;&gt;2. Existence of equilibrium&lt;/h4&gt;

&lt;p&gt;For GANs to be successful, they must find an equilibrium in the G-D game where the generator wins. In the context of the minimax equation, this means that switching min and max in the objective should not cause any change in the equilibrium. In the paper, the authors prove an $\epsilon$-approximate equilibrium, i.e., one where such a switching affects the expression by at most $\epsilon$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If a generator net is able to generate a Gaussian distribution, then there exists an $\epsilon$-approximate equilibrium where the generator has capacity $\mathcal{O}(n\log n / \epsilon^2)$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The proof of this result lies in a classical result in statistics, which says that any probability distribution can be approximated by a mixture of infinite Gaussians. For this, we just need to take the standard Gaussian $P(x)\mathcal{N}(x,\sigma^2)$ at every $x \in \mathcal{X}$ such that $\sigma^2 \rightarrow 0$, and take the mixture of all such Gaussians. The remaining proof is similar to the one done for the previous result, so I will not repeat it here.&lt;/p&gt;

&lt;h4 id=&#34;3-empirically-detecting-mode-collapse&#34;&gt;3. Empirically detecting mode collapse&lt;/h4&gt;

&lt;p&gt;We have already seen that GAN training can be successful even if the generator has not learnt a good enough distribution, if the discriminator is small. But suppose we take a really large discriminator and then train our GAN to a minima. How do we still make sure that the generator distribution is good? It could well be the case that the generator has simply memorized the training data, due to which the discriminator cannot make a better guess than random. Researchers have proposed several qualitative checks to test this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Check the similarity of each generated image to the nearest image in the training set.&lt;/li&gt;
&lt;li&gt;If the seed formed by interpolating two seeds $s_1$ and $s_2$ that produce realistic images, also produces realistic images, then the learnt distribution probably has many realistic images.&lt;/li&gt;
&lt;li&gt;Check for semantically important directions in latent space, which cause predictable changes in generated image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will now see a new empirical measure for the support size of the trained distribution, based on the Birthday Paradox.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The birthday paradox&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a room of just 23 people, there&amp;rsquo;s a 50% chance of finding 2 people who share their birthday.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see why, refer to &lt;a href=&#34;https://betterexplained.com/articles/understanding-the-birthday-paradox/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. It is a simple problem of permutation and combination, followed by using the approximation for $e^x$.&lt;/p&gt;

&lt;p&gt;Since $23 \approx \sqrt{365}$, we can generalize this to mean that if a distribution has support $N$, we are likely to find a duplicate in a batch of about $\sqrt{N}$ samples taken from this distribution. As such, finding the smallest batch size $s$ which ensures duplicate images with good probability almost guarantees that the distribution has support $s^2$. Let us formalize this guarantee.&lt;/p&gt;

&lt;p&gt;Suppose we have a probability distribution $P$ on a set $\Omega$. Also, let $S \subset \Omega$ such that $\sum_{s\in S}P(s)\geq \rho$ and $|S|=N$. Then from calculations similar to the one done for birthday paradox, we can say that the probability of finding at least one collision on drawing $M$ i.i.d samples is at least $1 - \exp\left( -\frac{(M^2 - M)\rho}{2N} \right)$.&lt;/p&gt;

&lt;p&gt;Now, suppose we have empirically found this minimum probability of collision to be $\gamma$. Then it can be shown that under realistic assumptions on parameters, the following holds:&lt;/p&gt;

&lt;p&gt;$$ N \leq \frac{2M\rho^2}{\left(-3 + \sqrt{9+\frac{24}{M}\log \frac{1}{1-\gamma}}\right)-2M(1-\rho)^2} $$&lt;/p&gt;

&lt;p&gt;This gives an upper bound on the support size of the distribution learned by the generator.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Generative models are definitely very promising, especially with the recent interest in transfer learning with unsupervised pretraining. While I have tried to explain the recent insights into GANs as best as possible, it is not possible to explain every detail in the proof in an overview post. Even so, I hope I have been able to at least give a flavor of how veterans in the field approach theoretical guarantees.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Role of Depth</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-3/</link>
      <pubDate>Sat, 28 Jul 2018 23:00:20 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-3/</guid>
      <description>

&lt;p&gt;In the previous posts of this series, we have looked at &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;how stochastic gradient descent is able to find a good solution&lt;/a&gt; despite the nonconvex objective, and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;why overparametrized neural networks generalize so well&lt;/a&gt;. In this post, we will look at the titular property of deep networks, namely depth, and what role they play in the learning ability of the model.&lt;/p&gt;

&lt;p&gt;An ideal result in this regard would be if we can show that there exists a class of natural learning problems (recall the idea of a &amp;ldquo;natural&amp;rdquo; problem from the first post) which cannot be solved with depth $d$ neural networks, but are solvable with at least one model of depth $d+1$. However, such a result is elusive at present, since we have already established that there exists no mathematical formulation of a &amp;ldquo;natural&amp;rdquo; learning problem.&lt;/p&gt;

&lt;p&gt;However, there has been some advancement in establishing similar results in the case of less natural problems, and in this regard, the following papers are worth mentioning.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v49/eldan16.pdf&#34; target=&#34;_blank&#34;&gt;The Power of Depth for Feedforward Neural Networks&lt;/a&gt; by Eldan and Shamir&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v49/telgarsky16.pdf&#34; target=&#34;_blank&#34;&gt;Benefit of Depth in Neural Networks&lt;/a&gt; by Telgarsky&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will now discuss both of these in some detail.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;role-of-depth-for-less-natural-problems&#34;&gt;Role of depth for &amp;ldquo;less natural&amp;rdquo; problems&lt;/h3&gt;

&lt;h4 id=&#34;1-approximating-radial-functions&#34;&gt;1. Approximating radial functions&lt;/h4&gt;

&lt;p&gt;At the outset, note that if we allow the neural network to be unbounded, i.e., have exponential width, even a 2-layer network can approximate any continuous function. As such, for our study, we only use &amp;ldquo;bounded&amp;rdquo; networks where the number of hidden layer units cannot be exponential in the dimension of input. With this understanding, we will look at the simplest case: we try to find a function (or a family of functions) that are expressible by a 3-layer network but cannot be expressed by any 2-layer network. Before we get into the details, we first look at what a 2-layer and 3-layer networks represent.&lt;/p&gt;

&lt;p&gt;A 2-layer network represents the following function:&lt;/p&gt;

&lt;p&gt;$$ f_2(\mathbf{x}) = \sum_{i=1}^w v_i \sigma(&amp;lt; \mathbf{w}_i,\mathbf{x} &amp;gt;+b_i), $$&lt;/p&gt;

&lt;p&gt;and a 3-layer network represents the following:&lt;/p&gt;

&lt;p&gt;$$ f_3(\mathbf{x}) = \sum_{i=1}^w u_i \sigma\left( \sum_{j=1}^w v_{i,j} \sigma (&amp;lt; \mathbf{w}_{i,j},\mathbf{x} &amp;gt;+ b_{i,j}) + c_i \right). $$&lt;/p&gt;

&lt;p&gt;Here, $w$ is the size of the hidden layer and $\sigma$ is an activation function. The only constraint on $\sigma$ is that it should be &amp;ldquo;universal&amp;rdquo;, i.e., a 2-layer network should be able to approximate any Lipschitz function that is non-constant on a bounded domain for some $w$ (which need not be bounded). This constraint is satisfied by all the standard activation functions such as sigmoid and ReLU.&lt;/p&gt;

&lt;p&gt;Under this assumption, the main result in the paper is as follows:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There exists a &lt;strong&gt;radial function $g$&lt;/strong&gt; depending only on the norm of the input, which is expressible by a 3-layer network of width polynomial in the input dimension, but not by any 2-layer neural network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;More importantly, apart from the universal assumption, this result does not depend on any characteristic of $\sigma$. Furthermore, there are no constraints on the size of the parameters $\mathbf{w}$. The only constraint worth noting is that $g$ must be a &lt;em&gt;radial function&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To prove this result, we need to show 2 things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$g$ can be approximated by a 3-layer neural network.&lt;/li&gt;
&lt;li&gt;$g$ cannot be approximated by any 2-layer network of bounded width.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Part 1:&lt;/strong&gt; This is trivial to show, since any radial function can be approximated by a 3-layer network. To do this, we compute the Euclidean norm $\lVert \mathbf{x} \rVert^2$ from the input $\mathbf{x}$ in the first layer using a linear combination of neurons. This is possible because the squared norm is just the sum of squares of all the components, and each squared component can be approximated in a finite range, for example, using the step function.&lt;/p&gt;

&lt;p&gt;Once the norm is computed, the second layer can be used to approximate the required radial function using RBF nodes. This completes the construction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 2:&lt;/strong&gt; Let the input be taken from a probability distribution $\mu$, which has a density function $\phi^2(x)$, for some known function $\phi$. Suppose we are trying to approximate a function $f$ using a function $g$. Then, the distance between the functions can be given as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E}_{\mu}(f(x)-g(x))^2 &amp;amp;= \int (f(x)-g(x))^2 \phi^2(x) dx \\\ &amp;amp;= \int (f(x)\phi (x) - g(x)\phi (x))^2 dx \\\ &amp;amp;= \lVert f\phi - g\phi \rVert_{L_2}^2 \end{align} $$&lt;/p&gt;

&lt;p&gt;Now, we can replace $f\phi$ and $g\phi$ with their respective Fourier transforms since the Fourier transform is an isometric mapping (i.e., distance remains same before and after the mapping). Therefore, we get&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}_{\mu}(f(x)-g(x))^2 = \lVert \hat{f\phi} - \hat{g\phi} \rVert_{L_2}^2 $$&lt;/p&gt;

&lt;p&gt;While this replacement may seem arbitrary at first, it has a very clear motivation. We have done this because the Fourier transform of functions expressible by a 2-layer network has a very particular form, which we will use here. Specifically, consider the function&lt;/p&gt;

&lt;p&gt;$$ f(x) = \sum_{i=1}^k f_i (&amp;lt; \mathbf{v}_i,\mathbf{x} &amp;gt;), $$&lt;/p&gt;

&lt;p&gt;which is expressible by any 2-layer network. The component function $f_i (&amp;lt; \mathbf{v}_i,\mathbf{x} &amp;gt;)$ is constant in any direction perpendicular to $\mathbf{v}_i$ and so its Fourier transform is non-zero only in the direction of $\mathbf{v}_i$, and so the whole distribution is supported on $\bigcup_i \text{span}(\mathbf{v}_i)$. Now we just need to compute the support of $\hat{\phi}$, and then we can directly use the convolution-multiplication principle.&lt;/p&gt;

&lt;p&gt;Since we haven&amp;rsquo;t yet chosen a density function, we choose $\phi$ to make the computation of support easier. Specifically, we choose $\phi$ to be the inverse Fourier transform of $\mathbb{1}\{x\in B\}$, which is the indicator function of a unit Euclidean ball. Then, $\hat{\phi}$ becomes $\mathbb{1}\{x\in B\}$ itself, and its support is simply the ball $B$. Using these, we get&lt;/p&gt;

&lt;p&gt;$$ \text{Supp}(\hat{f\phi}) \subseteq T = \bigcup_{i=1}^k (\text{span}\{ \mathbf{v}_i \} + B ), $$&lt;/p&gt;

&lt;p&gt;which is basically the union of $k$ tubes passing through origin. This is because $\text{span}\{\mathbf{v}_i\}$ is just a straight line, and $B$ is a ball. Sum here means the direct sum, i.e., for every element $a \in A$ and $b \in B$, form a set of $a+b$. So we just put the Euclidean ball on every point on the line, which gives us a cylinder passing through the origin.&lt;/p&gt;

&lt;p&gt;Since we are looking for a $g$ which cannot be approximated by the neural network, we try to make $\lVert f\phi - g\phi \rVert_{L_2}^2$ as large as possible. We have already seen what the support of $\hat{f\phi}$ looks like. Now, we want a $g$ such that&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$g$ should have most of its mass away from the origin in all the directions, and&lt;/li&gt;
&lt;li&gt;the Fourier transform $\hat{g}$ should be outside $B$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If $g$ is chosen as a radial function, the first criteria will be satisfied if we just put large mass away from the origin in one direction. To satisfy the second criteria, $g$ should have a high-frequency component. To see why, see the following figure which shows the sine curve and its Fourier transform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/23/fourier.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The only thing that remains to be shown is that if $\hat{g}$ contains a significant mass away from the origin, then so does $\hat{g\phi}$. But this proof is somewhat technical in nature and I avoid it here for sake of simplicity.&lt;/p&gt;

&lt;p&gt;This completes our proof for the result given in the paper. While this result is an important step in quantifying the role of depth in a neural network, it is still limited in that it only holds for radial functions. This is what I meant earlier by &amp;ldquo;less natural&amp;rdquo; problems, since in most of the common learning problems, the $(x,y)$ pairs are not generated from a simple radial distribution, and are much more complex in nature.&lt;/p&gt;

&lt;h4 id=&#34;2-exponential-separation-between-shallow-and-deep-nets&#34;&gt;2. Exponential separation between shallow and deep nets&lt;/h4&gt;

&lt;p&gt;In the proof of the previous result, the key idea was to have a high-frequency component in the function required to be approximated. This means that the function was highly oscillatory. In this paper as well, a similar oscillation argument is used to prove another important result.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For every positive integer $k$, there exists neural networks with $\theta(k^3)$ layers, $\theta(1)$ nodes per layer, and $\theta(1)$ distinct parameters, which cannot be approximated by networks with $\mathcal{O}(k)$ layers and $o(2^k)$ nodes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This result is proven using three steps.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Functions with few oscillations poorly approximate functions with many oscillations.&lt;/li&gt;
&lt;li&gt;Functions computed by networks with few layers must have few oscillations.&lt;/li&gt;
&lt;li&gt;Functions computed by networks with many layers can have many oscillations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;approximation-via-oscillation-counting&#34;&gt;Approximation via oscillation counting&lt;/h5&gt;

&lt;p&gt;We will first look at a metric to count oscillations of a function. For this, consider the following graph which shows functions $f$ and $g$ which are defined from $\mathbb{R}$ to $[0,1]$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/23/oscillations.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the horizontal line denotes $y = \frac{1}{2}$. The classifiers $\tilde{f}$ and $\tilde{g}$ obtained from $f$ and $g$ perform binary classification according to the rule $\tilde{f}(x) = \mathbb{1}[f(x)\geq \frac{1}{2}]$. Let $\mathcal{I}_f$ denote the set of partitions of $\mathbb{R}$ into intervals so that the classifier $\tilde{f}$ is constant in each interval. Then, the crossing number is defined as&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) = |\mathcal{I}_f|. $$&lt;/p&gt;

&lt;p&gt;From our definition of $\tilde{f}$, this clearly means that $\text{Cr}(f)$ counts the number of times that $f$ crosses the line $y = \frac{1}{2}$, and hence the name. In this way, we formalize the notion of counting the number of oscillations of a function.&lt;/p&gt;

&lt;p&gt;With this definition, if $\text{Cr}(f)$ is much larger than $\text{Cr}(g)$, then most piecewise constant regions of $\tilde{g}$ will exhibit many oscillations of $f$, and thus $g$ poorly approximates $f$.&lt;/p&gt;

&lt;p&gt;Now we will prove the following lemma, where the counting number $\text{Cr}(f)$ is denoted by $s_f$ for sake of convenience.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$$ \frac{\text{No. of regions of }\mathcal{I}_f \text{ where} ~ \tilde{f}\neq \tilde{g}}{s_f} \geq \frac{1}{2} - \frac{s_g}{s_f} $$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, if $s_f &amp;gt;&amp;gt; s_g$, then the RHS approximately becomes $\frac{1}{2}$, which implies that for more than half of all the regions of $f$, $\tilde{g}$ classifies $x$ incorrectly, and so $g$ is a poor approximation of $f$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We choose a region $J$ where $\tilde{g}$ is constant but $\tilde{f}$ alternates, such as the region where $g$ is red in the above figure. We denote by $X_J$ all the partitions of $\mathcal{I}_f$ that are contained in $J$. Since $f$ oscillates within $g$, this means that $\tilde{g}$ disagrees with $\tilde{f}$ for half of all $X_J$, i.e., at least $\frac{|X_J|-1}{2}$ in general.&lt;/p&gt;

&lt;p&gt;In the LHS of the claim, we need to count all the regions of $\mathcal{I}_f$ where the classifiers disagree for all points in the region. From above, we have a lower bound on the number of such regions within one $J$. So now we just take sum over all $J \in \mathcal{I}_g$ to get&lt;/p&gt;

&lt;p&gt;$$ \frac{\text{No. of regions of }\mathcal{I}_f \text{ where} ~ \tilde{f}\neq \tilde{g}}{s_f} \geq \frac{1}{s_f}\sum_{J \in \mathcal{I}_g} \frac{|X_J|-1}{2}. $$&lt;/p&gt;

&lt;p&gt;Now we need to bound $s_f$. For this, see that the total number of oscillations of $f$ are at least its number of oscillations within a single partition of $\mathcal{I}_g$ summed over all such partitions. I say &amp;ldquo;at least&amp;rdquo; because this will not include those partitions of $\mathcal{I}_f$ whose interior intersects with the boundary of an interval in $\mathcal{I}_g$. At most, there would be $s_g$ such partitions, and so&lt;/p&gt;

&lt;p&gt;$$ s_f \leq s_g + \sum_{J\in \mathcal{I}_g}|X_J|. $$&lt;/p&gt;

&lt;p&gt;This means that $\sum_{J\in \mathcal{I}_g}|X_J| \geq s_f - s_g$. Using this bound in the previously obtained inequality, we get the desired result.&lt;/p&gt;

&lt;h5 id=&#34;few-layers-few-oscillations&#34;&gt;Few layers, few oscillations&lt;/h5&gt;

&lt;p&gt;Adding more nodes is similar to adding polynomials, while adding layers is like composition of polynomials. Adding polynomials yields a new polynomial with degree equal to the higher of the two and at most twice as many terms, but composing them (i.e. taking product) would yield a polynomial with higher degree and more than the product of terms. Clearly, composition would lead to more number of roots of the new polynomial. This suggests that adding layers should lead to a higher number of oscillations than adding nodes.&lt;/p&gt;

&lt;p&gt;Let $f$ be a function computed by the neural network $\mathcal{N}((m_i,t_i,\alpha_i,\beta_i)_{i=1}^l)$, i.e. a network of $l$ layers where the $i$th layer has $m_i$ nodes, such that the activation function at each node is $(t,\alpha)-poly$ (a piecewise function containing $t$ parts where each piece is a polynomial of degree at most $\alpha$). Then, we claim that&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) \leq \mathcal{O}\left( \left( \frac{tm\alpha}{l} \right)^l \beta^{l^2} \right). $$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We will prove this in two parts. First, we bound the counting number of a $(t,\alpha)-poly$ function, and then we will show that the function $f$ as computed by the above network is $(t,\alpha)-poly$.&lt;/p&gt;

&lt;p&gt;For the first part, see that each piece of the function $f$ is a polynomial of degree at most $\alpha$, which means that each piece oscillates at most $1 + \alpha$ times. Since there are $t$ such pieces&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) \leq t(1+\alpha). $$&lt;/p&gt;

&lt;p&gt;Now it remains to show that the function $f$ computed by the network is indeed $(t,\alpha)-poly$. To see this, consider the function computed by a single layer. Each node in the layer computes a $(t,\alpha)-poly$ function, say $g_i$, and we apply a composition function, say $f$, on these $g_i$&amp;rsquo;s, which is a polynomial with degree at most $\gamma$. The final function computed by this layer is&lt;/p&gt;

&lt;p&gt;$$ h(x) = f(g_1(x),\ldots,g_k(x)). $$&lt;/p&gt;

&lt;p&gt;To visualize such a composition, consider the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/23/poly.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here, each horizontal line denotes one node&amp;rsquo;s partition function, i.e., $\tilde{g_i}$. There are $k$ such lines with at most $t$ intervals each. The composition takes the union of all the partitions of all these lines. As such, the maximum number of intervals after composition will be equal to $kt$. Within each such interval, since we are taking a composition of a degree $\gamma$ polynomial with one with degree at most $\alpha$, the resulting polynomial has degree at most $\alpha \gamma$. Hence, $h$ is $(tk,\alpha\gamma)-poly$.&lt;/p&gt;

&lt;p&gt;Since there are $l$ layers and the total number of nodes in the network is $m$, it implies there are $\frac{m}{l}$ nodes on average in each layer, and each node has at most $t$ intervals. So after every layer, the number of intervals gets multiplied by a factor of $\frac{mt}{l}$. Finally, the total number of intervals will be of the order $\left(\frac{mt}{l}\right)^l$.&lt;/p&gt;

&lt;p&gt;Similarly, the degree of resulting function gets multiplied by $\alpha$ after every layer, so the final degree is of the order $\alpha^l$. Using the result shown in the first part, the resulting function will have a counting number bounded by $\mathcal{O}\left(\frac{tm\alpha}{l} \right)^l$.&lt;/p&gt;

&lt;p&gt;The $\beta$ term comes due to technicalities associated with taking an activation function which is semi-algebraic rather than piecewise polynomial, but the proof technique remains the same.&lt;/p&gt;

&lt;h5 id=&#34;many-layers-many-oscillations&#34;&gt;Many layers, many oscillations&lt;/h5&gt;

&lt;p&gt;In the figure that I showed for explaining counting number, notice that oscillations usually (always?) mean repetitions of a triangle-like function (strictly increasing till some point and then strictly decreasing thereafter). Also, the usual functions computed by a single layer of most of the common neural networks are like these triangular functions.&lt;/p&gt;

&lt;p&gt;In the last result, we used the composition of $(t,\alpha)-poly$ functions across several layers to bound the counting number of a network. Similarly in this section, we will use the concept of a $(t,[a,b])$-triangle. It represents a function which is continuous in $[a,b]$ and consists of $t$ triangle-like pieces. Also, since this function oscillates $2t$ times, its counting number is $2t+1$.&lt;/p&gt;

&lt;p&gt;Now it remains to show that the composition of 2 such functions gives a similar function (which is a similar technique to what we used earlier). More formally, we will prove this claim.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; If $f$ is a $(s,[0,1])$-triangle and $g$ is a $(t,[0,1])$-triangle, then $f \circ g$ is a $(2st,[0,1])$-triangle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First, we note that $f \circ g$ is continuous in $[0,1]$ since a composition of continuous functions is continuous in the same domain.&lt;/p&gt;

&lt;p&gt;Now, consider any odd (i.e., strictly increasing) interval $g_j$ of $g$. Suppose $(a_1,\ldots,a_{2s+1})$ are the interval boundaries of $f$. Since the range of $g_j$ is $[0,1]$, $g_j^{-1}(a_i)$ exists for all $i$ and is unique, since $g_j$ is strictly increasing. Let $a_i^{\prime}=g_j^{-1}(a_i)$, i.e., $g_j(a_i^{\prime})=a_i$. If $i$ is odd, the composition $f \circ g_j(a_i^{\prime}) = f(a_i)=0$, and $f \circ g_j$ is strictly increasing in $[a_i^{\prime},a_{i+1}^{\prime}]$, since $g_j$ is strictly increasing everywhere and $f$ is strictly increasing in $[a_i,a_{i+1}]$. By a similar argument, if $i$ is even, $f \circ g_j$ is strictly decreasing along $[a_i^{\prime},a_{i+1}^{\prime}]$. In this way, we get $2s$ triangular pieces for a single $g_j$, and so the overall composition $f \circ g$ has $2st$ triangular pieces.&lt;/p&gt;

&lt;p&gt;Having shown this, it is easy to see that if there are $l$ layers and each layer computes a $(t,[0,1])$-triangle, the final layer will output a $((2t)^l,[0,1])$-triangle. In this way, the counting number of the overall function becomes $(2t)^l + 1$.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;implicit-acceleration-by-overparametrization&#34;&gt;Implicit acceleration by overparametrization&lt;/h3&gt;

&lt;p&gt;In the previous section, we have seen some results which show that depth plays a role in the expressive capacity of neural networks. Specifically, we saw that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Radial functions can be approximated by depth-3 networks but not with depth-2 networks.&lt;/li&gt;
&lt;li&gt;Functions expressible by $\theta(k^3)$-depth networks of constant width cannot be approximated by $\mathcal{O}(k)$-depth networks with polynomial width.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this section, we will look at a &lt;a href=&#34;https://arxiv.org/pdf/1802.06509.pdf&#34; target=&#34;_blank&#34;&gt;new paper from Arora, Cohen, and Hazan&lt;/a&gt; that suggests that, sometimes, increasing depth can speed up optimization (which is rather counterintuitive given the consensus on expressiveness vs. optimization trade-off), i.e., depth plays some role in convergence. Furthermore, this acceleration is more than what could be obtained by commonly used techniques, and is theoretically shown to be a combination of momentum and adaptive regularization (which we will discuss later).&lt;/p&gt;

&lt;p&gt;To isloate convergence from expressiveness, the authors focus solely on linear neural networks, where increasing depth has no impact on the expressiveness of the network. This is because in such networks, adding layers manifests itself only in the replacement of a matrix parameter by a product of matrices – an
overparameterization.&lt;/p&gt;

&lt;h4 id=&#34;equivalence-to-adaptive-learning-rate-and-momentum&#34;&gt;Equivalence to adaptive learning rate and momentum&lt;/h4&gt;

&lt;p&gt;The first result that we prove is the following.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Overparametrized gradient descent with small learning rate and near-zero initialization is equivalent to GD with adaptive learning rate and momentum terms.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; This can be seen by simple analysis of gradients for an $l_p$-regression with parameter $\mathbf{w}\in \mathbb{R}^d$. The loss function can be given as&lt;/p&gt;

&lt;p&gt;$$ L(\mathbf{w}) = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ \frac{1}{p}(\mathbf{x}^T\mathbf{w} - y)^p \right]. $$&lt;/p&gt;

&lt;p&gt;Now, if we add a scalar parameter, the new parameters are $\mathbf{w}_1$ and $w_2 \in \mathbb{R}$, i.e., $\mathbf{w} = w_2 \mathbf{w}_1$, and we can write the new loss function as&lt;/p&gt;

&lt;p&gt;$$ L(\mathbf{w}_1,w_2) = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ \frac{1}{p}(\mathbf{x}^T\mathbf{w}_1 w_2 - y)^p \right]. $$&lt;/p&gt;

&lt;p&gt;We can now compute the gradients of the objective with respect to the parameters as&lt;/p&gt;

&lt;p&gt;$$ \nabla_{\mathbf{w}} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w} - y)^{p-1}\mathbf{x} \right] $$&lt;/p&gt;

&lt;p&gt;$$ \nabla_{\mathbf{w}_1} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w}_1 w_2 - y)^{p-1}w_2\mathbf{x} \right] = w_2 \nabla_{\mathbf{w}} $$&lt;/p&gt;

&lt;p&gt;$$ \nabla_{w_2} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w} - y)^{p-1}\mathbf{w}_1^T \mathbf{x} \right] $$&lt;/p&gt;

&lt;p&gt;The update rules for $\mathbf{w}_1$ and $w_2$ can be given as&lt;/p&gt;

&lt;p&gt;$$ \mathbf{w}_1^{(t+1)} = \mathbf{w}_1^{(t)} - \eta \nabla_{\mathbf{w}_1}^{(t)} \quad \text{and} \quad w_2^{(t+1)} = w_2^{(t)} - \eta \nabla_{w_2}^{(t)}, $$&lt;/p&gt;

&lt;p&gt;and the updated parameter $\mathbf{w}$ is&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbf{w}^{(t+1)} &amp;amp;= \mathbf{w}_1^{(t+1)} w_2^{(t)} \\\ &amp;amp;= \left( \mathbf{w}_1^{(t)} - \eta \nabla_{\mathbf{w}_1}^{(t)} \right) \left( w_2^{(t)} - \eta \nabla_{w_2}^{(t)} \right) \\\ &amp;amp;= \mathbf{w}_1^{(t)}w_2^{(t)} - \eta w_2^{(t)}\nabla_{\mathbf{w}_1^{(t)}} - \eta \nabla_{w_2^{(t)}}\mathbf{w}_1^{(t)} + \mathcal{O}(\eta^2) \\\ &amp;amp;= \mathbf{w}^{(t)} - \eta \left( w_2^{(t)} \right)^2 \nabla_{\mathbf{w}^{(t)}} -\eta \left( w_2^{(t)} \right)^{-1} \nabla_{w_2^{(t)}} \mathbf{w}^{(t)} + \mathcal{O}(\eta^2). \end{align}$$&lt;/p&gt;

&lt;p&gt;We can ignore $\mathcal{O}(\eta^2)$ since the learning rate is assumed to be low. Also, we take $\rho^{(t)} = \eta(w_2^{(t)})^2$ and $\gamma^{(t)}=\eta(w_2^{(t)})^{-1}\nabla_{w_2^{(t)}}$, so the update becomes&lt;/p&gt;

&lt;p&gt;$$ \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \rho^{(t)}\nabla_{\mathbf{w}^{(t)}} - \gamma^{(t)}\mathbf{w}^{(t)}. $$&lt;/p&gt;

&lt;p&gt;Since $\mathbf{w}$ is initialized near $0$, it is essentially a weighted combination of the past gradients at any given time, i.e., $\gamma^{(t)}\mathbf{w}^{(t)} = \sum_{\tau=1}^{t-1}\mu^{(t,\tau)}\nabla_{\mathbf{w}^{(\tau)}}$.&lt;/p&gt;

&lt;p&gt;This is similar to the momentum term in the popular momentum algorithm for optimization (see &lt;a href=&#34;https://desh2608.github.io/post/short-note-sgd-algorithms/&#34; target=&#34;_blank&#34;&gt;this earlier post&lt;/a&gt; for an overview), and the learning rate term $\rho^{(t)}$ is time-varying and adaptive.&lt;/p&gt;

&lt;h4 id=&#34;update-rule-for-end-to-end-matrix&#34;&gt;Update rule for end-to-end matrix&lt;/h4&gt;

&lt;p&gt;The next derivation is a little more involved, and I defer the reader to the actual paper for the detailed proof. I will give a brief outline here.&lt;/p&gt;

&lt;p&gt;Suppose we have a depth-$N$ linear network such that the weight matrices are given by $W_1,\ldots,W_N$. Let $W_e$ denote the final end-to-end update matrix. The authors use differential techniques to compute an update rule for $W_e$. For this, the important assumption is that $\eta^2 \approx 0$. When step sizes are taken to be small, trajectories of discrete optimization algorithms converge to smooth curves modeled by continuous-time differential equations.&lt;/p&gt;

&lt;p&gt;After obtaining such a differential equation, integration over the $N$ layers gives the derivative of $W_e$, which is then transformed back to the discrete update rule given as&lt;/p&gt;

&lt;p&gt;$$ W_e^{(t+1)} = (1 - \eta\lambda N)W_e^{(t)} - \eta \sum_{i=1}^N \left[ W_e^{(t)} (W_e^{(t)})^T \right]^{\frac{j-1}{N}} \frac{\partial L^1}{\partial W}(W_e^{(t)}) \cdot \left[ (W_e^{(t)})^T W_e^{(t)} \right]^{\frac{N-j}{N}}. $$&lt;/p&gt;

&lt;p&gt;Let us break down this expression. The first part is similar to a weight-decay term for a 1-layer update. The second part also has the derivative w.r.t parameters, but it is multiplied by some preconditioning terms. On further inspection of these terms, it is found that their eigenvalues and eigenvectors depend on the singular value decomposition of $W_e$. Qualitatively, this means that these multipliers favor the gradient along those directions that correspond to singular values whose presence in $W_e$ is stronger. If we assume, as is usually the case in deep learning, that the initialization was near 0, this means that these multipliers act similar to acceleration and push the gradient along the direction already taken by the optimization.&lt;/p&gt;

&lt;p&gt;For further reading, check out the author&amp;rsquo;s &lt;a href=&#34;http://www.offconvex.org/2018/03/02/acceleration-overparameterization/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; about the paper.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To summarize, we looked at three recent papers which prove results on the role of depth in expressibility and optimization of neural networks. People often think that working on the mathematics of deep learning would require complex group theory formalisms and difficult techniques in high-dimensional probability, but as we saw in the proofs of some of these results (especially in Telgarsky&amp;rsquo;s paper), a lot can be achieved using simple counting logic and concentration inequalities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Generalization</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-2/</link>
      <pubDate>Fri, 27 Jul 2018 13:45:11 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-2/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;Part 1&lt;/a&gt; of this series, based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, we looked at several aspects of optimization of the nonconvex objective function that is a part of most deep learning models. In this article, we will turn our attention to another important aspect, namely generalization.&lt;/p&gt;

&lt;p&gt;A distinguishing feature of most modern deep learning architectures is that they generalize to test cases exceptionally well, even though the number of parameters is far greater than the number of training samples. VGG19, for instance, which has approximately 20 million weights to be tuned, gives $\sim 93\%$ classification accuracy on CIFAR-10, which has only 50000 training images. If you have studied statistical learning theory (see my &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;previous&lt;/a&gt; &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;blogs&lt;/a&gt; on the topic), this behavior is extremely counter-intuitive, and begs the question: &lt;em&gt;why don&amp;rsquo;t deep neural networks overfit even with small number of training samples?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we try to understand the reason, let us look at a popular folklore experiment that is described in &lt;a href=&#34;http://www.cs.princeton.edu/~rlivni/files/papers/LivnComputational.pdf&#34; target=&#34;_blank&#34;&gt;Livni et al &amp;lsquo;14&lt;/a&gt; related to over-specification.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For sufficiently over-specified networks, global optima are ubiquitous and in general computationally easy to find.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see this, we fix a depth-2 neural network (i.e. a network with 1 hidden layer) consisting of $n$ hidden nodes. We provide random inputs to the network and obtain their corresponding output. Now, take a randomly initialized neural network with the same architecture as the above, and train it using the input-output pairs obtained earlier. It is found that this is really difficult to achieve. However, if we take a large number of hidden nodes, the training becomes easier.&lt;/p&gt;

&lt;p&gt;Although this result has been known and verified empirically for some time, it remains to be proven theoretically. This is a striking example of the difficulty of proving generalization guarantees in deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;effective-capacity-of-learning&#34;&gt;Effective capacity of learning&lt;/h3&gt;

&lt;p&gt;The capacity of a learning model, in an abstract sense, means the complexity of training samples that it can fit. For instance, a quadratic regression has inherently more capacity than linear regression, but is also more prone to overfitting. Furthermore, the effective capacity can be thought of as analogous to the number of bits required to represent all possible states that the hypothesis class contains. For this reason, the capacity is approximately the log of the number of apriori functions in the hypothesis class.&lt;/p&gt;

&lt;p&gt;We will now see a general result that is true for learning models including deep neural networks.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; Test loss - training loss $\leq \sqrt{\frac{N}{m}}$, where $N$ is the effective capacity and $m$ is the number of training samples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First let us fix our neural network $\theta$ and its parameters. Suppose we take an i.i.d sample $S$ containing $m$ data points. Consider &lt;em&gt;Hoeffding&amp;rsquo;s inequality&lt;/em&gt;: If $x_1,\ldots,x_m$ are $m$ i.i.d samples of a random variable $X$ distributed by $P$, and $a\leq x_i \leq b$ for every $i$, then for a small postive non-zero value $\epsilon$:&lt;/p&gt;

&lt;p&gt;$$ P\left( \mathbb{E}_{X \sim P} - \frac{1}{m}\sum_{i=1}^m x_i \right) \leq 2\exp \left( \frac{-2m\epsilon^2}{(b-a)^2} \right) $$&lt;/p&gt;

&lt;p&gt;We can apply this inequality to our generalization probability, assuming that our errors are bounded between 0 and 1 (which is a reasonable assumption, as we can get that using a 0/1 loss function or by squashing any other loss between 0 and 1) and get for a single hypothesis $h$:&lt;/p&gt;

&lt;p&gt;$$ P(|R(h) - \hat{R}(h)| &amp;gt; \epsilon) \leq 2\exp (-2m\epsilon^2), $$&lt;/p&gt;

&lt;p&gt;where $R(h)$ denotes generalization error and $\hat{R}(h)$ denotes empirical error on the sample.&lt;/p&gt;

&lt;p&gt;However, this is not the true generalization bound. This is because we have first fixed out network and we are then choosing the sample i.i.d. However, in a real learning problem, we are given the sample $S$ and we have to learn the parameters to best fit this sample. Therefore, to obtain the actual generalization bound, we take the union bound over all possible neural net configurations $\mathcal{W}$. Now, equating the RHS with the confidence $\delta$, we get&lt;/p&gt;

&lt;p&gt;$$ \begin{align} &amp;amp; 2\mathcal{W}\exp(-2m\epsilon^2) \leq \delta \\\ \Rightarrow &amp;amp; -2m\epsilon^2 \leq \log \frac{\delta}{2\mathcal{W}} \\\ \Rightarrow &amp;amp; \epsilon \geq \sqrt{\frac{\log \frac{2\mathcal{W}}{\delta}}{2m}}, \end{align} $$&lt;/p&gt;

&lt;p&gt;which completes the proof.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In statistical learning theory, the most popular metrics for measuring the capacity of a model are Rademacher complexity and VC dimension, which I have explained in &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. I will quickly summarize them here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rademacher complexity:&lt;/strong&gt; It is a measure of how well the model can fit a random assignment of labels. Its mathematical formulation is:&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathcal{R}_S}(G) = \mathbb{E}_{\sigma}[\text{sup}_{g\in G}\frac{1}{m}\sigma_i g(z_i)] $$&lt;/p&gt;

&lt;p&gt;Essentially, it denotes an expectation of the best possible average correlation that the random labels have with any function present in the hypothesis class $G$. Therefore, a higher Rademacher complexity would imply that the function class $G$ is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class $G$ is, higher is the probability that it would have some $g$ which correlates well with random noise.&lt;/p&gt;

&lt;p&gt;The generalization error $R(h)$ can be written in terms of R.C. as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{R}_m(H) + \sqrt{\frac{\log \frac{1}{\delta}}{2m}}, $$&lt;/p&gt;

&lt;p&gt;where $\hat{R}(h)$ is the empirical error, $\delta$ is the confidence, and $m$ is the number of training samples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VC dimension:&lt;/strong&gt; It is the size of the largest set that can be fully shattered by $G$. By shattering, we mean that $G$ can classify the given set in all possible ways. As such, higher the VC-dimension, more is the capacity of the hypothesis class. We can bound the generalization error in terms of the VC-dimension of the hypothesis class as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{O}\left( \sqrt{\frac{\log(m/d)}{m/d}} \right) $$&lt;/p&gt;

&lt;p&gt;Although these metrics are well established in learning theory, they fail for deep neural networks since they are usually equally vacuous, i.e, the upper bound is greater than 1. This means that the bounds are so large that they are meaningless, since error can never exceed 1, and in practice the generalization error of the networks is many orders of magnitude less than these bounds.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;deep-networks-have-excess-capacity&#34;&gt;Deep networks have &amp;ldquo;excess capacity&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;As mentioned earlier, deep neural networks generalize surprisingly well despite having a huge number of parameters. They can be shown by the dotted red line (figure taken from tutorial slides) in the following popular figure which is often found in textbooks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/generalize.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Other learning models with a &amp;ldquo;high capacity&amp;rdquo; would follow the general trend and fail to generalize well, which may be evidence that somehow, the large number of parameters in deep networks is not necessarily translating to a high capacity. For a long time, it was believed that a combination of stochastic gradient descent and regularization eliminates the &amp;ldquo;excess capacity&amp;rdquo; of the neural network.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But this belief is wrong!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In their &lt;a href=&#34;https://arxiv.org/abs/1611.03530&#34; target=&#34;_blank&#34;&gt;ICLR &amp;lsquo;17 paper&lt;/a&gt; (which I have previously discussed in &lt;a href=&#34;https://desh2608.github.io/post/best-papers-at-iclr-17/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;), Zhang et. al., in a series of well-designed experiments, showed that deep networks do retain this excess capacity. From &lt;a href=&#34;http://www.offconvex.org/2017/12/08/generalization1/&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s blog post&lt;/a&gt; on the subject: &amp;ldquo;Their main experimental finding is that if you take a classic convnet architecture, say Alexnet, and train it on images with random labels, then you can still achieve very high accuracy on the training data. (Furthermore, usual regularization strategies, which are believed to promote better generalization, do not help much.) Needless to say, the trained net is subsequently unable to predict the (random) labels of still-unseen images, which means it doesn’t generalize.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/iclr-17.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An interesting (and provable) guarantee that the paper contains is the following theorem: &lt;em&gt;There exists a two-layer neural network with ReLU activations and $2n+d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&#34;https://arxiv.org/abs/1802.01396&#34; target=&#34;_blank&#34;&gt;related paper&lt;/a&gt; published recently, it was shown that the &amp;ldquo;excess capacity&amp;rdquo; is not just limited to deep networks, since even linear models possess this feature. Furthermore, when it comes to fitting noise, there are some interesting similarities between Laplacian kernel machines and ReLU networks. But before we get to that, I will briefly define Laplacian and Gaussian kernels. (For an overview of several kernel functions, check out &lt;a href=&#34;http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/&#34; target=&#34;_blank&#34;&gt;this article&lt;/a&gt;.)&lt;/p&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;kernel-methods&#34;&gt;Kernel Methods&lt;/h4&gt;

&lt;p&gt;Kernel methods map the data into higher-dimensional spaces, in the hope that in this higher-dimensional space the data could become more easily separated or better structured. However, when we talk about transforming data to a higher dimension, called a $z$-space, an actual transformation would involve paying computation costs. To avoid this, we need to look at what we actually want from the $z$-space.&lt;/p&gt;

&lt;p&gt;Support Vector Machines (SVMs), which are among the most popular kernel-based methods for classification, involve solving for the following Lagrangian.&lt;/p&gt;

&lt;p&gt;$$ \mathcal{L}(\alpha) = \sum_{n=1}^N \alpha_n - \frac{1}{2}\sum_{n=1}^N \sum_{m=1}^M y_n y_m \alpha_n \alpha_m z_n^T z_m $$&lt;/p&gt;

&lt;p&gt;under the constraints $\alpha_n \geq 0 \forall n$ and $\sum_{n=1}^N \alpha_n y_n = 0$. On solving this, we get the boundary as&lt;/p&gt;

&lt;p&gt;$$ g(x) = \text{sgn}(w^T z + b) $$&lt;/p&gt;

&lt;p&gt;where $w = \sum_{z_n \in SV} \alpha_n y_n z_n$.&lt;/p&gt;

&lt;p&gt;We can see from this that the only value we need from the $z$-space is the inner product $z^T z^{\prime}$. If we can show that obtaining this inner product is possible without actually going to the $z$-space, we are done.&lt;/p&gt;

&lt;p&gt;It turns out that this is indeed possible, and there are several such functions, known as &lt;strong&gt;kernel functions&lt;/strong&gt;, which can be written as the inner product in some space. The only constraint on the $z$-space is that it should exist. Interestingly, kernels such as the radial basis function (RBF) kernel exist in an $\infty$-dimensional space. Furthermore, in order for the problem to be convex and have a unique solution, it is important to select a positive semi-definite kernel, i.e., whose kernel matrix contain only non-negative eigenvalues. Such a kernel is said to obey &lt;a href=&#34;https://en.wikipedia.org/wiki/Mercer&#39;s_theorem&#34; target=&#34;_blank&#34;&gt;Mercer&amp;rsquo;s theorem&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we have some idea what kernels are, let us look at Laplacian and Gaussian kernels.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Laplacian kernel:&lt;/strong&gt; It is mathematically defined as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ K(x,y) = \exp \left( - \frac{\lVert x-y \rVert}{\sigma} \right). $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gaussian kernel:&lt;/strong&gt; Its mathematical formulation is&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ K(x,y) = \exp \left( - \frac{\lVert x-y \rVert^2}{2\sigma^2} \right). $$&lt;/p&gt;

&lt;p&gt;Both the Laplacian and Gaussian kernels are examples of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Radial_basis_function&#34; target=&#34;_blank&#34;&gt;radial basis function&lt;/a&gt; kernels. The difference lies only in the parameter $\sigma$. Since the Gaussian depends on the square of this parameter, it is more sensitive to changes in $\sigma$ than the Laplacian.&lt;/p&gt;

&lt;p&gt;The authors found in their empirical evaluations that Laplacian kernels were much more adept at fitting random labels than Gaussian kernels. This property may be attributed to the inherent non-smoothness of Laplacians as opposed to the Gaussians being smooth. This discontinuity in derivative is reminiscent of that for ReLU units, which, as we saw above, were found to fit random labels exceptionally well. As such, the conjecture is that the radial structure of the kernels, as opposed to the specifics of optimization, plays a key role in ensuring strong classification performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/laplace.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Another take-away from this paper is that they establish stronger bounds for classification performance of kernel methods. If understanding kernels can indeed lead to a better understanding of deep learning, then maybe these bounds will lead to tighter bounds for the effective capactity of deep neural networks.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;other-notions-of-generalizability&#34;&gt;Other notions of generalizability&lt;/h3&gt;

&lt;p&gt;We now look at 2 other concepts that seek to explain why deep neural networks generalize well: flat minima, and noise stability.&lt;/p&gt;

&lt;h4 id=&#34;flat-minima&#34;&gt;Flat minima&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/minima.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/899-simplifying-neural-nets-by-discovering-flat-minima.pdf&#34; target=&#34;_blank&#34;&gt;Hochreiter and Schmidhuber&lt;/a&gt; first conjectured that the flatness of the local minima found by the stochastic gradient descent may be an indicator of its generalization performance. Sharpness of a minimizer can be characterized by the magnitude of the eigenvalues of $\nabla^2 f(x)$, but since the computation of this quantity is expensive, &lt;a href=&#34;https://arxiv.org/pdf/1609.04836.pdf&#34; target=&#34;_blank&#34;&gt;Keskar et. al.&lt;/a&gt; defined a new metric for sharpness that is easier to compute.&lt;/p&gt;

&lt;p&gt;Given $x \in \mathbb{R}^n$, $\epsilon &amp;gt; 0$, and $A \in \mathbb{R}^{n \times p}$, the $(C_{\epsilon},A)$-sharpness of $f$ at $x$ is defined as&lt;/p&gt;

&lt;p&gt;$$ \phi_{x,f}(\epsilon,A) = \frac{(\max_{y\in C_{\epsilon}} f(x+Ay))-f(x)}{1+f(x)}\times 100 $$&lt;/p&gt;

&lt;p&gt;The metric is based on exploring a small neighborhood of a solution and computing the largest value that $f$ can attain in that neighborhood. We use that value to measure the sensitivity of the training function at the given local minimizer.&lt;/p&gt;

&lt;p&gt;Intuitively, flat minima have lower description lengths (since less information is required to represent a flat surface), and consequently, fewer number of models are possible with this length. The effective capactiy thus becomes less, and so the hypothesis is able to generalize well.&lt;/p&gt;

&lt;p&gt;However, &lt;a href=&#34;https://arxiv.org/abs/1703.04933&#34; target=&#34;_blank&#34;&gt;recent research&lt;/a&gt; suggests that flatness is sensitive to reparametrizations of the neural network: we can reparametrize a neural network without changing its outputs while making sharp minima look arbitrarily flat and vice versa. As a consequence the flatness alone cannot explain or predict good generalization.&lt;/p&gt;

&lt;p&gt;As Prof. Arora pointed out in his talk, most of the existing theory that tries to explain generalization is only doing a &amp;ldquo;postmortem analysis&amp;rdquo;. This means that they look at some property $\phi$ that is seemingly possessed by a few neural networks that generalize well, and they argue that the generalization is due to this property. The notion of &amp;ldquo;flat minima&amp;rdquo; is a prime example of this. However, &lt;em&gt;correlation is not causation.&lt;/em&gt; Instead of such a qualitative check, the theoretical approach would be to use the property $\phi$ to compute an upper bound on the number of possible neural networks that would generalize well with this property. This computation is very nontrivial and is therefore ignored.&lt;/p&gt;

&lt;h4 id=&#34;noise-stability&#34;&gt;Noise stability&lt;/h4&gt;

&lt;p&gt;While flat minima was an old concept, the notion of noise stability is a very recent formalization for the same, proposed in &lt;a href=&#34;https://arxiv.org/abs/1802.05296&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s ICML&amp;rsquo;18 paper&lt;/a&gt;. Essentially, it means that if we add some zero-mean Gaussian noise at an intermediate output of a neural network, the noise gets attenuated as the signal moves to higher layers. Therefore, the capacity of a network to fit random noise can be measured by adding a Gaussian noise at an intermediate layer and measuring the change in output at higher layers.&lt;/p&gt;

&lt;p&gt;This is also biologically inspired, since neurologists believe that single neurons are extremely susceptible to errors. However, the fact that we still function well suggests that there must be some mechanism to attenuate these errors.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Noise stability implies compressibility.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, what is meant by compression of a neural network? Given a network $C$ with $N$ parameters and some training loss, compression means obtaining a new network $C^{\prime}$ containing $N^{\prime}$ parameters ($N^{\prime} &amp;lt; N$), such that the training loss effectively remains the same. From the generalization claim proved earlier, this compression would mean better generalization capability for the network $C^{\prime}$.&lt;/p&gt;

&lt;p&gt;Now, let us consider a depth-2 network consisting only of linear transformations. This network can be represented by some matrix $M$, which transforms input $x$ to output $Mx$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/compression.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the above figure, $\eta$ is a zero-mean Gaussian noise that is added to the input. We say that the matrix $M$ is noise stable, i.e. $M(x+\eta)\approx Mx$. This means that $\frac{|Mx|}{|x|} &amp;gt;&amp;gt; \frac{|M\eta|}{|\eta|}$. Here, the value $\frac{|Mx|}{|x|}$ is at most equal to the largest singular value of $M$, which we denote by $\sigma_{\max}(M)$. The RHS is approximately $\frac{(\sum_i (\sigma_i (M))^2)^{\frac{1}{2}}}{\sqrt{n}}$ where $\sigma_i(M)$ is the $i$th singular value of $M$ and $n$ is dimension of $Mx$. The reason is that gaussian noise divides itself evenly across all directions, with variance in each direction $1/n$. Thus,&lt;/p&gt;

&lt;p&gt;$$ (\sigma_{max}(M))^2 \gg \frac{1}{h} \sum_i (\sigma_i(M)^2) $$&lt;/p&gt;

&lt;p&gt;The ratio of the LHS to the RHS in the above inequality is known as the &lt;em&gt;stable rank&lt;/em&gt;. Higher the stable rank, more uneven is the distribution of singular values in the matrix. This is easily seen since the highest singular value is much larger than the RMS of all the singular values, something similar to the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/singular.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The actual signal $x$ is usually correlated with the eigenvectors corresponding to the larger singular values, and as such, the other directions can be ignored without any loss in performance. This is similar to feature selection by a principal component analysis approach.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;nonvacuous-bounds-for-true-capacity&#34;&gt;Nonvacuous bounds for true capacity&lt;/h3&gt;

&lt;p&gt;We have earlier seen that most of the classical metrics used for bounding the generalization error in learning systems prove to be vacuous in case of deep neural networks. The following blog posts by Prof. Arora discuss this issue in some detail and also introduce a new generalization bound based on the compressibility of neural networks explained in the previous section.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2017/12/08/generalization1/&#34; target=&#34;_blank&#34;&gt;Generalization theory and deep nets, an introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2018/02/17/generalization2/&#34; target=&#34;_blank&#34;&gt;Proving generalization of deep nets via compression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this section, I will discuss two approaches for computing nonvacuous bounds for deep networks. The first is from &lt;a href=&#34;https://arxiv.org/pdf/1703.11008.pdf&#34; target=&#34;_blank&#34;&gt;Dziugaite and Roy&lt;/a&gt;, and the second is from &lt;a href=&#34;https://arxiv.org/pdf/1802.05296.pdf&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s ICML&amp;rsquo;18 paper&lt;/a&gt; mentioned previously.&lt;/p&gt;

&lt;p&gt;As discussed earlier, a common framework for addressing this problem would involve showing under certain assumptions that either SGD performs implicit regularization, or that it finds a solution with some known structure connected to regularization. Once this is found, a nonvacuous bound for the generalization error of such models would have to be determined.&lt;/p&gt;

&lt;h4 id=&#34;1-pac-bayes-approach&#34;&gt;1. PAC-Bayes approach&lt;/h4&gt;

&lt;p&gt;The first question is how to identify structure in the solutions found by SGD? For this, we again turn to the old notion of flat minima. If SGD finds a flat minima, it means that the solution is surrounded by a large volume of solutions that are nearly as good. If we then represent these nearby solutions by some distribution and pick an average classifier from this distribution, it would be very likely that its generalization error is very close to that of the true solution.&lt;/p&gt;

&lt;p&gt;This concept is very similar to the PAC-Bayes theorem, which informally bounds the expected error of a classifier chosen from a distribution $Q$ in terms of its KL divergence from a priori fixed distribution $P$. But first, &lt;em&gt;what is KL divergence?&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;h5 id=&#34;kullback-leibler-divergence&#34;&gt;Kullback-Leibler divergence&lt;/h5&gt;

&lt;p&gt;It is a metric that compares the similarity between two probability distributions. Mathematically, it is the expectation of the log difference between the probability of data in the original distribution $p$ and the approximating distribution $q$.&lt;/p&gt;

&lt;p&gt;$$ \begin{align} KL(p||q) &amp;amp;= \mathbb{E}(\log p(x) - \log q(x)) \\\ &amp;amp;= \sum_{i=1}^N p(x_i)(\log p(x_i) - \log q(x_i)) \end{align}$$&lt;/p&gt;

&lt;p&gt;In information theory, the most important notion is that of &lt;strong&gt;entropy&lt;/strong&gt;, which represents the minimum number of bits required to encode some information, and is mathematically represented as&lt;/p&gt;

&lt;p&gt;$$ H = -\sum_{i=1}^N p(x_i)\log p(x_i). $$&lt;/p&gt;

&lt;p&gt;As such, the KL divergence can be seen to compute how many bits of information will be lost in approximating a distribution $p$ with another distribution $q$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The PAC-Bayes bound is given as&lt;/p&gt;

&lt;p&gt;$$ KL(\hat{e}(Q,S_m)||e(Q)) \leq \frac{KL(Q||P)+\log \frac{m}{\delta}}{m-1}, $$&lt;/p&gt;

&lt;p&gt;where $\hat{e}(Q,S_m)$ is the empirical loss of $Q$ w.r.t some i.i.d sample $S_m$, and $e(Q)$ is the expected loss. If we now find a $Q$ that minimizes this value, we are likely to find a minima that generalizes well and has a nonvacuous bound. This is exactly what is proposed in the paper.&lt;/p&gt;

&lt;p&gt;On a binary variant of MNIST, the computed PAC-Bayes bounds on the test error are in the range 16-22%. While this is a loose bound (actual bounds are around 3%), it is still surprising to find a non-trivial numerical bound for a model with such a large capacity on so few training examples. The authors comment that these are, in all likelihood, &amp;ldquo;the first explicit and nonvacuous numerical bounds computed for trained neural networks in the deep learning regime&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;2-compressibility-approach&#34;&gt;2. Compressibility approach&lt;/h4&gt;

&lt;p&gt;Although the PAC-Bayes bound is nonvacuous, it is still looser than actual sample complexity bounds computed empirically. Instead, Arora et al. introduce a new &lt;em&gt;compression framework&lt;/em&gt; to address this problem. Earlier while discussing noise stability, we have already seen that if we can compress a classifier $f$ without decreasing the empirical loss, it becomes much more generalizable according to the fundamental theorem proved earlier.&lt;/p&gt;

&lt;p&gt;We say that $f$ is $(\gamma,S)$-compressible using helper string $s$ if there exists some other classifier $g_{A,s}$ on a class of parameters $A$ such that the classification loss of $f$ on every $x \in S$ differs from that of $g_{A,s}$ by at most $\gamma$. Here, $s$ is fixed before looking at the training sample, and is often just for randomization.&lt;/p&gt;

&lt;p&gt;Then, the main theorem in the paper is as follows: If $f$ is $(\gamma,S)$-compressible using helper string $s$, then with high probability,&lt;/p&gt;

&lt;p&gt;$$ L_0 (g_A) \leq \hat{L}_{\gamma}(f) + \mathcal{O}\left( \sqrt{\frac{q \log r}{m}} \right), $$&lt;/p&gt;

&lt;p&gt;where $A$ is a set of $q$ parameters each having at most $r$ discrete values, $L_0 (g_A)$ is the generalization loss of compressed classifier, and $\hat{L}_{\gamma}(f)$ is the empirical estimate of the marginal loss of original classifier. Note that the bound is for the compressed classifier, but the same is also true for earlier works (like the PAC-Bayes approach). The proof is very elementary and uses just simple concentration inequalities.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First, using Hoeffding&amp;rsquo;s inequality, we can write&lt;/p&gt;

&lt;p&gt;$$ P(L_0 (g_A) - \hat{L}_0 (g_A) \geq \epsilon) \leq 2\exp(-2m\epsilon^2). $$&lt;/p&gt;

&lt;p&gt;Taking $\epsilon = \sqrt{\frac{q \log r}{m}}$, we get, with probability at least $1 - \exp(-2q\log r)$,&lt;/p&gt;

&lt;p&gt;$$ L_0 (g_A) \leq \hat{L}_0 (g_A) + \mathcal{O}\left( \sqrt{\frac{q \log r}{m}} \right). $$&lt;/p&gt;

&lt;p&gt;Next, by definition of $(\gamma,S)$-compressibility, we can write&lt;/p&gt;

&lt;p&gt;$$ \lvert f(x)[y] - g_A(x)[y] \rvert \leq \gamma. $$&lt;/p&gt;

&lt;p&gt;This means that as long as the original function has margin at least $\gamma$, the new function classifies the example correctly. Therefore,&lt;/p&gt;

&lt;p&gt;$$ \hat{L}_0 (g_A) \leq \hat{L}_{\gamma}(f). $$&lt;/p&gt;

&lt;p&gt;Combining this with the earlier inequality, we immediately get the result.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to providing a tighter generalization bound for fully connected networks, the paper also proposes some theory for convolutional nets, which have been notoroiusly difficult to theorize. For details, readers are suggested to refer to the paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Optimization</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-1/</link>
      <pubDate>Thu, 26 Jul 2018 11:15:18 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-1/</guid>
      <description>

&lt;p&gt;I only just got around to watching the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;. In this and the next few posts, I will discuss the subject in some detail, including the referenced papers and blogs. Very conveniently, the talk itself was divided into 5 parts, and I will structure this series accordingly.&lt;/p&gt;

&lt;p&gt;At the outset, we should understand that a number of important concepts in deep learning are already shaped by optimization theory. Backpropagation, for instance, is basically just a linear time dynamic programming algorithm to compute gradient. Recent methods for gradient descent, such as momentum, Adagrad, etc. (see &lt;a href=&#34;https://desh2608.github.io/post/short-note-sgd-algorithms/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; for a quick overview) are obtained from convex optimization techniques. However, over the last decade, the deep learning community has come up with several models based on intuition mostly, that do not have any theoretical support yet.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The goal, then, is to find theorems that support these intuitions, leading to new insights and concepts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this first part of the series, we will try to understand why (and how) deep learning almost always finds decent solutions to problems that are highly nonconvex.&lt;/p&gt;

&lt;h3 id=&#34;possible-goals-for-optimization&#34;&gt;Possible goals for optimization&lt;/h3&gt;

&lt;p&gt;Any neural network essentially tries to minimize a loss function. However, in almost all cases, this loss function is highly nonconvex (and sometimes NP-hard), which means that no provably polytime algorithm exists for its optimization. Even so, deep networks are quite adept at finding an approximately good solution.&lt;/p&gt;

&lt;p&gt;Whenever the gradient $\nabla$ is non-zero, there exists a descent direction. As such, a possible goal for the network may be any of the following (in increasing order of difficulty):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Finding a critical point, i.e. $\nabla = 0$.&lt;/li&gt;
&lt;li&gt;Finding a local optimum, i.e. $\nabla = 0$ and $\nabla^2$ is positive semi-definite.&lt;/li&gt;
&lt;li&gt;Finding a global optimum.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Furthermore, this descent may be from several possible initializations, namely all points, random points, or specially-chosen points. Now, if there are $d$ parameters (weights) to be optimized, we say that the problem is in $\mathbb{R}^d$ space. It is usually visualized by the following sea-urchin figure (or a $d$-urchin figure, according to Prof. Arora).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/21/high-dim-space.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In $\mathbb{R}^d$ space, there exit exp($d$) directions which can be explored to find the optimal solution, which makes the naive approach infeasible. Also, we cannot use non black box approaches to prune the number of explorations, since there is no clean mathematical formulation for the problem.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But what does this mean?&lt;/em&gt; This means that problems in deep learning are usually of the kind where, given pixels of an image, you have to label it as a cat or a dog. Such an $(x_i,y_i)$ has no mathematical meaning. This means that we do not understand the inherent landscape of the problem we are trying to solve, and so no special pruning can be done.&lt;/p&gt;

&lt;p&gt;This, combined with the nonconvex nature of the loss function, also means that it becomes infeasible to find a global optimum for the optimization problem. As such, we have to settle for goals 1 and 2, i.e. a critical point or a local optimum.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;finding-critical-points&#34;&gt;Finding critical points&lt;/h3&gt;

&lt;p&gt;The update function for a parameter $\theta$ is given as&lt;/p&gt;

&lt;p&gt;$$ \theta_{t+1} = \theta_t - \eta \nabla f(\theta_t) $$&lt;/p&gt;

&lt;p&gt;If the second derivative $\nabla^2$ is high, $\nabla f(\theta_t)$ will vary a lot, and we may miss the actual critical point. To prevent this, it is advisable to take &lt;em&gt;small&lt;/em&gt; steps.&lt;/p&gt;

&lt;p&gt;But how do we quantify small? In other words, &lt;em&gt;how do we determine a good learning rate for the optimization problem&lt;/em&gt;? For this, we again look at $\nabla^2$, which will determine the smoothness of the function. Suppose there exists a $\beta$ such that the Hessian $-\beta I \leq \nabla^2 f(\theta) \leq \beta I$, where $I$ is the identity matrix. Essentially, a higher $\beta$ means that $\nabla^2$ varies more, and so the learning rate should be lower. From this understanding, we can prove the following claim.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim (&lt;a href=&#34;https://rd.springer.com/book/10.1007%2F978-1-4419-8853-9&#34; target=&#34;_blank&#34;&gt;Nesterov 1998&lt;/a&gt;):&lt;/strong&gt; If we choose $\eta = \frac{1}{2\beta}$, we can achieve $|\nabla f|&amp;lt;\epsilon$ in number of steps proportional to $\frac{\beta}{\epsilon^2}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; See the proof of Lemma 2.8 &lt;a href=&#34;https://ee227c.github.io/notes/ee227c-notes.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; (see Definition 2.7). So a single update reduces the function value by at least $\frac{\epsilon^2}{2\beta}$. Therefore, it would take $\mathcal{O}(\frac{\beta}{\epsilon^2})$ steps to arrive at a critical point.&lt;/p&gt;

&lt;h4 id=&#34;evading-saddle-points&#34;&gt;Evading saddle points&lt;/h4&gt;

&lt;p&gt;While we have a theoretical upper limit for the time taken for convergence at a critical point, this is still problematic since it may be a saddle point, i.e., the function value is minimum in $d-1$ directions but maximum in one direction. Such a surface literally looks like a saddle as follows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/21/saddle-point.png&#34; alt=&#34;Saddle point&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An important question, then, is how to evade saddle points while looking for critical points. This question is explored in a series of papers and corresponding blog posts on &lt;a href=&#34;www.offconvex.org&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s blog&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2016/03/22/saddlepoints/&#34; target=&#34;_blank&#34;&gt;Polynomial time guarantee for GD to escape saddle points&lt;/a&gt; (based on &lt;a href=&#34;http://proceedings.mlr.press/v40/Ge15.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2016/03/24/saddles-again/&#34; target=&#34;_blank&#34;&gt;Random initialization for asymptotically avoiding saddle points&lt;/a&gt; (based on &lt;a href=&#34;https://arxiv.org/pdf/1602.04915.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2017/07/19/saddle-efficiency/&#34; target=&#34;_blank&#34;&gt;Perturbing gradient descent&lt;/a&gt; (based on &lt;a href=&#34;https://arxiv.org/pdf/1703.00887.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here I will try to summarize these discussions in several bullet points.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Most learning problems have exponentially many saddle points.&lt;/em&gt; Learning problems usually involve searching for $k$ components, for example clustering, $k$-node hidden layer in a neural network, etc. Suppose $(x_1,x_2,\ldots,x_k)$ is an optimal solution. Then, $(x_2,x_1,\ldots,x_k)$ is also an optimal solution, but the mean of these is not an optimal solution. This suffices to show that the learning problem is nonconvex, since for a convex function, the average of optimal solutions is also optimal. Furthermore, we can keep swapping the $k$ components to obtain exponential optimal solutions. Saddle points lie on the paths joining these isolated solutions, and hence, are exponential in number themselves.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Hessians can be used to evade saddle points.&lt;/em&gt; Consider the second order Taylor expansion given below. If there exists a direction where $\frac{1}{2}(y-x)^T \nabla^2 f(x)(y-x)$ is significantly less than 0, then using this update rule can avoid saddle points. Such saddle points are called &amp;ldquo;strict,&amp;rdquo; and for these, methods such as trust region algorithms and cubic regularization can find the local optimum.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ f(y) = f(x) + &amp;lt;\nabla f(x), y-x&amp;gt; + \frac{1}{2}(y-x)^T \nabla^2 f(x)(y-x) $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Noisy gradient descent converges to local optimum in polynomial number of steps.&lt;/em&gt; Although the Hessian method provides a theoretical way to escape saddle points, the computation of $\nabla^2$ is still expensive. Suppose we put a ball on a saddle point. Then, giving it only a slight push will move it away from the saddle. This intuition leads to the notion of &amp;ldquo;noisy&amp;rdquo; GD, i.e., $y = x - \eta \nabla f(x) + \epsilon$, where $\epsilon$ is a zero-mean error, which is often cheaper to compute than the true gradient. The authors in also prove the theorem in &lt;a href=&#34;http://proceedings.mlr.press/v40/Ge15.pdf&#34; target=&#34;_blank&#34;&gt;the paper&lt;/a&gt;, but it is very non-trivial.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;It is hard to converge to a saddle point.&lt;/em&gt; Furthermore, a random initialization of GD will asymptotically converge to a local minimum, rather than other stationary points. In (2), &lt;a href=&#34;http://people.eecs.berkeley.edu/~brecht/&#34; target=&#34;_blank&#34;&gt;Ben Recht&lt;/a&gt; emphasized that &amp;ldquo;even simple algorithms like gradient descent with constant step sizes can’t converge to saddle points unless you try really hard.&amp;rdquo; To prove this, they use the Stable Manifold Theorem, taking $x^{\ast}$ to be an arbitrary saddle point and showing that this measure was &lt;em&gt;always&lt;/em&gt; zero.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;The Stable Manifold theorem is concerned with fixed point operations of the form $x^{(k+1)}=\psi(x^{(k)})$. It quantifies that the set of points that locally converge to a fixed point $x^{\ast}$ of such an iteration have measure zero whenever the Jacobian of $\psi$ at $x^{\ast}$ has eigenvalues bigger than 1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In fact, it has been &lt;a href=&#34;https://www.math.upenn.edu/~pemantle/papers/nonconvergence.pdf&#34; target=&#34;_blank&#34;&gt;shown long back&lt;/a&gt; that additive Gaussian noise is sufficient to prevent convergence to saddles, without even assuming the &amp;ldquo;strictness&amp;rdquo; criteria of (1).&lt;/p&gt;

&lt;p&gt;Now that it is clear that GD can avoid saddle points almost certainly, it remains to be seen whether it is &lt;em&gt;efficient&lt;/em&gt; in doing so. The paper (1), although it did show a poly-time convergence for the noisy GD, was still inefficient because its polynomial dependency on the dimension $n$ and the smallest eigenvalue of the Hessian are impractical. The paper (3) further improves this aspect of the problem.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;A perturbed form of GD, under an additional Hessian-Lipschitz condition, converges to a second-order stationary point in almost the same time required for GD to converge to a first-order stationary point.&lt;/em&gt; Furthermore, the dimensional dependence is only polynomial in $\log(d)$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finally, recent work definitely shows that &lt;em&gt;PGD is much better than GD with random initialization&lt;/em&gt;, since the latter can be slowed down by saddle points, taking exponential time to escape. This is because if there are a sequence of closely-spaced saddle points, GD gets closer to the later ones, and takes $e^i$ iterations to escape the $i^{th}$ saddle point. PGD, on the other hand, escapes each saddle point in a small number of steps regardless of history.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Although most learning problems have exponentially many saddle points, they are hard to converge to, and even random initializations can escape them. They take a long time for this escape though, which is why using perturbations is more efficient, and actually as efficient as GD for first-order stationary points. Therefore, using information from Hessians is not necessary to escape saddle points efficiently.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;second-order-methods-for-local-optimum&#34;&gt;Second-order methods for local optimum&lt;/h3&gt;

&lt;p&gt;Although we have established that Hessians are unnecessary for finding the local optimum, it would still be enlightening to look at some approaches for the same.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.03943.pdf&#34; target=&#34;_blank&#34;&gt;Agarwal et. al &amp;lsquo;17&lt;/a&gt; proposed LiSSA, or Linear (time) Stochastic Second-order Algorithm. The basic update rule is&lt;/p&gt;

&lt;p&gt;$$ x_{t+1} = x_t - \eta [\nabla^2 f(x)]^{-1}\nabla f(x), $$&lt;/p&gt;

&lt;p&gt;i.e. the gradient is scaled by the inverse of the Hessian, which intuitively makes sense as discussed earlier. Although backpropagation can compute the Hessian itself in linear time, we require the inverse. In this paper, the LiSSA algorithm uses the idea that $(\nabla^2)^{-1} = \sum_{i=1}^{\infty}(I - \nabla^2)^i$, but with finite truncation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.00756.pdf&#34; target=&#34;_blank&#34;&gt;Carmon et al. &amp;lsquo;17&lt;/a&gt; further improved upon the $\mathcal{O}(\frac{1}{\epsilon^2})$ guarantee provided by gradient descent for $\epsilon$-first-order convergence, without any need for Hessian computation. They use two competing techniques for this purpose. The first has already been discussed above:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If the problem is locally non-convex, the Hessian must have a negative eigenvalue. In this case, under the assumption that the Hessian is Lipschitz continuous, moving in the direction of the corresponding eigenvector must make progress on the objective.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second technique is more novel. They show that if the Hessian&amp;rsquo;s smallest eigenvalue is at least $-\gamma$, we can apply &lt;a href=&#34;https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf&#34; target=&#34;_blank&#34;&gt;proximal point techniques&lt;/a&gt; and accelerated gradient descent to a carefully constructed regularized problem to obtain a faster running time.&lt;/p&gt;

&lt;p&gt;While their approach is asymptotically faster than first-order methods, it is still empirically slower. Furthermore, it doesn&amp;rsquo;t seem to find better quality neural networks in practice.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;understanding-the-landscape-matrix-completion&#34;&gt;Understanding the landscape: Matrix completion&lt;/h3&gt;

&lt;p&gt;Very early on in this post, we established that in deep learning problems, the landscape is unknown, i.e. the problem does not have a meaningful mathematical formulation. In this vein, we now look at a &lt;a href=&#34;https://arxiv.org/pdf/1704.00708.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; that develops a new framework to capture the landscape. In particular, we will approach this problem in the context of matrix completion. (Interestingly, this paper is again from &lt;a href=&#34;https://users.cs.duke.edu/~rongge/index.html&#34; target=&#34;_blank&#34;&gt;Rong Ge&lt;/a&gt;, who first showed polytime convergence to local minimum for noisy GD.)&lt;/p&gt;

&lt;p&gt;But first, what is matrix completion. Matrix completion is a learning problem wherein the objective is to recover a low-rank matrix from partially observed entries. The mathematical formulation of the problem is:&lt;/p&gt;

&lt;p&gt;$$ \min_{X} \text(rank)(X) \quad \text{subject to} \quad X_{ij} = M_{ij} ~~ \forall i,j \in E $$&lt;/p&gt;

&lt;p&gt;where $E$ is the set of observed entries. Most approaches to solve this problem represent it in the form of the following nonconvex objective.&lt;/p&gt;

&lt;p&gt;$$ f(X) = \frac{1}{2}\sum_{i,j\in E}[M_{i,j}-(XX^T)_{i,j}]^2 +R(X) $$&lt;/p&gt;

&lt;p&gt;Here, $R(X)$ is a regularization term which ensures that no single row of $X$ becomes too large, otherwise most observed entries will be 0.&lt;/p&gt;

&lt;p&gt;Ge showed in &lt;a href=&#34;https://arxiv.org/pdf/1605.07272.pdf&#34; target=&#34;_blank&#34;&gt;an earlier paper&lt;/a&gt; that in case of matrix completion (others have shown the same result for other problems like tensor decomposition and dictionary learning), all local minima are also global minima.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For matrix completion, all local minima are also global minima.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the present paper, the authors proposed the new insight that for the case of the matrix completion objective as defined above, the function $f$ is quadratic in $X$, which means that its Hessian w.r.t $X$ is constant. Furthermore, any saddle point has at least one strictly negative eigenvalue in its Hessian. Together, these ensure that simple local search algorithms can find the desired low rank matrix from an arbitrary starting point in polynomial time with high probability.&lt;/p&gt;

&lt;p&gt;These advances, while mathematically involved, show that characterizing the various stationary points of the learning objective can be helpful in providing theoretical guarantees for learning algorithms. While I have avoided proof details for the several important theorems here, I will try to understand and explain them lucidly in some later post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Approaches for NMT</title>
      <link>https://desh2608.github.io/post/unsupervised-approaches-for-nmt/</link>
      <pubDate>Thu, 14 Dec 2017 13:39:30 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/unsupervised-approaches-for-nmt/</guid>
      <description>

&lt;p&gt;Translation is one of those tasks in language where the arrival of deep learning systems, and in particular sequence-to-sequence, has been something like a boon. In less than 4 years since the first paper on Neural Machine Translation, software giants such as Google and Microsoft have already announced that their translation systems have almost completely shifted from statistical to neural. Gone are the days when researchers mulled over complex word and phrase alignment techniques, and yet fell short on several language combinations. With the latest framework, all you need are a million parallel sentences, and your system can then translate between this pair sufficiently well.&lt;/p&gt;

&lt;p&gt;A million parallel sentences — that’s a little constraining, though! It is often difficult and sometimes even impossible to obtain a bilingual parallel corpus for many pairs of languages. In such cases, using a pivot language for triangulation has been found to be helpful. However, even in such supervised systems, the performance is still constrained by the size of the training corpus.&lt;/p&gt;

&lt;p&gt;Monolingual data, on the other hand, is available in abundance, and a number of semi-supervised systems do use these, but mostly for the language modeling part of translation. For example, a naive system may perform word-by-word substitution and use a language model trained on the target language to obtain the most probable word order.&lt;/p&gt;

&lt;p&gt;Recently, there have been 2 very similar papers (both currently under review at ICLR ’18) which propose to perform completely unsupervised machine translation. In this article, I will discuss both of these papers. A similar blog is available &lt;a href=&#34;http://ankitg.me/blog/2017/11/05/unsupervised-machine-translation.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, but I didn’t know of its existence until I was already halfway through this post.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;unsupervised-neural-machine-translation&#34;&gt;Unsupervised Neural Machine Translation&lt;/h4&gt;

&lt;p&gt;This paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is from Prof. &lt;a href=&#34;http://www.kyunghyuncho.me/&#34; target=&#34;_blank&#34;&gt;Kyunghyu Cho&lt;/a&gt; (NYU), and the authors have used the traditional seq2seq model with a twist. The encoder is shared across all languages, but each language has its own decoder. The intuition is that a shared encoder will transform a sentence to a shared space representation, from where the language-specific decoder will be able to decode it to its own language.&lt;/p&gt;

&lt;p&gt;Both the encoder and decoder are 2 layer bidirectional RNNs with GRU units. Furthermore, the embeddings used in the feature layer are fixed, and are obtained from pre-trained cross-lingual dictionary. This ensures that the shared space representation obtained using the encoder is language-independent.&lt;/p&gt;

&lt;p&gt;The paper uses 2 interesting techniques for the unsupervised training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Denoising:&lt;/strong&gt; The autoencoder (or seq2seq) is used to reconstruct a sentence in a language, since we only have a monolingual corpus on which to train the system. Due to such a setting, an optimal system would essentially learn to copy the input to the output, and the system would reduce to a word-by-word substitution system. To prevent this, “denoising” is used, which introduces random noise in the input sentence so that copying cannot give the best output. This is dones by making $\frac{N}{2}$ random swaps for any sequence of $N$ tokens. There are 2 advantages to this technique:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Since copying is out of the picture, the system needs to learn the internal structure of language to perform well.&lt;/li&gt;
&lt;li&gt;By swapping words randomly, we also account for word order divergence across languages. For instance, &lt;em&gt;Los Angeles International Airport&lt;/em&gt; in English becomes &lt;em&gt;Aéroport international de Los Angeles&lt;/em&gt; in French.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Backtranslation:&lt;/strong&gt; Even with denoising added, the system is still monolingual. To integrate some element of cross-lingual training, the authors use the method of backtranslation. Given a sentence $x$ in language L1, the shared encoder is used to get the latent representation, and the decoder for the other language L2 is used to obtain a noisy translation $y$. This translation $y$ is then used to
predict the original sentence $x$ using the encoder and decoder for L1. This technique creates a pseudo-parallel corpus so that the system can learn cross-lingual translation.&lt;/p&gt;

&lt;p&gt;Denoising forces the system to capture broad word-level equivalences, while backtranslation helps it to learn more subtle relations between the language pairs. Furthermore, using pretrained cross-lingual embeddings ensures that the shared latent space representations for sentences in both the languages are near each other when the sentences have the same sense (or meaning).&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;unsupervised-machine-translation-using-monolingual-corpora-only&#34;&gt;Unsupervised Machine Translation using Monolingual Corpora Only&lt;/h4&gt;

&lt;p&gt;A very similar paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; from researchers at Facebook employs almost the same techniques, but differs slightly in the encoding mechanism. I personally enjoyed reading this paper more than the first one, although they haven’t gone into details of the components they use in their model. The explanation of the loss function for end-to-end training is very lucid, and the overall structuring itself is appealing to a novice researcher like myself.&lt;/p&gt;

&lt;p&gt;Anyway, the model used in this paper consists of a single encoder and a single decoder (bidirectional LSTM with attention in the decoder, similar to the NMT model used in Google Translate) which is shared by both the languages. For the unsupervised training, 3 techniques are employed.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Denoising&lt;/strong&gt;: Similar to the above paper, the autoencoder is denoised so that it does not learn a word-by-word substitution. The noise model in this case consists of: (i) dropping every word with some random probability, and (ii) shuffling the sentence by applying a random permutation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-domain training&lt;/strong&gt;: This is the same as the “backtranslation” technique used in the above paper. However, the authors have explicitly mentioned that to obtain the translation $x$ from the sentence $y$, the model of the previous iteration is used. This requires that the model be initialized with a naive translation strategy, which in this case, is simple word-by-word substitution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adversarial training&lt;/strong&gt;: In the above paper, due to the use of cross-lingual fixed embeddings in the shared encoder, the latent space representations were arguably similar for similar sentences in different languages. This method does not use cross-lingual embeddings, and hence, the representations will be similar only “as long as the two monolingual corpora exhibit strong structure in feature space.” (Full disclosure: This statement is written as a hand-waving argument without a justification, and one of the reviewers has even pointed this out.) In order to overcome this constraint, the authors employ a discriminator whose task is to predict the language of the encoded sentence. In turn, the encoder has an added term in its loss function which ensures that the representation of similar sentences in different languages are nearby in the latent space.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/9/mono.png&#34; alt=&#34;Training objectives for the system. Figure taken from the paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since the training is done iteratively and BLEU scores are computed at every step, we can simply select the hyperparameters corresponding to the best performing iteration. Empirically, the authors found that this selection has good correlation with test-time performance of the system. Furthermore, this unsupervised model was found to perform as good as a comparable supervised model trained on 100,000 parallel sentences, which is definitely an encouraging achievement for further research in unsupervised NMT.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Artetxe, Mikel, et al. “&lt;a href=&#34;https://arxiv.org/abs/1710.11041&#34; target=&#34;_blank&#34;&gt;Unsupervised Neural Machine Translation&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1710.11041&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Lample, Guillaume, Ludovic Denoyer, and Marc’Aurelio Ranzato. “&lt;a href=&#34;https://arxiv.org/abs/1711.00043&#34; target=&#34;_blank&#34;&gt;Unsupervised Machine Translation Using Monolingual Corpora Only&lt;/a&gt;.” &lt;em&gt;arXiv preprint arXiv:1711.00043&lt;/em&gt;(2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning for Multimodal Systems</title>
      <link>https://desh2608.github.io/post/deep-learning-multimodal-systems/</link>
      <pubDate>Thu, 09 Nov 2017 13:38:58 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-multimodal-systems/</guid>
      <description>

&lt;p&gt;When I was browsing through research groups for my grad school applications, I came across some interesting applications of new deep learning methods in a multimodal setting. ‘Multimodal,’ as the name suggests, refers to any system involving two or more modes of input or output. For example, an image captioning system provides images as input and expects a textual output. Similarly, speech-to-text, descriptive art, video summarization, etc., are all examples of multimodal objectives. In this article, I will discuss 3 recent papers from &lt;a href=&#34;http://www.cs.unc.edu/~mbansal/&#34; target=&#34;_blank&#34;&gt;Mohit Bansal&lt;/a&gt; (who joined UNC last year), based on album summarization, video
captioning, and image captioning (with a twist).&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;creating-a-story-from-an-album&#34;&gt;Creating a story from an album&lt;/h4&gt;

&lt;p&gt;Given an album containing several images (which may or may not be similar), the task of Visual Storytelling is to generate a natural language story describing the album. In this EMNLP ’17 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, the task is decomposed into 3 steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Album encoder&lt;/em&gt;: Encode the individual photos in the album to form photo vectors&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Photo selector&lt;/em&gt;: Select a small number of representative photos.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Story generator&lt;/em&gt;: Compose a coherent story from the selected photo vectors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/storytelling.png&#34; alt=&#34;Architecture of the Visual Storytelling system. Image taken from original paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For each of these three components, the paper uses a hierarchically-attentive RNN. The first component is similar to an embedding layer in a text classification setting, wherein a lookup table assigns some pretrained vectors to each word and then an RNN is applied to add sentence-level information to each word vector. In a similar fashion in this paper, the initial embeddings for each image are obtained using a pretrained ResNet101 layer, and then a bidirectional RNN with GRU cells is used to add information pertaining to the entire album in every image embedding.&lt;/p&gt;

&lt;p&gt;In the Photo Selector stage, the selection is treated as a latent variable since we only have end-to-end ground truth labels. As such, we use soft attention to output $t$ probability distributions over all the images in the album, where $t$ is the number of summary images required, i.e., each image has $t$ probabilities associated with it. For this purpose, a GRU takes the previous $p$ and the previous hidden state &lt;em&gt;h&lt;/em&gt; as input and outputs the next hidden state. We use a multilayer perceptron with sigmoid activation to fuse the hidden state with the photo vector and obtain the soft attention for the particular image.&lt;/p&gt;

&lt;p&gt;$$ h_t = GRU_{select}(p_{t-1},h_{t-1}) \\\ p(y_{a_i}(t)=1) = \sigma(MLP([h_t,v_i])) $$&lt;/p&gt;

&lt;p&gt;Finally, we can obtain $t$ weighted album representations by taking the weighted sum of the photo vectors with the corresponding probability distributions. Each of these vectors is then used to decode a single sentence. For this purpose, a GRU takes the joint input of the album vector at step $t$, the previous word embedding, and the previous hidden state, and outputs the next hidden state. We repeat this for $t$ steps, thus obtaining the required album summary.&lt;/p&gt;

&lt;p&gt;How do we define loss in such a setting? First, since we already know the correct summary sentences, we can define a &lt;em&gt;generation loss&lt;/em&gt; which is simply the sum of negative log likelihoods of the correct words. However, in addition to the words being similar, the story should be temporally coherent, i.e., the sentences themselves should be in a specific order. For this purpose, we apply a max-margin ranking loss as:&lt;/p&gt;

&lt;p&gt;$$ h_t = GRU_{select}(p_{t-1},h_{t-1}) \\\ p(y_{a_i}(t)=1) = \sigma(MLP([h_t,v_i])) $$&lt;/p&gt;

&lt;p&gt;The total loss is just a linear combination of these two losses. This provides a framework for end-to-end training for the system.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;captioning-videos-using-multi-task-learning&#34;&gt;Captioning videos using multi-task learning&lt;/h4&gt;

&lt;p&gt;It seems multitask learning was under the spotlight in ACL ’17. Two semantic parsing papers I discussed in &lt;a href=&#34;https://desh2608.github.io/post/trends-in-semantic-parsing-2/&#34; target=&#34;_blank&#34;&gt;yesterday’s blog&lt;/a&gt; were both based on this paradigm, and so is this one.&lt;/p&gt;

&lt;p&gt;At this point, I would like to clarify the difference between transfer learning and multitask learning by quoting directly from &lt;a href=&#34;https://www.researchgate.net/post/What_is_the_difference_between_Multi-task_Learning_and_Transfer_Learning&#34; target=&#34;_blank&#34;&gt;this answer&lt;/a&gt; on ResearchGate:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Multi-task learning can be seen as one type of transfer learning, where the information to transfer is some inner representation/substructure of the models under consideration, or the relevant features for a prediction, and where all
the target tasks use the same data samples, but predict different target
features for these (e.g. Part Of Speech tagging and Named Entity Recognition for
natural language processing tasks).&lt;/p&gt;

&lt;p&gt;Transfer Learning, on the other hand, would be the very general problem setting, where the “what” to transfer (representation, model substructures, data samples, parameter priors, …), the concurrency of learning (one or multiple target tasks using one or multiple source tasks, or learning several tasks jointly), the differences in domain (same data or different samples, samples from same or different/related distribution, same or partially different input features) and prediction problem (same target feature or different target features/tasks, same conditional or different/related conditional) are characteristics identifying the subclass of transfer learning problem, and maybe the approach taken to address this problem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A more formal definition can be found &lt;a href=&#34;https://stats.stackexchange.com/questions/255025/difference-between-multitask-learning-and-transfer-learning&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Essentially in multitask learning, all the tasks are learnt simultaneously, whereas in transfer learning, the knowledge from one task is used in another. Now that the terminology is clear, let us look at the tasks and the model used.&lt;/p&gt;

&lt;p&gt;The objective in this paper is video captioning, and the co-learnt tasks are video prediction and language entailment generation. It is arguably difficult to obtain large amounts of annotated data for a video prediction task, and hence learning from other tasks is especially relevant in this context.&lt;/p&gt;

&lt;p&gt;Video prediction refers to the task of predicting the next frame in a video given a sequence of frames. Recognizing textual entailment (RTE), means identifying the logical relationship between two sentences, i.e., whether a premise and hypothesis follow entailment, contradiction, or independence. Knowledge transfer from a video prediction setting helps the model learn the temporal flow of information in a video, while learning from an RTE setting helps it in logically infering a caption from the video. This is the rationale behind using these tasks for the multi-task learning framework.&lt;/p&gt;

&lt;p&gt;The overall architecture of the system is given below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/captioning.png&#34; alt=&#34;Architecture of video captioning system. Image taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For each subsystem, the paper uses a simple attention-based bidirectional LSTM for the encoding and decoding purposes. This is a fine example of how a simple sequence-to-sequence block can be leveraged in different settings to perform interesting tasks.&lt;/p&gt;

&lt;h4 id=&#34;puns-in-image-captions&#34;&gt;Puns in image captions&lt;/h4&gt;

&lt;p&gt;Humor is difficult to capture or create in general. Heterographic homophones (words with different spelling but similar sound) are often used by cartoonists to add subtext to illustrations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/puns.jpg&#34; alt=&#34;Heterographic homophone used for humor in a comic. Taken from http://cartoonsbyjim.com&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, the authors have proposed 2 different methods to generate “punny” captions for images, namely a Generation model, and a Retrieval model.&lt;/p&gt;

&lt;p&gt;The Generation model works as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first step is &lt;em&gt;tagging&lt;/em&gt;. We identify the top 5 objects in the given image using an Inception-ResNet-v2 model trained on ImageNet. We also get the words from a simple caption generated for the image using a Show-and-Tell architecture. The objects and the words together are considered as tags for pun generation.&lt;/li&gt;
&lt;li&gt;We then generate a vocabulary of puns by mining the web and selecting all pairs of words with an edit distance of 0 based on articulatory features.&lt;/li&gt;
&lt;li&gt;From this pun vocabulary, we filter those puns where at least one of the homophones is related to the image in question.&lt;/li&gt;
&lt;li&gt;During the caption generation, at specific time steps, the model is forced to produce a phonological counterpart of a pun word associated with the image. The decoder generates next words based on all previously generated words.&lt;/li&gt;
&lt;li&gt;To solve the issue of non-grammatical sentences due to puns later in the sentence, two models are trained to decode the image in both forward and reverse directions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/7/generation.png&#34; alt=&#34;Architecture of the Generation model. Image taken from paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Retrieval model, on the other hand, tries to find relevant captions from a prebuilt corpus of captions. This is an entirely deterministic model which requires two conditions to be satisfied:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The caption must contain the counterpart of the pun word present in the image so that incongruity is attained.&lt;/li&gt;
&lt;li&gt;The caption must be contextually relevant to the image, i.e., it must contain at least one of the &amp;ldquo;tagged&amp;rdquo; words that we found earlier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, the captions obtained from both models are pooled together and ranked by taking their log-probability score with respect to the original caption generated from the simple image captioning model. Non-maximal suppression is applied to remove captions which are similar to a higher-ranked caption, and the top 3 such obtained are retained.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;From these examples of multimodal systems, we see that simple sequence-to-sequence models work satsifactorily if used in conjuction with intelligent frameworks such as multitask learning or transfer learning, as is the trend in recent days. A cool thing is that reading about the various transfer learning approaches for this and the previous post has helped me come up with a new solution for a project that I have been working on. More on that later!&lt;/p&gt;

&lt;p&gt;Conference on Empirical Methods in Natural Language Processing*. 2017.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Yu, Licheng, Mohit Bansal, and Tamara Berg. “&lt;a href=&#34;https://arxiv.org/pdf/1708.02977.pdf&#34; target=&#34;_blank&#34;&gt;Hierarchically-Attentive RNN for Album Summarization and Storytelling&lt;/a&gt;.” *Proceedings of the 2017
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Chandrasekaran, Arjun, Devi Parikh, and Mohit Bansal. “&lt;a href=&#34;https://arxiv.org/pdf/1704.08224.pdf&#34; target=&#34;_blank&#34;&gt;Punny Captions: Witty Wordplay in Image Descriptions&lt;/a&gt;.”  &lt;em&gt;arXiv preprint arXiv:1704.08224&lt;/em&gt; (2017).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Best Papers at ICLR 2017</title>
      <link>https://desh2608.github.io/post/best-papers-at-iclr-17/</link>
      <pubDate>Sun, 15 Oct 2017 13:38:17 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/best-papers-at-iclr-17/</guid>
      <description>

&lt;p&gt;The International Conference on Learning Representations (ICLR) has evolved into &lt;em&gt;the&lt;/em&gt; deep learning conference over the last few years, and with its open review system, it is not difficult to understand why. I was recently going through some of the papers accepted at this year’s ICLR, especially the 3 that were awarded the Best Paper award. In this article, I will try to summarize these 3 papers in simple words, and hopefully get an idea about what’s hot in deep learning.&lt;/p&gt;

&lt;p&gt;The 3 best papers are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.03530.pdf&#34; target=&#34;_blank&#34;&gt;Understanding deep learning requires rethinking generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1704.06611.pdf&#34; target=&#34;_blank&#34;&gt;Making neural programming architectures generalize via recursion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1610.05755.pdf&#34; target=&#34;_blank&#34;&gt;Semi-supervised knowledge knowledge transfer for deep learning from private training data&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Statisticians always like saying that deep learning is a black box and eveything that happens is a result of hyperparameter tuning. You cannot say why you have obtained a good result, let alone providing a guarantee for a result. Well, not anymore. The 3 best papers were all about providing proveable guarantees, and it looks like the deep learning community is all set to move past its “black box” days.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;understanding-deep-learning-requires-rethinking-generalization&#34;&gt;Understanding deep learning requires rethinking generalization&lt;/h4&gt;

&lt;p&gt;This paper from Google Brain is very readable and discusses some very common occurences. The authors make 2 very straightforward observations: (i) Deep neural networks easily fit random labels, and (ii) Explicit regularization is neither necessary nor sufficient for controlling generalization error.&lt;/p&gt;

&lt;p&gt;Essentially, they evaluate well-known deep architectures on several popular datasets, with some randomness added, such as random labels, or Gaussian noise-added labels, or shifted input features, in an attempt to show that even though training error is still close to negligible, generalization error increases alarmingly even in the presence of explicit regularizers such as weight decay, dropout, and data augmentation, and for implicit regularizers such as early stopping. This serves as a wake-up call for people who study the performance of neural networks.&lt;/p&gt;

&lt;p&gt;The most interesting (and PROVABLE) guarantee that the paper contains is the following theorem: &lt;em&gt;There exists a two-layer neural network with ReLU activations and 2n+d weights that can represent any function on a sample of size n in d dimensions.&lt;/em&gt; While I will not go into the detailed proof here, it is essentially based on solving the system of linear equations based on the ReLU activation function. For the system to have a solution, the coefficient matrix should be full-ranked, which the authors show is indeed the case. If the proof in the paper is too formal (read: succinct) for you, you can find a more detailed one &lt;a href=&#34;https://danieltakeshi.github.io/2017/05/19/understanding-deep-learning-requires-rethinking-generalization-my-thoughts-and-notes&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. In addition, they also show ways in which we can reduce the width of the network
at each layer by increasing its depth.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;making-neural-programming-architectures-generalize-via-recursion&#34;&gt;Making neural programming architectures generalize via recursion&lt;/h4&gt;

&lt;p&gt;Understanding this paper from researchers at UC Berkeley requires a little background of neural programmer-interpreter (NPI) architectures, which can be found in the paper as well as in the &lt;a href=&#34;https://arxiv.org/pdf/1511.06279.pdf&#34; target=&#34;_blank&#34;&gt;ICLR ’16 paper&lt;/a&gt; in which they were introduced. Basically, an NPI framework consists of a controller (such as an LSTM), which takes as input the environment state and arguments, and returns the next program pointer to be executed. In this way, given a set of sequences that an algorithm must follow to get to the output, the NPI can learn the algorithm itself. In the original paper, the authors learned to perform tasks such as adding, sorting,
etc. using NPIs.&lt;/p&gt;

&lt;p&gt;In this paper, the concept of &lt;em&gt;recursion&lt;/em&gt; is added to the existing NPI framework, and this makes it capable of performing much more complex tasks such as quick sort. Formally, a function exhibits recursive behavior when it possesses two properties: (1) Base cases — terminating scenarios that do not use recursion to produce answers; (2) A set of rules that reduces all other problems toward the base cases. In the paper, the author describe how they construct NPI training traces so as to make them contain recursive elements and thus enable NPI to learn recursive programs.&lt;/p&gt;

&lt;p&gt;Furthermore, the authors show &lt;em&gt;provably perfect generalization&lt;/em&gt; for their new architecture. The theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn.&lt;/p&gt;

&lt;p&gt;To prove this, we again go to the notions of base case and recursive case. For example, in the addition task, the base case is always a set of small, fixed size step input sequences during which the LSTM state remains constant. So the base case is trivially true. The key in proving the recursive step is to construct the verification set well, so that the step inputs are neither too large so as to be outside the scope of evaluation, nor too small so that the semantics of the problem are not well defined. The actual verification process is simple and can be read in the paper.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data&#34;&gt;Semi-supervised knowledge transfer for deep learning from private training data&lt;/h4&gt;

&lt;p&gt;Another paper from Google Brain, this deals with the important subject of building a model which learns from sensitive data while also keeping it private. A new model called PATE (Private Aggregation of Teacher Ensembles) is introduced which basically trains in a 2-step strategy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;An ensemble of teacher models is trained on disjoint subsets of the sensitive dataset.&lt;/li&gt;
&lt;li&gt;A student model is trained on the aggregate output of the ensemble.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The aggregation is “private” because the number of times the student can access the teacher model is limited, and the top vote of the ensemble is revealed only after adding random noise. Due to these restrictions, no amount of querying can get hold of the private training data used to train the teacher models. Furthermore, in the Laplacian noise added to the teacher aggregation, the noise
parameter can be used to tune the privacy-accuracy tradeoff for the student model. For example, if the noise is large, privacy is high at the cost of reduced accuracy, and vice versa.&lt;/p&gt;

&lt;p&gt;For the transfer of knowledge from the ensemble to the student model, the authors experimented with various techniques and finally used semi-supervised learning with GANs. The student is trained on nonsensitive data (which may be labeled or unlabeled). The discriminator is a multi-class classifier which is trained such that it classifies the labeled data into the correct class, the unlabeled (true) data into any of the &lt;em&gt;k&lt;/em&gt; classes, and the generated data (from the generator) into an extra class.&lt;/p&gt;

&lt;p&gt;Again, the authors use the notion of “differentiable privacy guarantee” to come up with a lower bound for the privacy guarantee for their model. (The derivation is a little involved and I skipped it since I don’t have the prerequisites of security and privacy.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To sum up, all the papers seem to provide some generalization guarantee rather than just proposing a “good” model. Looks like sunny days for deep learning!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Last 3 Years in Text Classification</title>
      <link>https://desh2608.github.io/post/last-3-years-in-text-classification/</link>
      <pubDate>Mon, 02 Oct 2017 12:49:14 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/last-3-years-in-text-classification/</guid>
      <description>

&lt;p&gt;While working on my &lt;a href=&#34;https://desh2608.github.io/project/btp/&#34; target=&#34;_blank&#34;&gt;undergrad thesis&lt;/a&gt; on relation classification of biomedical text using deep learning methods, I quickly hacked together models in Tensorflow that combined convolutional and recurrent layers in various combinations. While some of these “network architectures” worked superbly (even surpassing state-of-the-art results), I had no clue what was happening inside the model. To gain such an intuition, I read about 20 recent papers on text classification (starting with the first “CNN for sentence classification” paper by Yoon Kim) over the course of a week. Aside from an obvious enlightenment about why my architecture was working the way it was, I also gained valuable insight into how results are presented by experts like Yann LeCunn and Tommi Jaakkola (which would later help me in getting my CoNLL paper accepted as an undergrad).&lt;/p&gt;

&lt;p&gt;Anyway, so while reading these myriad of text classification papers, I subconsciously began organizing them under different heads, depending upon the kind of approach used. The common objective across each of these approaches was that they all wanted to model the structural information of the sentence into the sentence embedding. All of this was in March, and ever since, I have wanted to organize the notes I made from my readings into a formal article, so that others may benefit from the insights.&lt;/p&gt;

&lt;p&gt;Some background in CNNs and LSTMs is assumed.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;character-to-sentence-level-embeddings&#34;&gt;Character to sentence level embeddings&lt;/h4&gt;

&lt;p&gt;Using word vectors (conventionally obtained using Word2Vec or GloVe) has been the most popular technique for input feature embedding. Yann LeCunn proposed character-level embeddings in his NIPS 2015 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, and the motivation behind this was that language could also be thought of as a signal similar to speech, with each character representing one bit of information. As such, it was reasonable to encode characters rather than words to obtain sentence level structure more efficiently. Although the proposed method was outperformed even by traditional tf-idf approaches for smaller datasets, the most important hypothesis obtained from empirical analyses was that character level CNNs tend to work well with uncurated user-generated data, such as reviews on Amazon. This makes them especially suitable for use in data wherever misspellings or use of exotic characters is frequent, such as in tweets.&lt;/p&gt;

&lt;p&gt;Even before LeCunn’s work, a COLING 2014 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; from IBM Research (Brazil) combined embeddings from the character, word, and sentence levels to obtain an amalgamation for the sentence representation. This is done in two convolutional layers as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first layer, vectors are obtained for words using traditional lookup techniques like Word2Vec. At the same time, character-level input vectors corresponding to each word are fed into a convolutional layer and a subsequent max pooling layer, and padding is used so that fixed length outputs are obtained for every word. These convolved features are concatenated with the word-level embeddings to obtain the joint word vector. The rationale behind this is that while word-level embeddings are meant to capture syntactic and semantic information, character-level embeddings capture morphological and shape information.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/4/char.png&#34; alt=&#34;Obtaining character-level embeddings. Image taken from Fig. 1 [^2].&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The second layer to obtain sentence-level vectors is similar to the character level. On applying  convolutions and max pooling, we obtain a global feature representation for the sentence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Embeddings have become a staple in deep learning models for NLP, and the latest trend is to use deep transfer learning to learn entire parameters for word vectors. While the community has mostly stabilized on using word-level vectors for input features, it wasn’t for lack for exploration, as is evident from these early approaches.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;encoding-structural-information-parse-trees-and-tensor-algebra&#34;&gt;Encoding structural information: parse trees and tensor algebra&lt;/h4&gt;

&lt;p&gt;Again, for accurate text classification, it is imperative to obtain a good sentence representation that effectively captures the structural information and any semantics possible. If we think about Yoon Kim’s original CNN model in this vein, the limitations in the simple “conv+pool” model becomes obvious. While the convolutional layer helps to recognize short phrases, the final max pooling layer completely disregards any word order or structural information in the sentence. Essentially, we can reorder phrases in the sentences, and the representation would still remain the same.&lt;/p&gt;

&lt;p&gt;To solve these problems, a myriad of techniques have been proposed. Here I will discuss 2 of them — the first involves using syntactic parse trees, and the second turns to good old tensor algebra.&lt;/p&gt;

&lt;p&gt;A 2015 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; from Peking University proposed two tree-based CNN models, namely c-TBCNN and d-TBCNN, depending on whether constituency or dependency parse trees were used. I will first outline the model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A sentence is first converted to a parse tree, and each node is represented as a distributed, real-valued vector. While the nodes of dependency trees are words themselves, those in constituency trees are not. To solve this problem, constituency tree nodes are pretrained using Socher’s RNN and kept fixed thereafter.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/4/tree-cnn.png&#34; alt=&#34;Tree-based convolutional window. Image taken from Fig. 2 [^3].&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;tree-based convolutional window&lt;/em&gt; is defined, which slides over the entire tree to extract structural information of the sentence. The convolutional equation for a window which slides over a parent and its direct children in a constituency tree is given by&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ y = f(w_p^{( c )}\cdot p + w_l^{( c )}\cdot c_l + w_r^{( c )}\cdot c_r + b^{( c )}). $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In a dependency tree, a node can have any number of children. To overcome this, weights in these trees are assigned according to dependency type rather than position, and so the convolution formula becomes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ y = f(W_p^{(d)}\cdot p + \sum_{i=1}^n W_{r[c_i]}^{(d)}\cdot c_i + b^{(d)}). $$&lt;/p&gt;

&lt;p&gt;In empirical evaluation, d-TBCNN was found to outperform c-TBCNN probably due to d-TBCNN being able to exploit structural features more efficiently because of the compact expressiveness of dependency trees. The paper also provides visualizations for understanding the mechanism of the proposed network, and they show that TBCNNs do integrate information about different words in a window.&lt;/p&gt;

&lt;p&gt;A 2015 paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; from Regina Barzilay and Tommi Jaakkola at MIT used non-linear, non-consecutive convolutions, and turned to tensor algebra to reduce computational complexity. The motivation behind this model is two-fold:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Conventional CNNs use linear operations on stacked word vectors, which ignores the interesting non-linear interaction between n-grams.&lt;/li&gt;
&lt;li&gt;Consecutive convolutions misses out on the non-consecutive phrases e.g. &amp;ldquo;&lt;em&gt;not&lt;/em&gt; nearly as &lt;em&gt;good&lt;/em&gt;&amp;rdquo; etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Essentially, they modified the 2 main components of a CNN-based text classification module, namely window-based convolutions, and the linear convolution operation, with 3 novel modifications.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stacked n-gram word vectors are replaced by tensor products, and this n-gram tensor can be seen as a generalization of the typical concatenated vector.&lt;/li&gt;
&lt;li&gt;Since the convolutional filters themselves are high-dimensional tensors (n dimensions corresponding to the size of tensor window, and 1 channel dimension), directly maintaining them as full tensors would lead to parametric explosion. To overcome this, the convolutional tensor is represented using &lt;em&gt;low-rank factorization.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Instead of applying convolutions only to consecutive n-grams, all possible n-grams are used. At each position, the aggregate representation is the weighted sum of all n-gram representations ending at that position.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper makes use of linear algebra very cleverly to extend simple convolution operations across the whole sentence without making it computationally infeasible. In the results section, the authors have also analyzed the importance of such non-linear and non-consecutive activations empirically.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;regional-two-view-embeddings&#34;&gt;Regional (two-view) embeddings&lt;/h4&gt;

&lt;p&gt;In a series of papers (published at NIPS 2015&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; and ICML 2016&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;), Rie Johnson and Tong Zhang introduced the concept of regional embedding in sentences, which was based on two-view embeddings. Essentially, they wanted to answer the question: &lt;em&gt;Can an unlabeled data be used to augment a CNN/LSTM module in a better way than by simply obtaining pretrained word vectors?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In some way, these embeddings are also related to the first section on character to sentence level  embeddings. However, I have put it in a separate section since my own network architecture in my CoNLL paper derived hugely from the interpretation given in these papers. (You can say this was when I gained enlightenment!)&lt;/p&gt;

&lt;p&gt;In an earlier paper, the authors had showed that using high-dimensional one-hot bag-of-words (BOW) vectors rather than pretrained word vectors proved to be better in simpler systems. Their new objective was to learn regional embeddings from unlabeled data and use it as additional input to the supervised CNN.&lt;/p&gt;

&lt;p&gt;But first, &lt;em&gt;what is a tv-embedding&lt;/em&gt;? Essentially, it is a function of a view that preserves everything required to predict another view. (See the paper section 2 for details. The motivation for using tv-embeddings is also explained theoretically in the Appendix &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.)&lt;/p&gt;

&lt;p&gt;In the papers, the authors used a CNN and an LSTM, respectively, to obtain these tv-embeddings for short regions in the sentences using an unlabeled corpus. They called these as “regional embeddings,” and used them as additional input for the supervised classification task. Furthermore, in their ICML paper, they did away with CNNs entirely, and argued that using bidirectional LSTMs for obtaining the regional embedding and then pooling for the sentence vector gives and adequate sentence representation. However, experimental results showed that using tv-embeddings from networks resulted in the best performing model.&lt;/p&gt;

&lt;p&gt;This “regional embedding+pooling” logic was what finally provided the necessary intuition for my own relation classification network.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Zhang, Xiang, Junbo Zhao, and Yann LeCun. “&lt;a href=&#34;http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf&#34; target=&#34;_blank&#34;&gt;Character-level convolutional networks for text classification.&lt;/a&gt;” &lt;em&gt;Advances in neural information processing systems&lt;/em&gt;. 2015.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Dos Santos, Cícero Nogueira, and Maira Gatti. “&lt;a href=&#34;http://anthology.aclweb.org/C/C14/C14-1008.pdf&#34; target=&#34;_blank&#34;&gt;Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts.&lt;/a&gt;” &lt;em&gt;COLING&lt;/em&gt;. 2014.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Mou, Lili, et al. “&lt;a href=&#34;https://arxiv.org/pdf/1504.01106.pdf&#34; target=&#34;_blank&#34;&gt;Discriminative neural sentence modeling by tree-based convolution.&lt;/a&gt;” &lt;em&gt;arXiv preprint arXiv:1504.01106&lt;/em&gt; (2015).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Lei, Tao, Regina Barzilay, and Tommi Jaakkola. “&lt;a href=&#34;https://arxiv.org/pdf/1508.04112.pdf&#34; target=&#34;_blank&#34;&gt;Molding CNNs for text: non-linear, non-consecutive convolutions.&lt;/a&gt;” &lt;em&gt;arXiv preprint arXiv:1508.04112&lt;/em&gt; (2015).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;Johnson, Rie, and Tong Zhang. “&lt;a href=&#34;http://papers.nips.cc/paper/5849-semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding.pdf&#34; target=&#34;_blank&#34;&gt;Semi-supervised convolutional neural networks for text categorization via region embedding.&lt;/a&gt;” &lt;em&gt;Advances in neural information processing systems&lt;/em&gt;. 2015.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Johnson, Rie, and Tong Zhang. “&lt;a href=&#34;http://proceedings.mlr.press/v48/johnson16.pdf&#34; target=&#34;_blank&#34;&gt;Supervised and semi-supervised text categorization using LSTM for region embeddings.&lt;/a&gt;” &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. 2016.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Word Vectors</title>
      <link>https://desh2608.github.io/post/understanding-word-vectors/</link>
      <pubDate>Fri, 29 Sep 2017 11:12:55 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/understanding-word-vectors/</guid>
      <description>

&lt;p&gt;&lt;em&gt;This article is a formal representation of my understanding of vector semantics, from course notes and reading reference papers and chapters from Jurafsky’s SLP book. I will be talking about sparse and dense vector semantics, including SVD, skip-gram, and GloVe. In many places, I will try to explain the ideas in language rather than equations (but I’ll provide links to derivations and stuff wherever it is absolutely essential, which is actually everywhere!).&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
&lt;p&gt;“You shall know a word by the company it keeps.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In vision, images are represented by the corresponding RGB values (or values obtained from other filters), so they are essentially matrices of integers. Language was more arbitrary because traditionally there was no formal method (or globally accepted standard) for representing words with numerical values. Well, not until &lt;strong&gt;word embeddings&lt;/strong&gt; came into the picture (no pun intended)!&lt;/p&gt;

&lt;p&gt;What are embeddings, though? They are called so because words are essentially transformed into vectors by “embedding” them into a vector space. For this, we make use of the hypothesis that words which occur in similar context tend to have similar meaning, i.e., the meaning of a word can be inferred from the distribution around it. For this reason, these methods are also called “distributional” methods.&lt;/p&gt;

&lt;p&gt;Word vectors may be sparse or dense. I’ll begin with sparse vectors and then describe dense ones.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;sparse-vectors&#34;&gt;Sparse vectors&lt;/h3&gt;

&lt;h4 id=&#34;term-document-and-term-term-matrix&#34;&gt;Term-document and term-term matrix&lt;/h4&gt;

&lt;p&gt;Suppose we have a set of 1000 documents, consisting of a total of 5000 unique words. In a very naive fashion, we can simply count the number of occurrences of each word in every document, and then represent each word by this 1000-dimensional vector of counts. This is exactly what a &lt;strong&gt;term-document matrix&lt;/strong&gt; does.&lt;/p&gt;

&lt;p&gt;Similarly, consider a large corpus of text with 5000 unique words. Now take a window of some fixed size and for each word pair, we count the number of times it occurs in the window. These counts form a &lt;strong&gt;term-term matrix&lt;/strong&gt;, also called a &lt;strong&gt;co-occurrence matrix&lt;/strong&gt; which in this case will be a 5000x5000 matrix (with most cells 0 if the window size is relatively small).&lt;/p&gt;

&lt;h4 id=&#34;pointwise-mutual-information-pmi&#34;&gt;Pointwise Mutual Information (PMI)&lt;/h4&gt;

&lt;p&gt;The co-occurrence matrix is not the best measure of similarity between 2 words since it is based on the raw frequency, and hence is very skewed. Instead, it would be desirable to have a quantity which measures how much more likely is it for 2 words to occur in a window, compared with pure chance. This is exactly what PMI measures.&lt;/p&gt;

&lt;p&gt;$$ \text{PMI}(x,y) = \log \left( \frac{P(x,y)}{P(x)P(y)} \right) $$&lt;/p&gt;

&lt;p&gt;If PMI is positive, the ($x$,$y$) pair is more likely to occur together than pure chance, and vice versa. However, a negative value is unreliable since it is unlikely to get many co-occurrences of a word pair in a small corpus. To solve this problem, we define a Positive PMI (PPMI) as&lt;/p&gt;

&lt;p&gt;$$ \text{PPMI}(x,y) = \max (\text{PMI}(x,y),0). $$&lt;/p&gt;

&lt;h4 id=&#34;tf-idf-term-frequency-inverse-document-frequency&#34;&gt;TF-IDF (Term frequency — inverse document frequency)&lt;/h4&gt;

&lt;p&gt;This is composed of 2 parts: TF, which denotes the count of the word in a document, and IDF, which is a weight component that gives higher weight to words occurring only in a few documents (and hence are more representative of the documents they are present in, in contrast to words like ‘the’ which are present in large number of documents).&lt;/p&gt;

&lt;p&gt;$$ idf_i = \log \left( \frac{N}{df_i} \right) $$&lt;/p&gt;

&lt;p&gt;Here, $N$ is the total number of documents and $df_i$ is the number of documents in which word $i$ occurs.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;dense-vectors&#34;&gt;Dense vectors&lt;/h3&gt;

&lt;p&gt;The problem with sparse vectors is the curse of dimensionality, which makes computation and storage infeasible. For this reason, we prefer dense vectors, with real-valued elements. Dense vector semantics fall into 2 categories: matrix factorization, and neural embeddings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;matrix-factorization&#34;&gt;Matrix Factorization&lt;/h3&gt;

&lt;h4 id=&#34;singular-vector-decomposition-svd&#34;&gt;Singular vector decomposition (SVD)&lt;/h4&gt;

&lt;p&gt;This is basically a dimensionality reduction technique where we find the dimensions with the highest variances. Suppose we have the co-occurence matrix A of size $m \times n$, then it is possible to factorize A into:&lt;/p&gt;

&lt;p&gt;$$ A_{m \times n} = U_{m\times r}S_{r\times r}V_{r\times n}^T $$&lt;/p&gt;

&lt;p&gt;where $r$ is the rank of matrix $A$ (i.e. $r$ = maximum number of linearly independent vectors that can be used to form $A$). Also, $U$ is a matrix of the eigenvectors of $AA^T)$ and $S$ is a diagonal matrix comprising its eigenvalues. If we rearrange the columns in $U$ to correspond with a decreasing order of eigenvalues, we can keep the first $k$ columns which will represent the dimensions in the latent space which have the highest variance. These will give us a $k$-dimensional representation for each of the $m$ words in the vocabulary.&lt;/p&gt;

&lt;p&gt;But why do we want to perform this truncation?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First, removing the lower variance dimensions filters the noise component from the word embeddings.&lt;/li&gt;
&lt;li&gt;More importantly, having a lower number of parameters leads to better generalization. It is found that 300-dimensional word embeddings perform much better than, say, 3000-dimensional ones.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, this approach is still constrained since the matrix factorization of $A$, which in itself may be a large matrix, is computationally complex.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;neural-embeddings&#34;&gt;Neural embeddings&lt;/h3&gt;

&lt;p&gt;The idea is simple. We can treat each element in the vector as a parameter to be updated while training a neural network model. We start with a randomly initialized vector and update it at each iteration. This update is based on the vectors of the context (window) words. The hypothesis is that such an update would ultimately result in similar words having vectors which are closer to each other in the vector space.&lt;/p&gt;

&lt;p&gt;Here, I will describe the 2 most popular neural models — Word2Vec and GloVe.&lt;/p&gt;

&lt;h4 id=&#34;word2vec&#34;&gt;Word2Vec&lt;/h4&gt;

&lt;p&gt;Word2Vec is actually the name of a tool which internally uses skip-gram or CBOW (continuous bag-of-words) with negative sampling. The objectives for both these models are quite similar, except a subtle distinction. In skip-gram, we predict the context words given the target word, and in CBOW, we predict the target word given the context words. In this article, I will limit my discussion to &lt;em&gt;skip-gram with negative sampling&lt;/em&gt; (SGNS).&lt;/p&gt;

&lt;p&gt;Suppose we have a context window where $w$ is the target word and $c$ is one of the context words. Then, skip-gram’s objective is to compute $P(c|w)$, which is given as&lt;/p&gt;

&lt;p&gt;$$ p(c|w;\theta) = \frac{\exp(v_c\cdot v_w)}{\sum_{c^{\prime}\in C}\exp(v_{c^{\prime}}\cdot v_w)}. $$&lt;/p&gt;

&lt;p&gt;Basically, it is just a softmax probability distribution over all the word-context pairs in the corpus, directed by the cosine similarity. However, the denominator term here is very expensive to compute since there may be a very large number of context words. To solve this problem, negative sampling is used.&lt;/p&gt;

&lt;p&gt;Goldberg and Levy have explained the derivation for the objective function in SGNS very clearly in their &lt;a href=&#34;https://arxiv.org/pdf/1402.3722.pdf&#34; target=&#34;_blank&#34;&gt;note&lt;/a&gt;. I will try to provide a little intuition here.&lt;/p&gt;

&lt;p&gt;For the word $w$, we are trying to predict the context word $c$. Since we are using softmax, this is essentially like a multi-class classification problem, where we are trying to classify the next word into one of $N$ classes (where $N$ is the number of words in the dictionary). Since $N$ may be quite large, this is a very difficult problem.&lt;/p&gt;

&lt;p&gt;What SGNS does is that it converts this multi-classification problem into binary classification. The new objective is to predict, for any given word-context pair ($w$,$c$), whether the pair is in the window or not. For this, we try to increase the probability of a “positive” pair ($w$,$c$), while at the same time reducing the probability of $k$ randomly chosen “negative samples” ($w$,$s$) where $s$ is a word not found in $w$’s context. This leads to the following objective function which we try to maximize in SGNS:&lt;/p&gt;

&lt;p&gt;$$ J = \log \sigma(c\cdot w) + \sum_{i=1}^k \mathbb{E}_{w_i \sim p(w)}[\log \sigma (-w_i \cdot w)]  $$&lt;/p&gt;

&lt;h4 id=&#34;glove-global-vectors&#34;&gt;GloVe (Global Vectors)&lt;/h4&gt;

&lt;p&gt;One grievance with skip-gram and CBOW is that since they are both window-based models, the co-occurrence statistics of the corpus are not used efficiently, thereby resulting in suboptimal embeddings. The GloVe model proposed by Pennington et al. seeks to solve this problem by formulating an objective function from probability statistics.&lt;/p&gt;

&lt;p&gt;Again, the original &lt;a href=&#34;https://nlp.stanford.edu/pubs/glove.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; is very pleasant to read (section 3 describes their model in detail), and it is interesting to note the derivation for the objective function:&lt;/p&gt;

&lt;p&gt;$$ J = \sum_{i,j=1}^V f(X_{ij})(w_i^Tw_j + b_i + b_j - \log X_{ij})^2 $$&lt;/p&gt;

&lt;p&gt;Here, $X_{ij}$ is the count of the word pair ($i$,$j$) in the corpus. The weight function $f(x)$ has 3 requirements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$f(0) = 0$, so that the entire term does not tend to $\infty$.&lt;/li&gt;
&lt;li&gt;It should be non-decreasing to assign low weights to rare occurrences.&lt;/li&gt;
&lt;li&gt;It should be relatively small for large values of $x$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again, please read the paper for details.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Although the matrix factorization approach and the neural embedding method may initially come off as completely independent, Levy and Goldberg (again!) ingeniously showed in a &lt;a href=&#34;https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf&#34; target=&#34;_blank&#34;&gt;NIPS 2014 paper&lt;/a&gt; that even the SGNS method implicitly factorizes a word-context matrix where the cells are the PMI (pointwise mutual information) of the respective word-context pairs, shifted by a global context. They derive this in Section 3.1 of the paper, and I urge you to go to the link and read it. It’s a delight! The derivation is really simple and I would have done it here, except that I would only be reproducing the exact proof.&lt;/p&gt;

&lt;p&gt;Very recently, Richard Socher’s group at Salesforce Research have proposed a new kind of embeddings called CoVe (Contextualized Word Vectors) in their paper. The idea is again borrowed from vision, where transfer learning has been used for a long time. Basically, models with various objectives are trained on a large dataset such as ImageNet, and then these weights are used to initialize model parameters for various vision tasks. Similarly, CoVe uses parameters trained on a attentional Seq2Seq machine translation task, and then uses it for various other tasks, including question-answering, where it has shown state-of-the-art performance on the SQuAD dataset. I have only skimmed through the paper, but I suppose such a deep transfer learning is naturally the next step towards improving word embeddings.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As an aside, there is a series of blog posts by Sanjeev Arora that analyzes the theory of semantic embeddings in great detail. There are 3 posts in the series:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.offconvex.org/2015/12/12/word-embeddings-1/&#34; target=&#34;_blank&#34;&gt;Semantic word embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.offconvex.org/2016/02/14/word-embeddings-2/&#34; target=&#34;_blank&#34;&gt;Word Embeddings: Explaining their properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.offconvex.org/2016/07/10/embeddingspolysemy/&#34; target=&#34;_blank&#34;&gt;Linear algebraic structure of word embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These provide great insight into the mathematics behind word vectors, and are beautifully written (which is no surprise since Prof. Arora is one of the authors of the famous and notoriously advanced book on Computational Complexity).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Relation extraction for clinical text</title>
      <link>https://desh2608.github.io/project/btp/</link>
      <pubDate>Sun, 30 Apr 2017 17:00:24 +0530</pubDate>
      
      <guid>https://desh2608.github.io/project/btp/</guid>
      <description>&lt;p&gt;The objective of the project was to devise a method for obtaining structured triplets from unstructured clinical records such as journal articles, patient health records etc. Simplifying this objective, I was tasked with creating a neural technique which can classify relations existing between entities in a given sentence, an NLP task known as relation classification.&lt;/p&gt;

&lt;p&gt;The key insight is that convolutions can capture short-term phrases, while recurrence learns long-term dependencies. Combining both, we proposed the CRNN model which outperformed earlier single and double layer methods on two benchmark datasets: i2b2-2010 and DDI. Details about the method can be found in the publication.&lt;/p&gt;

&lt;p&gt;This project was done as part of my undergraduate senior thesis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://desh2608.github.io/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://desh2608.github.io/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
