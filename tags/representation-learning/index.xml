<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Representation learning on Desh Raj</title>
    <link>https://desh2608.github.io/tags/representation-learning/</link>
    <description>Recent content in Representation learning on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 12 Apr 2018 13:41:14 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/tags/representation-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to Obtain Sentence Vectors</title>
      <link>https://desh2608.github.io/post/how-to-obtain-sentence-vectors/</link>
      <pubDate>Thu, 12 Apr 2018 13:41:14 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/how-to-obtain-sentence-vectors/</guid>
      <description>In several of my previous posts, I have discussed methods for obtaining word embeddings, such as SVD, word2vec, or GloVe. In this post, I will abstract a level higher and talk about 4 different methods that have been proposed to get embeddings for sentences.
But first, some of you may ask why do we even need a different method for obtaining sentence vectors. Since sentences are essentially made up of words, it may be reasonable to argue that simply taking the sum or the average of the constituent word vectors should give a decent sentence representation.</description>
    </item>
    
    <item>
      <title>Online Learning of Word Embeddings</title>
      <link>https://desh2608.github.io/post/online-learning-word-embeddings/</link>
      <pubDate>Wed, 14 Mar 2018 13:40:57 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/online-learning-word-embeddings/</guid>
      <description>Word vectors have become the building blocks for all natural language processing systems. I have earlier written an overview of popular algorithms for learning word embeddings here. One limitation with all these methods (namely SVD, skip-gram, and GloVe) is that they are all “batch” techniques. In this post, I will discuss two recent papers (which are very similar but were developed independently) which aim to provide an online approximation for the skip-gram algorithm.</description>
    </item>
    
    <item>
      <title>Irony Detection in Tweets</title>
      <link>https://desh2608.github.io/post/irony-detection-in-tweets/</link>
      <pubDate>Wed, 07 Feb 2018 13:40:06 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/irony-detection-in-tweets/</guid>
      <description>There was a SemEval 2018 Shared Task on “irony detection in tweets” that ended recently. As a fun personal project, I thought of giving it a shot, just to implement some new ideas. In this post, I will describe my approach for the problem along with some code.
Problem description The task itself was divided into two subtasks:
 Task A: Binary classification. Given a tweet, detect whether it has irony or not.</description>
    </item>
    
    <item>
      <title>Beyond Euclidean Embeddings</title>
      <link>https://desh2608.github.io/post/beyond-euclidean-embeddings/</link>
      <pubDate>Wed, 06 Dec 2017 13:39:15 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/beyond-euclidean-embeddings/</guid>
      <description>Representation learning, as the name suggests, seeks to learn representations for structures such as images, videos, words, sentencences, graphs, etc., which may then be used for several objectives. Arguably the most important representations used nowadays are word embeddings, usually learnt using the distributional semantics methods such as skip-gram or GloVe. I have previously written about these methods here.
Two assumptions are inherent while using these methods to learn word vectors:</description>
    </item>
    
    <item>
      <title>Understanding Word Vectors</title>
      <link>https://desh2608.github.io/post/understanding-word-vectors/</link>
      <pubDate>Fri, 29 Sep 2017 11:12:55 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/understanding-word-vectors/</guid>
      <description>This article is a formal representation of my understanding of vector semantics, from course notes and reading reference papers and chapters from Jurafsky’s SLP book. I will be talking about sparse and dense vector semantics, including SVD, skip-gram, and GloVe. In many places, I will try to explain the ideas in language rather than equations (but I’ll provide links to derivations and stuff wherever it is absolutely essential, which is actually everywhere!</description>
    </item>
    
  </channel>
</rss>