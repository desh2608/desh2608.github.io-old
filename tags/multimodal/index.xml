<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multimodal on Desh Raj</title>
    <link>https://desh2608.github.io/tags/multimodal/</link>
    <description>Recent content in Multimodal on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 09 Nov 2017 13:38:58 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/tags/multimodal/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning for Multimodal Systems</title>
      <link>https://desh2608.github.io/post/deep-learning-multimodal-systems/</link>
      <pubDate>Thu, 09 Nov 2017 13:38:58 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-multimodal-systems/</guid>
      <description>When I was browsing through research groups for my grad school applications, I came across some interesting applications of new deep learning methods in a multimodal setting. ‘Multimodal,’ as the name suggests, refers to any system involving two or more modes of input or output. For example, an image captioning system provides images as input and expects a textual output. Similarly, speech-to-text, descriptive art, video summarization, etc., are all examples of multimodal objectives.</description>
    </item>
    
  </channel>
</rss>