<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>learning theory on Desh Raj</title>
    <link>https://desh2608.github.io/tags/learning-theory/</link>
    <description>Recent content in learning theory on Desh Raj</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Tue, 31 Jul 2018 18:45:15 +0530</lastBuildDate>
    
	    <atom:link href="https://desh2608.github.io/tags/learning-theory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Theory of Deep Learning: An Illustration with Embeddings</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-5/</link>
      <pubDate>Tue, 31 Jul 2018 18:45:15 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-5/</guid>
      <description>

&lt;p&gt;We have discussed several aspects of deep learning theory, ranging from &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;optimization&lt;/a&gt; and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;generalization guarantees&lt;/a&gt; to &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-3/&#34; target=&#34;_blank&#34;&gt;role of depth&lt;/a&gt; and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-4/&#34; target=&#34;_blank&#34;&gt;generative models&lt;/a&gt;. In this final post of this series, I will illustrate how theory can motivate simple solutions to problems, which can then outperform complex techniques. For this, we will consider a field where deep learning has done exceptionally well, namely, word and sentence embeddings.&lt;/p&gt;

&lt;p&gt;If you need a refresher on word embeddings, I have previously explained them, along with the most popular methods, in &lt;a href=&#34;https://desh2608.github.io/post/understanding-word-vectors/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. The &lt;em&gt;distributional hypothesis&lt;/em&gt; forms the basis for all word embedding techniques used at present. Instead of naively taking the co-occurence matrix, though, almost all techniques use some low-rank approximation for the same. This gives rise to low-dimensional ($\sim 300$) dense embeddings for text. An important question, then, is the following: How can low-dimensional embeddings represent the complex linguistic structure in text? We will first look at this question from a theoretical perspective, based on &lt;a href=&#34;http://aclweb.org/anthology/Q16-1028&#34; target=&#34;_blank&#34;&gt;this ACL&amp;rsquo;16 paper&lt;/a&gt; from Arora et al.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;how-do-low-dimensional-embeddings-approximate-co-occurence-matrices&#34;&gt;How do low-dimensional embeddings approximate co-occurence matrices?&lt;/h3&gt;

&lt;p&gt;Formally, we want to see why, for some low-dimensional vector representations $v$, we have&lt;/p&gt;

&lt;p&gt;$$ \langle v_w,v_{w^{\prime}} \rangle \approx \text{PMI}(w,w^{\prime}), $$&lt;/p&gt;

&lt;p&gt;where $\text{PMI}(w,w^{\prime})$ is the pointwise mutual information between $w$ and $w^{\prime}$, defined as $\log \frac{P(w,w^{\prime})}{P(w)P(w^{\prime})}$, where the probabilities are computed empirically from the co-occurence matrix.&lt;/p&gt;

&lt;p&gt;For this, the authors propose a generative model of language, as opposed to the usual discriminative model that is based on predicting the context words given a target word (i.e., multiclass classification). This is based on the random walk of a discourse vector $c_t \in \mathcal{R}^d$, which generates $t$th word in step $t$. Every word has a time-invariant latent vector $v_w \in \mathcal{R}^d$, and the word production model is given as&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted at time} ~ t|c_t] \propto \exp(\langle c_t,v_w \rangle). $$&lt;/p&gt;

&lt;p&gt;Here, &lt;em&gt;random walk&lt;/em&gt; means that $c_{t+1}$ is obtained by adding a small random displacement vector to $c_t$. For a theoretic analysis, we make an isotropy assumption about the word vectors.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Isotropy assumption&lt;/strong&gt;: In the bulk, word vectors are distributed uniformly in the $\mathcal{R}^d$ space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To generate such a dsitribution, we can just sample i.i.d from $v = s \cdot v^{\prime}$, where $s$ is a scalar random variable ($s \leq \kappa$), and $v^{\prime}$ is obtained from a spherical Gaussian distribution. This is a simple Bayesian prior similar to the assumptions commonly used in statistics.&lt;/p&gt;

&lt;p&gt;Let us define $Z_c = \sum_{w}\exp(\langle v_w,c \rangle)$. This is like the normalization factor used with the above equation, but it is very difficult to compute. In the paper, the authors prove that this value is very close to some constant $Z$ for a fixed $c$. This allows us to remove this factor from consideration. Empirically, it has also been seen that some log-linear models have self-normalization properties, and this may be a reason for the observation. Let us now see how to prove this lemma.&lt;/p&gt;

&lt;p&gt;Since $Z_c$ is a sum of random variables, it may be tempting to use concentration inequalities to bound its value. However, we cannot do this since $Z_c$ is neither sub-Gaussian nor sub-exponential. We approach the problem it two parts. First we bound the mean and variance of $Z_c$, and then show that it is concentrated around its mean.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 1:&lt;/strong&gt; Suppose there are $n$ vectors in our space. Since they are identically distributed, we have&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[Z_c] = n\mathbb{E}[\exp(\langle v_w,c \rangle)] \geq n\mathbb{E}[1 + \langle v_w,c \rangle] = n. $$&lt;/p&gt;

&lt;p&gt;Here, we have used $\mathbb{E}[\langle v_w,c \rangle] = 0$, since $v_w$&amp;rsquo;s are drawn from a scaled uniform spherical Gaussian. Now, suppose all the scalar variables $s_w$ are equal in distribution to $s$. Then, we can write&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[Z_c] = n\mathbb{E}[\exp(\langle v_w,c \rangle)] = n\mathbb{E}\left[ \mathbb{E} [\exp(\langle v_w,c \rangle)|s]\right]. $$&lt;/p&gt;

&lt;p&gt;We can compute the conditional expectation as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E} [\exp(\langle v_w,c \rangle)|s] &amp;amp;= \int_x \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{x^2}{2\sigma^2} \right)\exp(x) dx \\\ &amp;amp;= \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{(x-\sigma^2)^2}{2\sigma^2} + \frac{\sigma^2}{2}\right) dx \\\ &amp;amp;= \exp(\frac{\sigma^2}{2}). \end{align} $$&lt;/p&gt;

&lt;p&gt;Here, the standard deviation is equal to the scaling factor $s$, and so $\sigma^2 = s^2$. It follows that&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}(Z_c) = n\exp(\frac{s^2}{2}). $$&lt;/p&gt;

&lt;p&gt;Similarly, we can show that the variance&lt;/p&gt;

&lt;p&gt;$$ \mathbb{V}(Z_c) \leq n\mathbb{E}[\exp(2s^2)]. $$&lt;/p&gt;

&lt;p&gt;Since $\langle v_w,c \rangle|s$ has a Gaussian distribution with variance $s^2 \leq \kappa^2$, we have using Chernoff bounds that&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[|\langle v_w,c \rangle| \geq \eta \log n |s] \leq \exp \left( - \frac{\eta^2 \log^2 n}{2\kappa^2} \right) = \exp (-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;Here we have removed $\eta$ and $\kappa$ since they are constants. We can now write the converse of this inequality, by taking expectation over all $s_w$, as&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[|\langle v_w,c \rangle| \leq \frac{1}{2}\log n] \geq 1 - \exp(-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;This means that, with high probability, $|\langle v_w,c \rangle| \leq \frac{1}{2}\log n$, or equivalently, $\exp(\langle v_w,c \rangle) \leq \sqrt{n}$. Now, let the random variable $X_w$ have the same distribution as $\exp(\langle v_w,c \rangle)$ when the above holds.&lt;/p&gt;

&lt;p&gt;Let us take a minute to understand what we are doing here. We do not know how to bound the original $Z_c$, since $\exp(\langle v_w,c \rangle)$ has no known concentration bounds. So we approximate it by a new random variable with high probability, so that we can compute bounds on the sum. Now, let $Z_{c}^{\prime} = \sum_{w}X_w$. We will now try to bound the mean and variance for this random variable.&lt;/p&gt;

&lt;p&gt;Computing the lower bound for the mean is simple since the mean of $\exp(\langle v_w,c \rangle)$ is zero, and so $\mathbb{E}[Z_c^{\prime}] \leq n$. We can similarly bound the variance as $\mathbb{V}[Z_c^{\prime}] \leq 1.1 \Lambda n$, where $\Lambda$ is a constant. Now, using Bernstein&amp;rsquo;s inequality, we get&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}\left[ | Z_c^{\prime} - \mathbb{E}[Z_c^{\prime}] | \geq \epsilon n \right] \leq \exp(-\Omega(\log^2 n)). $$&lt;/p&gt;

&lt;p&gt;Since $Z_c$ has the same distribution as $Z_c^{\prime}$, the above inequality also holds for the former. This means that the probability of $Z_c$ deviating from its mean is very low, and so we can say with high probability that&lt;/p&gt;

&lt;p&gt;$$ (1-\epsilon_z)Z \leq Z_c \leq (1+\epsilon_z)Z. $$&lt;/p&gt;

&lt;p&gt;The above proof was just to remove the normalization factor as a constant from the original problem, so that analysis becomes easier. We now come to the main result itself. Suppose $c$ and $c^{\prime}$ are consecutive discourse vectors and $w$ and $w^{\prime}$ are words generated from them. We have&lt;/p&gt;

&lt;p&gt;$$ \begin{align} p(w,w^{\prime}) &amp;amp;= \mathbb{E}_{c,c^{\prime}}[\text{Pr}[w,w^{\prime}|c,c^{\prime}]] \\\ &amp;amp;= \mathbb{E}_{c,c^{\prime}}[p(w|c)p(w^{\prime}|c^{\prime})] \\\ &amp;amp;= \mathbb{E}_{c,c^{\prime}}\left[ \frac{\exp(\langle v_w,c \rangle)}{Z_c}\right] \frac{\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)}{Z_{c^{\prime}}}. \end{align} $$&lt;/p&gt;

&lt;p&gt;As proved above, we can approximate the denominators to $Z$ and take them out of the expectation. This gives&lt;/p&gt;

&lt;p&gt;$$ \begin{align} p(w,w^{\prime}) &amp;amp;= \frac{1}{Z^2}\mathbb{E}_{c,c^{\prime}}[\exp(\langle v_w,c \rangle)\exp(\langle v_{w^{\prime}},c^{\prime} \rangle))] \\\ &amp;amp;= \frac{1}{Z^2}\mathbb{E}_c [\exp(\langle v_w,c \rangle)\mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)]]. \end{align}. $$&lt;/p&gt;

&lt;p&gt;We can compute the internal expectation term as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} \rangle)] &amp;amp;= \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} - c + c \rangle)] \\\ &amp;amp;= \mathbb{E}_{c^{\prime}|c}[\exp(\langle v_{w^{\prime}},c^{\prime} -c \rangle)]\exp(\langle v_{w^{\prime}},c \rangle) \\\ &amp;amp;\approx \exp(\langle v_{w^{\prime}},c \rangle). \end{align}$$&lt;/p&gt;

&lt;p&gt;Here, the last approximation can be done because we have assumed that our random walk has small steps, i.e., $|c^{\prime} - c|$ is small. Using this in above, we get&lt;/p&gt;

&lt;p&gt;$$ p(w,w^{\prime}) = \frac{1}{Z^2}\mathbb{E}[\exp(\langle v_w + v_{w^{\prime}},c \rangle)]. $$&lt;/p&gt;

&lt;p&gt;Since $c$ has uniform distribution over the sphere, the above resembles a Gaussian centered at 0 and variance $\frac{\lVert  v_w + v_{w^{\prime}} \rVert^2}{d}$. Since $\mathbb{E}[\exp(X)] = \exp(\frac{\sigma^2}{2})$ for $X \sim \mathcal{N}(0,\sigma^2)$, we get the closed form expression as&lt;/p&gt;

&lt;p&gt;$$ p(w,w^{\prime}) = \frac{1}{Z^2}\exp\left( \frac{\lVert  v_w + v_{w^{\prime}} \rVert^2}{2d} \right), $$&lt;/p&gt;

&lt;p&gt;which is the desired result. Note that I have ignored some technicalities for error bounds in this proof. We have now shown the original result that we wanted, but how did dimensionality help?&lt;/p&gt;

&lt;p&gt;The answer lies in the &lt;em&gt;isotropy assumption&lt;/em&gt; that we made at the very beginning. Having $n$ vectors be isotropic in $d$ dimensions requires $d &amp;lt;&amp;lt; n$, which is indeed what is observed empirically. Hence, theory justifies experimental findings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;an-algorithm-for-sentence-embeddings&#34;&gt;An algorithm for sentence embeddings&lt;/h3&gt;

&lt;p&gt;In a previous part of this series, I echoed Prof. Arora&amp;rsquo;s concern that theoretical analysis at present is like a postmortem analysis, where we try to find properties of the model that can explain certain empirical findings. The ideal scenario would be where we can use this understanding to guide future learning models. In this section, I will look at &lt;a href=&#34;https://openreview.net/pdf?id=SyK00v5xx&#34; target=&#34;_blank&#34;&gt;this paper from ICLR&amp;rsquo;17&lt;/a&gt; which uses the understanding from the previous section to build simple but strong word embeddings.&lt;/p&gt;

&lt;p&gt;Suppose we want to obtain the vector for a piece of text, say, a sentence. From our generative model defined in the previous section, it would be reasonable to say that this can be approximated by a &lt;em&gt;max a priori&lt;/em&gt; (MAP) estimate of the discourse vector that generated the sentence, i.e.,&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted in sentence} ~ s | c_s] = \frac{\exp(\langle c_s,v_w \rangle)}{Z_{c_s}}, $$&lt;/p&gt;

&lt;p&gt;where $c_s$ is the discourse vector that remains approximately constant for the sentence. However, we need to modify this slightly to account for two real situations.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Some words often appear out of context, and some stop words appear regardless of discourse. To approximate this, we add a term $\alpha p(w)$ to the log-linear model, where $p(w)$ is the unigram probability of the word. This makes probability of appearance of some words high even if they have low correlation with the discourse vector.&lt;/li&gt;
&lt;li&gt;Generation of words depends not just on current sentence, but on entire history of discourse. To model this, we use discourse vector $\tilde{c}_s = \beta c_0 + (1-\beta)c_s$, where $c_0$ is the common discourse vector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, the modified log-linear objective is as follows.&lt;/p&gt;

&lt;p&gt;$$ \text{Pr}[w ~ \text{emitted in sentence} ~ s | c_s] = \alpha p(w) + (1-\alpha) \frac{\exp(\langle \tilde{c}_s,v_w \rangle)}{Z_{\tilde{c}_s}} $$&lt;/p&gt;

&lt;p&gt;After the word embeddings have been trained using this objective, we can model the likelihood for obtaining sentence $s$ given discourse vector $c_s$ as&lt;/p&gt;

&lt;p&gt;$$ p[s|c_s] = \prod_{w\in s}p(w|c_s) = \prod_{w\in s}\left[ \alpha p(w) + (1-\alpha) \frac{\exp(\langle \tilde{c}_s,v_w \rangle)}{Z} \right]. $$&lt;/p&gt;

&lt;p&gt;Here, we have taken $Z_{\tilde{c}_s} = Z$, in accordance with the result we proved earlier. To maximize this expression, we just need to maximize the term inside the product. Taking $f_w(\tilde{c}_s)$ to denote the term inside the product, we can easily compute its derivative, and then use Taylor expansion, $f_w(\tilde{c}_s) = f_w(0) + \nabla f_w(\tilde{c}_s)^T \tilde{c}_s$, to get an expression for $f_w(\tilde{c}_s)$. Finally, we have&lt;/p&gt;

&lt;p&gt;$$ \text{arg}\max\sum_{w\in s}f_w(\tilde{c}_s) \propto \sum_{w\in s}\frac{a}{p(w)+a}v_w, $$&lt;/p&gt;

&lt;p&gt;where $a = \frac{1-\alpha}{\alpha Z}$. If we analyze this expression, this is simply a weighted sum of the word vectors in the sentence, which is one of the most common bag-of-words technique to obtain sentence embeddings. Furthermore, the weight is low if the unigram frequency of the word is high. This is similar to Tf-idf weighting of words. Now, this theory gives rise to the following algorithm, taken from the original paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/25/sif.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is a striking illustration of how rigorously developed theoretical results can guide construction of simple algorithms in deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Final note:&lt;/strong&gt; This series was based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, which is why the discussion revolved mostly around the work done by his group. The papers themselves are not very trivial to understand, but the &lt;a href=&#34;www.offconvex.org&#34; target=&#34;_blank&#34;&gt;blog posts&lt;/a&gt; are more beginner friendly, and highly recommended. Several people criticize deep learning for being purely intuition-based, but I believe that will change soon, given that so much good research is being done to develop a theory for it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Generative Models</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-4/</link>
      <pubDate>Mon, 30 Jul 2018 23:39:37 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-4/</guid>
      <description>

&lt;p&gt;Till now, in this series based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, we have limited our discussion to the theory of supervised discriminative neural models, i.e., those models which learn the conditional probability $P(y|x)$ from a set of given $(x_i,y_i)$ samples. In particular, we saw &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;how deep networks find good solutions&lt;/a&gt;, &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;why they generalize well&lt;/a&gt; despite being overparametrized, and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-3/&#34; target=&#34;_blank&#34;&gt;what role depth plays&lt;/a&gt; in all of this.&lt;/p&gt;

&lt;p&gt;We now turn our attention towards the theory of unsupervised learning and generative models, with special emphasis on variational autoencoders and generative adversarial networks (GANs). But first, &lt;em&gt;what is unsupervised learning&lt;/em&gt;?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Evidently, unsupervised learning is much more abstract than its supervised counterpart. In the latter, our objective was essentially to find a function that approximates the original mapping of the distribution $\mathcal{X}\times\mathcal{Y}$. In the unsupervised domain, there is no such objective. We are given input data, and we want to learn &amp;ldquo;structure&amp;rdquo;. The most obvious way to understand why this is more difficult is to realize that &lt;em&gt;drawing a picture of a lion is much more difficult than identifying a lion in a picture&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Why is learning structures important? Creating large annotated datasets is an expensive task, and may even be infeasible for some problems such as parsing, which require significant domain knowledge. Let&amp;rsquo;s consider the simplest problem of image classification. The largest dataset for this problem, ImageNet, contains 14 million images, with 20000 distinct output labels. However, the number of images freely available online far exceeds 14 million, which means that we can probably learn something from them. This kind of &lt;strong&gt;transfer learning&lt;/strong&gt; is the most important motivation for unsupervised learning.&lt;/p&gt;

&lt;p&gt;For instance, while training a machine translation model, obtaining a parallel corpus may be difficult, but we always have access to unilateral text corpora in different languages. If we then try to learn some underlying structure present in these languages, it can assist the downstream translation task. In fact, recent advances in &lt;a href=&#34;https://desh2608.github.io/post/transfer-learning-nlp/&#34; target=&#34;_blank&#34;&gt;transfer learning for NLP&lt;/a&gt; have empirically proven that huge performance gains are possible using such a technique.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1206.5538.pdf&#34; target=&#34;_blank&#34;&gt;Representation learning&lt;/a&gt;&lt;/strong&gt; is perhaps the most widely studied aspect of unsupervised learning. A &amp;ldquo;good representation&amp;rdquo; often means one which disentangles factors of variation, i.e, each coordinate in the representation corresponds to one meaningful factor of variation. For example, if we consider word embeddings, an ideal vector representing a word would depict different features of the word along each dimension. However, this is easier said than done, since learning representations require an objective function, and it is still unknown how to translate these notions of &amp;ldquo;good representation&amp;rdquo; into training criteria. For this reason, representation learning is often criticized for getting too much attention for transfer learning. The essence of the criticism, taken from &lt;a href=&#34;https://www.inference.vc/goals-and-principles-of-representation-learning/&#34; target=&#34;_blank&#34;&gt;this post by Ferenc Husz√°r&lt;/a&gt; is this:&lt;/p&gt;

&lt;p&gt;If we identified transfer learning as the primary task representation learning is supposed to solve, are we actually sure that representation learning is the way to solve it? One can argue that there may be many ways to transfer information from some dataset over to a novel task. Learning a representation and transferring that is just one approach. Meta-learning, for example, might provide another approach.&lt;/p&gt;

&lt;p&gt;In the discussion so far, we have blindly assumed that the data indeed contains structures that can be learnt. This is not an oversight; it is actually based on the &lt;strong&gt;manifold assumption&lt;/strong&gt; which we will discuss next.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;the-manifold-assumption&#34;&gt;The manifold assumption&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;A manifold is a topological space that locally resembles Euclidean space near each point.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that globally, a manifold may not be a Euclidean space. The only requirement for an $n$-manifold, i.e., a manifold in $n$ dimensions, is that each point of the manifold must have a neighborhood that is homeomorphic to the Euclidean space of $n$ dimensions. There are three technicalities in this definition.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;neighborhood&lt;/em&gt; of a point $p$ in $X$ is a $V \subset X$ which contains an open set $U$ containing $p$, i.e., $p$ must be in the interior of $V$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A function $f: X \rightarrow Y$ between two topological spaces $X$ and $Y$ is called a &lt;em&gt;homeomorphism&lt;/em&gt; if it has the following properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$f$ is a bijection,&lt;/li&gt;
&lt;li&gt;$f$ is continuous,&lt;/li&gt;
&lt;li&gt;$f^{-1}$ is continuous.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Euclidean space&lt;/em&gt; is a topological space such that&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;it is in 2 or 3 dimensions and obeys Euclidean postulates, or&lt;/li&gt;
&lt;li&gt;it is in any dimension such that points are given by coordinates and satisfy Euclidean distance.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that the dimension of a manifold may not always be the same as the dimension of the space in which the manifold is embedded. Dimension here simply means the degree of freedom of the underlying process that generated the manifold. As such, lines and curves, even if embedded in $\mathbb{R}^3$, are one-dimensional manifolds.&lt;/p&gt;

&lt;p&gt;With this definition in place, we can now state the manifold assumption. It hypothesizes that the intrinsic dimensionality of the data is much smaller than the ambient space in which the data is embedded. This means that if we have some data in $N$ dimensions, there must be an underlying manifold $\mathcal{M}$ of dimension $n &amp;lt;&amp;lt; N$, from which the data is drawn based on some probability distribution $f$. The goal of unsupervised learning in most cases, is to identify such a manifold.&lt;/p&gt;

&lt;p&gt;It is easy to see that the manifold assumption is, as the name suggests, just an assumption, and does not hold universally. Otherwise, applying the assumption consecutively, we would be able to represent any high-dimensional data using a one-dimensional manifold, which, of course, is not possible.&lt;/p&gt;

&lt;p&gt;The task of manifold learning is modeled as approximating the joint probability density $p(x,z)$, where $x$ is the data point and $z$ is its underlying &amp;ldquo;code&amp;rdquo; on the manifold. Deep generative models have come to be accepted as the standard for estimating this probability, because of two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Deep models promote reuse of features. We have already seen in the previous post that depth is analogous to composition whereas width is analogous to addition. Composition offers more representation capability than addition using the same number of parameters.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep models are conjectured to lead to progressively more abstract features at higher levels of representation. An example of this is the commonly known phenomenon in training deep convolutional networks on image data, where it is found that the first few layers learn lines, blobs, and other local features, and higher level layers learn more abstract features. This is done explicitly using the pooling mechanism.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;theory-of-variational-autoencoders&#34;&gt;Theory of Variational Autoencoders&lt;/h3&gt;

&lt;p&gt;Deep learning models often face some flak for being purely intution-based. &lt;a href=&#34;https://arxiv.org/pdf/1606.05908.pdf&#34; target=&#34;_blank&#34;&gt;Variational autoencoders (VAEs)&lt;/a&gt; are the practitioner&amp;rsquo;s answer to such criticisms, since they are rooted in the theory of Bayesian inference, and also perform well empirically. In this section, we will look at the theory that forms VAEs.&lt;/p&gt;

&lt;p&gt;First, we formalize the notion of the &amp;ldquo;code&amp;rdquo; that we mentioned earlier using the concept of a &lt;strong&gt;latent variable&lt;/strong&gt;. These are those variables that are not directly observed but are inferred from the observable variables. For instance, if the model is drawing a picture of an MNIST digit, it would make sense to first have a variable choose a digit from $[0,\ldots,9]$, and then draw the strokes corresponding to the digit.&lt;/p&gt;

&lt;p&gt;Formally, suppose we have a vector of latent variables $z$ in a high-dimensional space $\mathcal{Z}$ which can be sampled using a probability distribution $P(z)$. Then, suppose we have a family of deterministic functions $f(z;\theta)$ parametrized by $\theta \in \Theta$, such that $f:\mathcal{Z}\times \Theta \rightarrow \mathcal{X}$. The task, then, is to optimize $\theta$ such that we can sample $z$ from $P(z)$ and with high probability, $f(z;\theta)$ will be like the $X$&amp;rsquo;s in our dataset. As such, we can write the expression for the generated data as&lt;/p&gt;

&lt;p&gt;$$ X^{\prime} = f(z;\theta). $$&lt;/p&gt;

&lt;p&gt;Now, since we have no idea how to check if randomly generated images are &amp;ldquo;like&amp;rdquo; our dataset, we use the notion of &amp;ldquo;maximum likelihood&amp;rdquo;, i.e., if the model is likely to produce training set samples, then it is also likely to produce similar samples and unlikely to produce dissimilar ones. With this assumption, we want to maximize the probability of each $X$ in the training process. We can now replace $f(z;\theta)$ by the conditional probability $P(X|z;\theta)$, and we get&lt;/p&gt;

&lt;p&gt;$$ P(X) = \int P(X|z;\theta)P(z)dz. $$&lt;/p&gt;

&lt;p&gt;In VAEs, we usually have $P(X|z;\theta) = \mathcal{N}(X|f(z;\theta),\sigma^2 I)$, which is a Gaussian. Using this formalism, we can use gradient descent to increase $P(X)$ by making $f(z;\theta)$ approach $X$ for some $z$. So essentially, VAEs do the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sample $z$ from some known distribution.&lt;/li&gt;
&lt;li&gt;Feed $z$ into some parametrized function to get $X$.&lt;/li&gt;
&lt;li&gt;Tune the parameters of the function such that generated $X$ resemble those in dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this process, two questions arise:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we define $z$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;VAEs simply sample $z$ from $\mathcal{N}(0,I)$, where $I$ is the identity matrix. The motivation for this choice is that any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function. I do not prove this here, but the proof is based on taking the composition of the inverse cumulative distribution function (CDF) of the desired distribution with the CDF of a Gaussian.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we deal with $\int dz$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We need to understand that the space $\mathcal{Z}$ is very large, and there are only few $z$ which generate realistic $X$, which makes it very difficult to sample &amp;ldquo;good&amp;rdquo; values of $z$ from $P(z)$ . Suppose we have a function $Q(z|X)$ which, given some $X$, gives a distribution over $z$ values that are likely to produce $X$. Now to compute $P(X)$, we need to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;relate $P(X)$ with $\mathbb{E}_{z\sim Q}P(X|z)$, and&lt;/li&gt;
&lt;li&gt;estimate $\mathbb{E}_{z\sim Q}P(X|z)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the first, we use KL-divergence (that we saw in the previous post) between the probability distribution estimated by $Q$ to the actual conditional probability distribution as follows.&lt;/p&gt;

&lt;p&gt;$$ \begin{align} &amp;amp; \mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q}[\log Q(z|X) - \log P(z|X)] \\\ &amp;amp;= \mathbb{E}_{z\sim Q}\left[ \log Q(z|X) - \log \frac{P(X|z)P(z)}{P(X)} \right] \\\ &amp;amp;= \mathbb{E}_{z\sim Q} [ \log Q(z|X) - \log P(X|z) - \log P(z) ] + \log P(X) \\\ \Rightarrow &amp;amp; \log P(X) - \mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \mathbb{E}_{z\sim Q}[\log P(X|z)] - \mathcal{D}_{KL}[Q(z|X)||P(z)] \end{align} $$&lt;/p&gt;

&lt;p&gt;In the LHS of the above equation, we have an expression that we want to maximize, since we want $P(X)$ to be large and we want $Q$ to approximate the conditional probability distribution (this was our objective of using KL-divergence). If we use a sufficiently high-capacity model for $Q$, the $\mathcal{D}_{KL}$ term will approximate $0$, in which case we will directly be optimizing $P(X)$.&lt;/p&gt;

&lt;p&gt;Now we are just left with finding some way to optimize the RHS in the equation. For this, we will have to choose some model for $Q$. An obvious (and usual) choice is to take the multivariate Gaussian, i.e., $Q(z|X) = \mathcal{N}(z|\mu(X),\Sigma(X))$. Since $P(z) = \mathcal{N}(0,I)$, the KL-divergence term on the RHS can now be written as&lt;/p&gt;

&lt;p&gt;$$ \mathcal{D}_{KL}[\mathcal{N}(\mu(X),\sum(X))||\mathcal{N}(0,I)] = \frac{1}{2}\left( \text{tr}(\Sigma(X)) + (\mu(X))^T (\mu(X)) - k - \log \text{det}(\Sigma(X)) \right). $$&lt;/p&gt;

&lt;p&gt;To estimate the first term on the RHS, we just compute the term for one sample of $z$, instead of iterating over several samples. This is because during stochastic gradient descent, different values of $X$ will automatically require us to sample $z$ several times. With this approximation, the optimization objective for a single sample $X$ becomes&lt;/p&gt;

&lt;p&gt;$$ J = \log P(X|z) - \mathcal{D}_{KL}[Q(z|X)||P(z)]. $$&lt;/p&gt;

&lt;p&gt;This can be represented in the form of a feedforward network by the figure on the left below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/24/vae.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There is, however, a caveat. The network is not trainable using backpropagation because the red box is a stochastic step, which means that it is not differentiable. To solve this problem, we use the &lt;strong&gt;reparametrization trick&lt;/strong&gt; as follows.&lt;/p&gt;

&lt;p&gt;$$ z = \mu(X) + \Sigma^{\frac{1}{2}}(X)  \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0,I) $$&lt;/p&gt;

&lt;p&gt;After this trick, we get the final network as shown in the right in the above figure. Furthermore, we must have $\mathcal{D}_{KL}[Q(z|X)||P(z|X)]$ approximately equal $0$ in the LHS. Since we have taken $Q$ to be a Gaussian, this means that the original density function $f$ should be such that $P(z|X)$ is a Gaussian. It turns out that such a function, which maximizes $P(X)$ and satisfies the said criteria, provably exists.&lt;/p&gt;

&lt;p&gt;Although VAEs have strong theoretical support, they do not work very well in practice, especially in problems such as face generation. This is because the loss function used for training is log-likelihood, which ultimately leads to fuzzy face images which have high match with several $X$. Instead of using likelihood, we use the power of discriminative deep learning, which is where GANs come into the picture.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;generative-adversarial-networks-new-insights&#34;&gt;Generative adversarial networks: new insights&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.00160&#34; target=&#34;_blank&#34;&gt;GANs&lt;/a&gt; were proposed in 2014, and have become immensely popular in computer vision ever since. They are basically motivated from game theory, and I will not get into the details here since the tutorial by Ian Goodfellow is a excellent resource for the same.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/24/gan.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since the prior learnt by the generator depends upon the discriminative process, an important issue with GANs is that of &lt;strong&gt;mode collapse&lt;/strong&gt;. The problem is that since the discriminator only learns from a few samples, it may be unable to teach the generator to produce $\mathcal{P}_{synth}$ with sufficiently large diversity. In the context of what we have already seen, this can be taken as the problem of generalization for GANs.&lt;/p&gt;

&lt;p&gt;In this section, I will discuss three results from two important papers from Arora et al. which deal with mode collapse in GANs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.00573.pdf&#34; target=&#34;_blank&#34;&gt;Generalization and equilibrium in generative adversarial nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=BJehNfW0-&#34; target=&#34;_blank&#34;&gt;Do GANs learn the distribution? Some theory and empirics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For all our discussions in this section, we will consider the Wasserstein GAN objective instead of the usual minimax objective, which is as follows (and arguably more intuitive)&lt;/p&gt;

&lt;p&gt;$$ J = \lvert \mathbb{E}_{x\in \mathcal{P}_{real}}[D(x)] - \mathbb{E}_{x\in \mathcal{P}_{synth}}[D(x)] \rvert, $$&lt;/p&gt;

&lt;p&gt;where $D$ is the discriminator.&lt;/p&gt;

&lt;h4 id=&#34;1-generalization-depends-on-discriminator-size&#34;&gt;1. Generalization depends on discriminator size&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;If the discriminator size is $n$, then there exists a generator supported on $\mathcal{O}(n\log n)$ images, which wins against all possible discriminators.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that if we have a discriminator of size $n$, then the best possible generator training is possible using $Cn/\epsilon^2 \log n$ images from the full training set. Any more images will improve the training objective by at most $\epsilon$. I will now give the proof (simplified from the actual proof in the paper).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Suppose $\mu$ denotes the actual distribution learnt by the generator and $\nu$ denotes the actual distribution of real images that the discriminator has access to. Let $\tilde{\mu}$ and $\tilde{\nu}$ be the empirical versions of the above distributions, i.e., the distributions that we actually use for training. Let $d(p,q)$ be some distance measure between the two distributions.&lt;/p&gt;

&lt;p&gt;In the paper, the authors have defined an $\mathcal{F}$-distance that has good generalization properties, but I will not get into the details of that here for sake of simplicity. For this discussion, just assume that the distance measure is $d$. From my earlier post on generalization error in supervised learning, we say that a model generalizes well when, for some $\epsilon$,&lt;/p&gt;

&lt;p&gt;$$ |\text{True error} - \text{Empirical error}| \leq \epsilon. $$&lt;/p&gt;

&lt;p&gt;Here, we don&amp;rsquo;t really know the error, but we can use our distance measure to the same effect. If the size of discriminator is $p$, we want to compute the sample complexity $m$ in terms of $p$ and $\epsilon$ such that the GAN generalizes. For that, we need a few approximations.&lt;/p&gt;

&lt;p&gt;First we approximate the parameter space $\mathcal{V}$ using its $\frac{\epsilon}{8}$-net $\mathcal{X}$. This means that for every $\nu \in \mathcal{V}$, we can find a $\nu^{\prime}\in \mathcal{X}$ which is at a distance of at most $\frac{\epsilon}{8}$ from it. Assuming that the function computed by the discriminator $D$ is 1-Lipschitz, we can then say that $\lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \mu}[D_{\nu^{\prime}}(x)]  \rvert \leq \frac{\epsilon}{8}$.&lt;/p&gt;

&lt;p&gt;The $\epsilon$-net is taken so that we can apply concentration inequalities in this continuous finite space. You can read more about them &lt;a href=&#34;https://www.ti.inf.ethz.ch/ew/lehre/CG12/lecture/Chapter%2015.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Now, we can use Hoeffding&amp;rsquo;s inequality to bound the difference between true and empirical errors on this space as&lt;/p&gt;

&lt;p&gt;$$ P\left[ $\lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \tilde{\mu}}[D_{\nu}(x)]  \rvert \geq \frac{\epsilon}{4} \right] \leq 2\exp \left( -\frac{\epsilon^2 m}{2} \right). $$&lt;/p&gt;

&lt;p&gt;Taking union bound over all $p$ parameters, we get that when $m \geq \frac{Cp\log (p/\epsilon)}{\epsilon^2}$, then the bound holds with high probability. Note that this sample complexity is $m = \mathcal{p\log p}$, which is what we wanted. Now we just need to show that this bound implies that the generalization error is bounded. Since we have taken the $\frac{\epsilon}{8}$-net approximation, we translate both the parameters in $\mathcal{X}$ back to $\mathcal{V}$, paying a cost of $\frac{\epsilon}{8}$ for each. Finally, we get, for every $D$,&lt;/p&gt;

&lt;p&gt;$$ \lvert \mathbb{E}_{x\sim \mu}[D_{\nu}(x)] - \mathbb{E}_{x\sim \tilde{\mu}}[D_{\nu}(x)]  \rvert \leq \frac{\epsilon}{2}. $$&lt;/p&gt;

&lt;p&gt;We can prove a similar upper bound for $\nu$. Finally, with similar approximation arguments, and from the definition of our distance function, we get the desired result.&lt;/p&gt;

&lt;h4 id=&#34;2-existence-of-equilibrium&#34;&gt;2. Existence of equilibrium&lt;/h4&gt;

&lt;p&gt;For GANs to be successful, they must find an equilibrium in the G-D game where the generator wins. In the context of the minimax equation, this means that switching min and max in the objective should not cause any change in the equilibrium. In the paper, the authors prove an $\epsilon$-approximate equilibrium, i.e., one where such a switching affects the expression by at most $\epsilon$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If a generator net is able to generate a Gaussian distribution, then there exists an $\epsilon$-approximate equilibrium where the generator has capacity $\mathcal{O}(n\log n / \epsilon^2)$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The proof of this result lies in a classical result in statistics, which says that any probability distribution can be approximated by a mixture of infinite Gaussians. For this, we just need to take the standard Gaussian $P(x)\mathcal{N}(x,\sigma^2)$ at every $x \in \mathcal{X}$ such that $\sigma^2 \rightarrow 0$, and take the mixture of all such Gaussians. The remaining proof is similar to the one done for the previous result, so I will not repeat it here.&lt;/p&gt;

&lt;h4 id=&#34;3-empirically-detecting-mode-collapse&#34;&gt;3. Empirically detecting mode collapse&lt;/h4&gt;

&lt;p&gt;We have already seen that GAN training can be successful even if the generator has not learnt a good enough distribution, if the discriminator is small. But suppose we take a really large discriminator and then train our GAN to a minima. How do we still make sure that the generator distribution is good? It could well be the case that the generator has simply memorized the training data, due to which the discriminator cannot make a better guess than random. Researchers have proposed several qualitative checks to test this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Check the similarity of each generated image to the nearest image in the training set.&lt;/li&gt;
&lt;li&gt;If the seed formed by interpolating two seeds $s_1$ and $s_2$ that produce realistic images, also produces realistic images, then the learnt distribution probably has many realistic images.&lt;/li&gt;
&lt;li&gt;Check for semantically important directions in latent space, which cause predictable changes in generated image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will now see a new empirical measure for the support size of the trained distribution, based on the Birthday Paradox.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The birthday paradox&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a room of just 23 people, there&amp;rsquo;s a 50% chance of finding 2 people who share their birthday.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see why, refer to &lt;a href=&#34;https://betterexplained.com/articles/understanding-the-birthday-paradox/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. It is a simple problem of permutation and combination, followed by using the approximation for $e^x$.&lt;/p&gt;

&lt;p&gt;Since $23 \approx \sqrt{365}$, we can generalize this to mean that if a distribution has support $N$, we are likely to find a duplicate in a batch of about $\sqrt{N}$ samples taken from this distribution. As such, finding the smallest batch size $s$ which ensures duplicate images with good probability almost guarantees that the distribution has support $s^2$. Let us formalize this guarantee.&lt;/p&gt;

&lt;p&gt;Suppose we have a probability distribution $P$ on a set $\Omega$. Also, let $S \subset \Omega$ such that $\sum_{s\in S}P(s)\geq \rho$ and $|S|=N$. Then from calculations similar to the one done for birthday paradox, we can say that the probability of finding at least one collision on drawing $M$ i.i.d samples is at least $1 - \exp\left( -\frac{(M^2 - M)\rho}{2N} \right)$.&lt;/p&gt;

&lt;p&gt;Now, suppose we have empirically found this minimum probability of collision to be $\gamma$. Then it can be shown that under realistic assumptions on parameters, the following holds:&lt;/p&gt;

&lt;p&gt;$$ N \leq \frac{2M\rho^2}{\left(-3 + \sqrt{9+\frac{24}{M}\log \frac{1}{1-\gamma}}\right)-2M(1-\rho)^2} $$&lt;/p&gt;

&lt;p&gt;This gives an upper bound on the support size of the distribution learned by the generator.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Generative models are definitely very promising, especially with the recent interest in transfer learning with unsupervised pretraining. While I have tried to explain the recent insights into GANs as best as possible, it is not possible to explain every detail in the proof in an overview post. Even so, I hope I have been able to at least give a flavor of how veterans in the field approach theoretical guarantees.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Role of Depth</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-3/</link>
      <pubDate>Sat, 28 Jul 2018 23:00:20 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-3/</guid>
      <description>

&lt;p&gt;In the previous posts of this series, we have looked at &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;how stochastic gradient descent is able to find a good solution&lt;/a&gt; despite the nonconvex objective, and &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;why overparametrized neural networks generalize so well&lt;/a&gt;. In this post, we will look at the titular property of deep networks, namely depth, and what role they play in the learning ability of the model.&lt;/p&gt;

&lt;p&gt;An ideal result in this regard would be if we can show that there exists a class of natural learning problems (recall the idea of a &amp;ldquo;natural&amp;rdquo; problem from the first post) which cannot be solved with depth $d$ neural networks, but are solvable with at least one model of depth $d+1$. However, such a result is elusive at present, since we have already established that there exists no mathematical formulation of a &amp;ldquo;natural&amp;rdquo; learning problem.&lt;/p&gt;

&lt;p&gt;However, there has been some advancement in establishing similar results in the case of less natural problems, and in this regard, the following papers are worth mentioning.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v49/eldan16.pdf&#34; target=&#34;_blank&#34;&gt;The Power of Depth for Feedforward Neural Networks&lt;/a&gt; by Eldan and Shamir&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v49/telgarsky16.pdf&#34; target=&#34;_blank&#34;&gt;Benefit of Depth in Neural Networks&lt;/a&gt; by Telgarsky&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will now discuss both of these in some detail.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;role-of-depth-for-less-natural-problems&#34;&gt;Role of depth for &amp;ldquo;less natural&amp;rdquo; problems&lt;/h3&gt;

&lt;h4 id=&#34;1-approximating-radial-functions&#34;&gt;1. Approximating radial functions&lt;/h4&gt;

&lt;p&gt;At the outset, note that if we allow the neural network to be unbounded, i.e., have exponential width, even a 2-layer network can approximate any continuous function. As such, for our study, we only use &amp;ldquo;bounded&amp;rdquo; networks where the number of hidden layer units cannot be exponential in the dimension of input. With this understanding, we will look at the simplest case: we try to find a function (or a family of functions) that are expressible by a 3-layer network but cannot be expressed by any 2-layer network. Before we get into the details, we first look at what a 2-layer and 3-layer networks represent.&lt;/p&gt;

&lt;p&gt;A 2-layer network represents the following function:&lt;/p&gt;

&lt;p&gt;$$ f_2(\mathbf{x}) = \sum_{i=1}^w v_i \sigma(&amp;lt; \mathbf{w}_i,\mathbf{x} &amp;gt;+b_i), $$&lt;/p&gt;

&lt;p&gt;and a 3-layer network represents the following:&lt;/p&gt;

&lt;p&gt;$$ f_3(\mathbf{x}) = \sum_{i=1}^w u_i \sigma\left( \sum_{j=1}^w v_{i,j} \sigma (&amp;lt; \mathbf{w}_{i,j},\mathbf{x} &amp;gt;+ b_{i,j}) + c_i \right). $$&lt;/p&gt;

&lt;p&gt;Here, $w$ is the size of the hidden layer and $\sigma$ is an activation function. The only constraint on $\sigma$ is that it should be &amp;ldquo;universal&amp;rdquo;, i.e., a 2-layer network should be able to approximate any Lipschitz function that is non-constant on a bounded domain for some $w$ (which need not be bounded). This constraint is satisfied by all the standard activation functions such as sigmoid and ReLU.&lt;/p&gt;

&lt;p&gt;Under this assumption, the main result in the paper is as follows:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There exists a &lt;strong&gt;radial function $g$&lt;/strong&gt; depending only on the norm of the input, which is expressible by a 3-layer network of width polynomial in the input dimension, but not by any 2-layer neural network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;More importantly, apart from the universal assumption, this result does not depend on any characteristic of $\sigma$. Furthermore, there are no constraints on the size of the parameters $\mathbf{w}$. The only constraint worth noting is that $g$ must be a &lt;em&gt;radial function&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To prove this result, we need to show 2 things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$g$ can be approximated by a 3-layer neural network.&lt;/li&gt;
&lt;li&gt;$g$ cannot be approximated by any 2-layer network of bounded width.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Part 1:&lt;/strong&gt; This is trivial to show, since any radial function can be approximated by a 3-layer network. To do this, we compute the Euclidean norm $\lVert \mathbf{x} \rVert^2$ from the input $\mathbf{x}$ in the first layer using a linear combination of neurons. This is possible because the squared norm is just the sum of squares of all the components, and each squared component can be approximated in a finite range, for example, using the step function.&lt;/p&gt;

&lt;p&gt;Once the norm is computed, the second layer can be used to approximate the required radial function using RBF nodes. This completes the construction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 2:&lt;/strong&gt; Let the input be taken from a probability distribution $\mu$, which has a density function $\phi^2(x)$, for some known function $\phi$. Suppose we are trying to approximate a function $f$ using a function $g$. Then, the distance between the functions can be given as&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbb{E}_{\mu}(f(x)-g(x))^2 &amp;amp;= \int (f(x)-g(x))^2 \phi^2(x) dx \\\ &amp;amp;= \int (f(x)\phi (x) - g(x)\phi (x))^2 dx \\\ &amp;amp;= \lVert f\phi - g\phi \rVert_{L_2}^2 \end{align} $$&lt;/p&gt;

&lt;p&gt;Now, we can replace $f\phi$ and $g\phi$ with their respective Fourier transforms since the Fourier transform is an isometric mapping (i.e., distance remains same before and after the mapping). Therefore, we get&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}_{\mu}(f(x)-g(x))^2 = \lVert \hat{f\phi} - \hat{g\phi} \rVert_{L_2}^2 $$&lt;/p&gt;

&lt;p&gt;While this replacement may seem arbitrary at first, it has a very clear motivation. We have done this because the Fourier transform of functions expressible by a 2-layer network has a very particular form, which we will use here. Specifically, consider the function&lt;/p&gt;

&lt;p&gt;$$ f(x) = \sum_{i=1}^k f_i (&amp;lt; \mathbf{v}_i,\mathbf{x} &amp;gt;), $$&lt;/p&gt;

&lt;p&gt;which is expressible by any 2-layer network. The component function $f_i (&amp;lt; \mathbf{v}_i,\mathbf{x} &amp;gt;)$ is constant in any direction perpendicular to $\mathbf{v}_i$ and so its Fourier transform is non-zero only in the direction of $\mathbf{v}_i$, and so the whole distribution is supported on $\bigcup_i \text{span}(\mathbf{v}_i)$. Now we just need to compute the support of $\hat{\phi}$, and then we can directly use the convolution-multiplication principle.&lt;/p&gt;

&lt;p&gt;Since we haven&amp;rsquo;t yet chosen a density function, we choose $\phi$ to make the computation of support easier. Specifically, we choose $\phi$ to be the inverse Fourier transform of $\mathbb{1}\{x\in B\}$, which is the indicator function of a unit Euclidean ball. Then, $\hat{\phi}$ becomes $\mathbb{1}\{x\in B\}$ itself, and its support is simply the ball $B$. Using these, we get&lt;/p&gt;

&lt;p&gt;$$ \text{Supp}(\hat{f\phi}) \subseteq T = \bigcup_{i=1}^k (\text{span}\{ \mathbf{v}_i \} + B ), $$&lt;/p&gt;

&lt;p&gt;which is basically the union of $k$ tubes passing through origin. This is because $\text{span}\{\mathbf{v}_i\}$ is just a straight line, and $B$ is a ball. Sum here means the direct sum, i.e., for every element $a \in A$ and $b \in B$, form a set of $a+b$. So we just put the Euclidean ball on every point on the line, which gives us a cylinder passing through the origin.&lt;/p&gt;

&lt;p&gt;Since we are looking for a $g$ which cannot be approximated by the neural network, we try to make $\lVert f\phi - g\phi \rVert_{L_2}^2$ as large as possible. We have already seen what the support of $\hat{f\phi}$ looks like. Now, we want a $g$ such that&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$g$ should have most of its mass away from the origin in all the directions, and&lt;/li&gt;
&lt;li&gt;the Fourier transform $\hat{g}$ should be outside $B$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If $g$ is chosen as a radial function, the first criteria will be satisfied if we just put large mass away from the origin in one direction. To satisfy the second criteria, $g$ should have a high-frequency component. To see why, see the following figure which shows the sine curve and its Fourier transform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/23/fourier.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The only thing that remains to be shown is that if $\hat{g}$ contains a significant mass away from the origin, then so does $\hat{g\phi}$. But this proof is somewhat technical in nature and I avoid it here for sake of simplicity.&lt;/p&gt;

&lt;p&gt;This completes our proof for the result given in the paper. While this result is an important step in quantifying the role of depth in a neural network, it is still limited in that it only holds for radial functions. This is what I meant earlier by &amp;ldquo;less natural&amp;rdquo; problems, since in most of the common learning problems, the $(x,y)$ pairs are not generated from a simple radial distribution, and are much more complex in nature.&lt;/p&gt;

&lt;h4 id=&#34;2-exponential-separation-between-shallow-and-deep-nets&#34;&gt;2. Exponential separation between shallow and deep nets&lt;/h4&gt;

&lt;p&gt;In the proof of the previous result, the key idea was to have a high-frequency component in the function required to be approximated. This means that the function was highly oscillatory. In this paper as well, a similar oscillation argument is used to prove another important result.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For every positive integer $k$, there exists neural networks with $\theta(k^3)$ layers, $\theta(1)$ nodes per layer, and $\theta(1)$ distinct parameters, which cannot be approximated by networks with $\mathcal{O}(k)$ layers and $o(2^k)$ nodes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This result is proven using three steps.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Functions with few oscillations poorly approximate functions with many oscillations.&lt;/li&gt;
&lt;li&gt;Functions computed by networks with few layers must have few oscillations.&lt;/li&gt;
&lt;li&gt;Functions computed by networks with many layers can have many oscillations.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;approximation-via-oscillation-counting&#34;&gt;Approximation via oscillation counting&lt;/h5&gt;

&lt;p&gt;We will first look at a metric to count oscillations of a function. For this, consider the following graph which shows functions $f$ and $g$ which are defined from $\mathbb{R}$ to $[0,1]$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/23/oscillations.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the horizontal line denotes $y = \frac{1}{2}$. The classifiers $\tilde{f}$ and $\tilde{g}$ obtained from $f$ and $g$ perform binary classification according to the rule $\tilde{f}(x) = \mathbb{1}[f(x)\geq \frac{1}{2}]$. Let $\mathcal{I}_f$ denote the set of partitions of $\mathbb{R}$ into intervals so that the classifier $\tilde{f}$ is constant in each interval. Then, the crossing number is defined as&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) = |\mathcal{I}_f|. $$&lt;/p&gt;

&lt;p&gt;From our definition of $\tilde{f}$, this clearly means that $\text{Cr}(f)$ counts the number of times that $f$ crosses the line $y = \frac{1}{2}$, and hence the name. In this way, we formalize the notion of counting the number of oscillations of a function.&lt;/p&gt;

&lt;p&gt;With this definition, if $\text{Cr}(f)$ is much larger than $\text{Cr}(g)$, then most piecewise constant regions of $\tilde{g}$ will exhibit many oscillations of $f$, and thus $g$ poorly approximates $f$.&lt;/p&gt;

&lt;p&gt;Now we will prove the following lemma, where the counting number $\text{Cr}(f)$ is denoted by $s_f$ for sake of convenience.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$$ \frac{\text{No. of regions of }\mathcal{I}_f \text{ where} ~ \tilde{f}\neq \tilde{g}}{s_f} \geq \frac{1}{2} - \frac{s_g}{s_f} $$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, if $s_f &amp;gt;&amp;gt; s_g$, then the RHS approximately becomes $\frac{1}{2}$, which implies that for more than half of all the regions of $f$, $\tilde{g}$ classifies $x$ incorrectly, and so $g$ is a poor approximation of $f$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We choose a region $J$ where $\tilde{g}$ is constant but $\tilde{f}$ alternates, such as the region where $g$ is red in the above figure. We denote by $X_J$ all the partitions of $\mathcal{I}_f$ that are contained in $J$. Since $f$ oscillates within $g$, this means that $\tilde{g}$ disagrees with $\tilde{f}$ for half of all $X_J$, i.e., at least $\frac{|X_J|-1}{2}$ in general.&lt;/p&gt;

&lt;p&gt;In the LHS of the claim, we need to count all the regions of $\mathcal{I}_f$ where the classifiers disagree for all points in the region. From above, we have a lower bound on the number of such regions within one $J$. So now we just take sum over all $J \in \mathcal{I}_g$ to get&lt;/p&gt;

&lt;p&gt;$$ \frac{\text{No. of regions of }\mathcal{I}_f \text{ where} ~ \tilde{f}\neq \tilde{g}}{s_f} \geq \frac{1}{s_f}\sum_{J \in \mathcal{I}_g} \frac{|X_J|-1}{2}. $$&lt;/p&gt;

&lt;p&gt;Now we need to bound $s_f$. For this, see that the total number of oscillations of $f$ are at least its number of oscillations within a single partition of $\mathcal{I}_g$ summed over all such partitions. I say &amp;ldquo;at least&amp;rdquo; because this will not include those partitions of $\mathcal{I}_f$ whose interior intersects with the boundary of an interval in $\mathcal{I}_g$. At most, there would be $s_g$ such partitions, and so&lt;/p&gt;

&lt;p&gt;$$ s_f \leq s_g + \sum_{J\in \mathcal{I}_g}|X_J|. $$&lt;/p&gt;

&lt;p&gt;This means that $\sum_{J\in \mathcal{I}_g}|X_J| \geq s_f - s_g$. Using this bound in the previously obtained inequality, we get the desired result.&lt;/p&gt;

&lt;h5 id=&#34;few-layers-few-oscillations&#34;&gt;Few layers, few oscillations&lt;/h5&gt;

&lt;p&gt;Adding more nodes is similar to adding polynomials, while adding layers is like composition of polynomials. Adding polynomials yields a new polynomial with degree equal to the higher of the two and at most twice as many terms, but composing them (i.e. taking product) would yield a polynomial with higher degree and more than the product of terms. Clearly, composition would lead to more number of roots of the new polynomial. This suggests that adding layers should lead to a higher number of oscillations than adding nodes.&lt;/p&gt;

&lt;p&gt;Let $f$ be a function computed by the neural network $\mathcal{N}((m_i,t_i,\alpha_i,\beta_i)_{i=1}^l)$, i.e. a network of $l$ layers where the $i$th layer has $m_i$ nodes, such that the activation function at each node is $(t,\alpha)-poly$ (a piecewise function containing $t$ parts where each piece is a polynomial of degree at most $\alpha$). Then, we claim that&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) \leq \mathcal{O}\left( \left( \frac{tm\alpha}{l} \right)^l \beta^{l^2} \right). $$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We will prove this in two parts. First, we bound the counting number of a $(t,\alpha)-poly$ function, and then we will show that the function $f$ as computed by the above network is $(t,\alpha)-poly$.&lt;/p&gt;

&lt;p&gt;For the first part, see that each piece of the function $f$ is a polynomial of degree at most $\alpha$, which means that each piece oscillates at most $1 + \alpha$ times. Since there are $t$ such pieces&lt;/p&gt;

&lt;p&gt;$$ \text{Cr}(f) \leq t(1+\alpha). $$&lt;/p&gt;

&lt;p&gt;Now it remains to show that the function $f$ computed by the network is indeed $(t,\alpha)-poly$. To see this, consider the function computed by a single layer. Each node in the layer computes a $(t,\alpha)-poly$ function, say $g_i$, and we apply a composition function, say $f$, on these $g_i$&amp;rsquo;s, which is a polynomial with degree at most $\gamma$. The final function computed by this layer is&lt;/p&gt;

&lt;p&gt;$$ h(x) = f(g_1(x),\ldots,g_k(x)). $$&lt;/p&gt;

&lt;p&gt;To visualize such a composition, consider the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/23/poly.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here, each horizontal line denotes one node&amp;rsquo;s partition function, i.e., $\tilde{g_i}$. There are $k$ such lines with at most $t$ intervals each. The composition takes the union of all the partitions of all these lines. As such, the maximum number of intervals after composition will be equal to $kt$. Within each such interval, since we are taking a composition of a degree $\gamma$ polynomial with one with degree at most $\alpha$, the resulting polynomial has degree at most $\alpha \gamma$. Hence, $h$ is $(tk,\alpha\gamma)-poly$.&lt;/p&gt;

&lt;p&gt;Since there are $l$ layers and the total number of nodes in the network is $m$, it implies there are $\frac{m}{l}$ nodes on average in each layer, and each node has at most $t$ intervals. So after every layer, the number of intervals gets multiplied by a factor of $\frac{mt}{l}$. Finally, the total number of intervals will be of the order $\left(\frac{mt}{l}\right)^l$.&lt;/p&gt;

&lt;p&gt;Similarly, the degree of resulting function gets multiplied by $\alpha$ after every layer, so the final degree is of the order $\alpha^l$. Using the result shown in the first part, the resulting function will have a counting number bounded by $\mathcal{O}\left(\frac{tm\alpha}{l} \right)^l$.&lt;/p&gt;

&lt;p&gt;The $\beta$ term comes due to technicalities associated with taking an activation function which is semi-algebraic rather than piecewise polynomial, but the proof technique remains the same.&lt;/p&gt;

&lt;h5 id=&#34;many-layers-many-oscillations&#34;&gt;Many layers, many oscillations&lt;/h5&gt;

&lt;p&gt;In the figure that I showed for explaining counting number, notice that oscillations usually (always?) mean repetitions of a triangle-like function (strictly increasing till some point and then strictly decreasing thereafter). Also, the usual functions computed by a single layer of most of the common neural networks are like these triangular functions.&lt;/p&gt;

&lt;p&gt;In the last result, we used the composition of $(t,\alpha)-poly$ functions across several layers to bound the counting number of a network. Similarly in this section, we will use the concept of a $(t,[a,b])$-triangle. It represents a function which is continuous in $[a,b]$ and consists of $t$ triangle-like pieces. Also, since this function oscillates $2t$ times, its counting number is $2t+1$.&lt;/p&gt;

&lt;p&gt;Now it remains to show that the composition of 2 such functions gives a similar function (which is a similar technique to what we used earlier). More formally, we will prove this claim.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; If $f$ is a $(s,[0,1])$-triangle and $g$ is a $(t,[0,1])$-triangle, then $f \circ g$ is a $(2st,[0,1])$-triangle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First, we note that $f \circ g$ is continuous in $[0,1]$ since a composition of continuous functions is continuous in the same domain.&lt;/p&gt;

&lt;p&gt;Now, consider any odd (i.e., strictly increasing) interval $g_j$ of $g$. Suppose $(a_1,\ldots,a_{2s+1})$ are the interval boundaries of $f$. Since the range of $g_j$ is $[0,1]$, $g_j^{-1}(a_i)$ exists for all $i$ and is unique, since $g_j$ is strictly increasing. Let $a_i^{\prime}=g_j^{-1}(a_i)$, i.e., $g_j(a_i^{\prime})=a_i$. If $i$ is odd, the composition $f \circ g_j(a_i^{\prime}) = f(a_i)=0$, and $f \circ g_j$ is strictly increasing in $[a_i^{\prime},a_{i+1}^{\prime}]$, since $g_j$ is strictly increasing everywhere and $f$ is strictly increasing in $[a_i,a_{i+1}]$. By a similar argument, if $i$ is even, $f \circ g_j$ is strictly decreasing along $[a_i^{\prime},a_{i+1}^{\prime}]$. In this way, we get $2s$ triangular pieces for a single $g_j$, and so the overall composition $f \circ g$ has $2st$ triangular pieces.&lt;/p&gt;

&lt;p&gt;Having shown this, it is easy to see that if there are $l$ layers and each layer computes a $(t,[0,1])$-triangle, the final layer will output a $((2t)^l,[0,1])$-triangle. In this way, the counting number of the overall function becomes $(2t)^l + 1$.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;implicit-acceleration-by-overparametrization&#34;&gt;Implicit acceleration by overparametrization&lt;/h3&gt;

&lt;p&gt;In the previous section, we have seen some results which show that depth plays a role in the expressive capacity of neural networks. Specifically, we saw that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Radial functions can be approximated by depth-3 networks but not with depth-2 networks.&lt;/li&gt;
&lt;li&gt;Functions expressible by $\theta(k^3)$-depth networks of constant width cannot be approximated by $\mathcal{O}(k)$-depth networks with polynomial width.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this section, we will look at a &lt;a href=&#34;https://arxiv.org/pdf/1802.06509.pdf&#34; target=&#34;_blank&#34;&gt;new paper from Arora, Cohen, and Hazan&lt;/a&gt; that suggests that, sometimes, increasing depth can speed up optimization (which is rather counterintuitive given the consensus on expressiveness vs. optimization trade-off), i.e., depth plays some role in convergence. Furthermore, this acceleration is more than what could be obtained by commonly used techniques, and is theoretically shown to be a combination of momentum and adaptive regularization (which we will discuss later).&lt;/p&gt;

&lt;p&gt;To isloate convergence from expressiveness, the authors focus solely on linear neural networks, where increasing depth has no impact on the expressiveness of the network. This is because in such networks, adding layers manifests itself only in the replacement of a matrix parameter by a product of matrices ‚Äì an
overparameterization.&lt;/p&gt;

&lt;h4 id=&#34;equivalence-to-adaptive-learning-rate-and-momentum&#34;&gt;Equivalence to adaptive learning rate and momentum&lt;/h4&gt;

&lt;p&gt;The first result that we prove is the following.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Overparametrized gradient descent with small learning rate and near-zero initialization is equivalent to GD with adaptive learning rate and momentum terms.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; This can be seen by simple analysis of gradients for an $l_p$-regression with parameter $\mathbf{w}\in \mathbb{R}^d$. The loss function can be given as&lt;/p&gt;

&lt;p&gt;$$ L(\mathbf{w}) = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ \frac{1}{p}(\mathbf{x}^T\mathbf{w} - y)^p \right]. $$&lt;/p&gt;

&lt;p&gt;Now, if we add a scalar parameter, the new parameters are $\mathbf{w}_1$ and $w_2 \in \mathbb{R}$, i.e., $\mathbf{w} = w_2 \mathbf{w}_1$, and we can write the new loss function as&lt;/p&gt;

&lt;p&gt;$$ L(\mathbf{w}_1,w_2) = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ \frac{1}{p}(\mathbf{x}^T\mathbf{w}_1 w_2 - y)^p \right]. $$&lt;/p&gt;

&lt;p&gt;We can now compute the gradients of the objective with respect to the parameters as&lt;/p&gt;

&lt;p&gt;$$ \nabla_{\mathbf{w}} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w} - y)^{p-1}\mathbf{x} \right] $$&lt;/p&gt;

&lt;p&gt;$$ \nabla_{\mathbf{w}_1} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w}_1 w_2 - y)^{p-1}w_2\mathbf{x} \right] = w_2 \nabla_{\mathbf{w}} $$&lt;/p&gt;

&lt;p&gt;$$ \nabla_{w_2} = \mathbb{E}_{(\mathbf{x},y)\sim S}\left[ (\mathbf{x}^T\mathbf{w} - y)^{p-1}\mathbf{w}_1^T \mathbf{x} \right] $$&lt;/p&gt;

&lt;p&gt;The update rules for $\mathbf{w}_1$ and $w_2$ can be given as&lt;/p&gt;

&lt;p&gt;$$ \mathbf{w}_1^{(t+1)} = \mathbf{w}_1^{(t)} - \eta \nabla_{\mathbf{w}_1}^{(t)} \quad \text{and} \quad w_2^{(t+1)} = w_2^{(t)} - \eta \nabla_{w_2}^{(t)}, $$&lt;/p&gt;

&lt;p&gt;and the updated parameter $\mathbf{w}$ is&lt;/p&gt;

&lt;p&gt;$$ \begin{align} \mathbf{w}^{(t+1)} &amp;amp;= \mathbf{w}_1^{(t+1)} w_2^{(t)} \\\ &amp;amp;= \left( \mathbf{w}_1^{(t)} - \eta \nabla_{\mathbf{w}_1}^{(t)} \right) \left( w_2^{(t)} - \eta \nabla_{w_2}^{(t)} \right) \\\ &amp;amp;= \mathbf{w}_1^{(t)}w_2^{(t)} - \eta w_2^{(t)}\nabla_{\mathbf{w}_1^{(t)}} - \eta \nabla_{w_2^{(t)}}\mathbf{w}_1^{(t)} + \mathcal{O}(\eta^2) \\\ &amp;amp;= \mathbf{w}^{(t)} - \eta \left( w_2^{(t)} \right)^2 \nabla_{\mathbf{w}^{(t)}} -\eta \left( w_2^{(t)} \right)^{-1} \nabla_{w_2^{(t)}} \mathbf{w}^{(t)} + \mathcal{O}(\eta^2). \end{align}$$&lt;/p&gt;

&lt;p&gt;We can ignore $\mathcal{O}(\eta^2)$ since the learning rate is assumed to be low. Also, we take $\rho^{(t)} = \eta(w_2^{(t)})^2$ and $\gamma^{(t)}=\eta(w_2^{(t)})^{-1}\nabla_{w_2^{(t)}}$, so the update becomes&lt;/p&gt;

&lt;p&gt;$$ \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \rho^{(t)}\nabla_{\mathbf{w}^{(t)}} - \gamma^{(t)}\mathbf{w}^{(t)}. $$&lt;/p&gt;

&lt;p&gt;Since $\mathbf{w}$ is initialized near $0$, it is essentially a weighted combination of the past gradients at any given time, i.e., $\gamma^{(t)}\mathbf{w}^{(t)} = \sum_{\tau=1}^{t-1}\mu^{(t,\tau)}\nabla_{\mathbf{w}^{(\tau)}}$.&lt;/p&gt;

&lt;p&gt;This is similar to the momentum term in the popular momentum algorithm for optimization (see &lt;a href=&#34;https://desh2608.github.io/post/short-note-sgd-algorithms/&#34; target=&#34;_blank&#34;&gt;this earlier post&lt;/a&gt; for an overview), and the learning rate term $\rho^{(t)}$ is time-varying and adaptive.&lt;/p&gt;

&lt;h4 id=&#34;update-rule-for-end-to-end-matrix&#34;&gt;Update rule for end-to-end matrix&lt;/h4&gt;

&lt;p&gt;The next derivation is a little more involved, and I defer the reader to the actual paper for the detailed proof. I will give a brief outline here.&lt;/p&gt;

&lt;p&gt;Suppose we have a depth-$N$ linear network such that the weight matrices are given by $W_1,\ldots,W_N$. Let $W_e$ denote the final end-to-end update matrix. The authors use differential techniques to compute an update rule for $W_e$. For this, the important assumption is that $\eta^2 \approx 0$. When step sizes are taken to be small, trajectories of discrete optimization algorithms converge to smooth curves modeled by continuous-time differential equations.&lt;/p&gt;

&lt;p&gt;After obtaining such a differential equation, integration over the $N$ layers gives the derivative of $W_e$, which is then transformed back to the discrete update rule given as&lt;/p&gt;

&lt;p&gt;$$ W_e^{(t+1)} = (1 - \eta\lambda N)W_e^{(t)} - \eta \sum_{i=1}^N \left[ W_e^{(t)} (W_e^{(t)})^T \right]^{\frac{j-1}{N}} \frac{\partial L^1}{\partial W}(W_e^{(t)}) \cdot \left[ (W_e^{(t)})^T W_e^{(t)} \right]^{\frac{N-j}{N}}. $$&lt;/p&gt;

&lt;p&gt;Let us break down this expression. The first part is similar to a weight-decay term for a 1-layer update. The second part also has the derivative w.r.t parameters, but it is multiplied by some preconditioning terms. On further inspection of these terms, it is found that their eigenvalues and eigenvectors depend on the singular value decomposition of $W_e$. Qualitatively, this means that these multipliers favor the gradient along those directions that correspond to singular values whose presence in $W_e$ is stronger. If we assume, as is usually the case in deep learning, that the initialization was near 0, this means that these multipliers act similar to acceleration and push the gradient along the direction already taken by the optimization.&lt;/p&gt;

&lt;p&gt;For further reading, check out the author&amp;rsquo;s &lt;a href=&#34;http://www.offconvex.org/2018/03/02/acceleration-overparameterization/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; about the paper.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To summarize, we looked at three recent papers which prove results on the role of depth in expressibility and optimization of neural networks. People often think that working on the mathematics of deep learning would require complex group theory formalisms and difficult techniques in high-dimensional probability, but as we saw in the proofs of some of these results (especially in Telgarsky&amp;rsquo;s paper), a lot can be achieved using simple counting logic and concentration inequalities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Generalization</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-2/</link>
      <pubDate>Fri, 27 Jul 2018 13:45:11 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-2/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;https://desh2608.github.io/post/deep-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;Part 1&lt;/a&gt; of this series, based on the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;, we looked at several aspects of optimization of the nonconvex objective function that is a part of most deep learning models. In this article, we will turn our attention to another important aspect, namely generalization.&lt;/p&gt;

&lt;p&gt;A distinguishing feature of most modern deep learning architectures is that they generalize to test cases exceptionally well, even though the number of parameters is far greater than the number of training samples. VGG19, for instance, which has approximately 20 million weights to be tuned, gives $\sim 93\%$ classification accuracy on CIFAR-10, which has only 50000 training images. If you have studied statistical learning theory (see my &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;previous&lt;/a&gt; &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;blogs&lt;/a&gt; on the topic), this behavior is extremely counter-intuitive, and begs the question: &lt;em&gt;why don&amp;rsquo;t deep neural networks overfit even with small number of training samples?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we try to understand the reason, let us look at a popular folklore experiment that is described in &lt;a href=&#34;http://www.cs.princeton.edu/~rlivni/files/papers/LivnComputational.pdf&#34; target=&#34;_blank&#34;&gt;Livni et al &amp;lsquo;14&lt;/a&gt; related to over-specification.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For sufficiently over-specified networks, global optima are ubiquitous and in general computationally easy to find.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see this, we fix a depth-2 neural network (i.e. a network with 1 hidden layer) consisting of $n$ hidden nodes. We provide random inputs to the network and obtain their corresponding output. Now, take a randomly initialized neural network with the same architecture as the above, and train it using the input-output pairs obtained earlier. It is found that this is really difficult to achieve. However, if we take a large number of hidden nodes, the training becomes easier.&lt;/p&gt;

&lt;p&gt;Although this result has been known and verified empirically for some time, it remains to be proven theoretically. This is a striking example of the difficulty of proving generalization guarantees in deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;effective-capacity-of-learning&#34;&gt;Effective capacity of learning&lt;/h3&gt;

&lt;p&gt;The capacity of a learning model, in an abstract sense, means the complexity of training samples that it can fit. For instance, a quadratic regression has inherently more capacity than linear regression, but is also more prone to overfitting. Furthermore, the effective capacity can be thought of as analogous to the number of bits required to represent all possible states that the hypothesis class contains. For this reason, the capacity is approximately the log of the number of apriori functions in the hypothesis class.&lt;/p&gt;

&lt;p&gt;We will now see a general result that is true for learning models including deep neural networks.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; Test loss - training loss $\leq \sqrt{\frac{N}{m}}$, where $N$ is the effective capacity and $m$ is the number of training samples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First let us fix our neural network $\theta$ and its parameters. Suppose we take an i.i.d sample $S$ containing $m$ data points. Consider &lt;em&gt;Hoeffding&amp;rsquo;s inequality&lt;/em&gt;: If $x_1,\ldots,x_m$ are $m$ i.i.d samples of a random variable $X$ distributed by $P$, and $a\leq x_i \leq b$ for every $i$, then for a small postive non-zero value $\epsilon$:&lt;/p&gt;

&lt;p&gt;$$ P\left( \mathbb{E}_{X \sim P} - \frac{1}{m}\sum_{i=1}^m x_i \right) \leq 2\exp \left( \frac{-2m\epsilon^2}{(b-a)^2} \right) $$&lt;/p&gt;

&lt;p&gt;We can apply this inequality to our generalization probability, assuming that our errors are bounded between 0 and 1 (which is a reasonable assumption, as we can get that using a 0/1 loss function or by squashing any other loss between 0 and 1) and get for a single hypothesis $h$:&lt;/p&gt;

&lt;p&gt;$$ P(|R(h) - \hat{R}(h)| &amp;gt; \epsilon) \leq 2\exp (-2m\epsilon^2), $$&lt;/p&gt;

&lt;p&gt;where $R(h)$ denotes generalization error and $\hat{R}(h)$ denotes empirical error on the sample.&lt;/p&gt;

&lt;p&gt;However, this is not the true generalization bound. This is because we have first fixed out network and we are then choosing the sample i.i.d. However, in a real learning problem, we are given the sample $S$ and we have to learn the parameters to best fit this sample. Therefore, to obtain the actual generalization bound, we take the union bound over all possible neural net configurations $\mathcal{W}$. Now, equating the RHS with the confidence $\delta$, we get&lt;/p&gt;

&lt;p&gt;$$ \begin{align} &amp;amp; 2\mathcal{W}\exp(-2m\epsilon^2) \leq \delta \\\ \Rightarrow &amp;amp; -2m\epsilon^2 \leq \log \frac{\delta}{2\mathcal{W}} \\\ \Rightarrow &amp;amp; \epsilon \geq \sqrt{\frac{\log \frac{2\mathcal{W}}{\delta}}{2m}}, \end{align} $$&lt;/p&gt;

&lt;p&gt;which completes the proof.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In statistical learning theory, the most popular metrics for measuring the capacity of a model are Rademacher complexity and VC dimension, which I have explained in &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-2/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. I will quickly summarize them here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rademacher complexity:&lt;/strong&gt; It is a measure of how well the model can fit a random assignment of labels. Its mathematical formulation is:&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathcal{R}_S}(G) = \mathbb{E}_{\sigma}[\text{sup}_{g\in G}\frac{1}{m}\sigma_i g(z_i)] $$&lt;/p&gt;

&lt;p&gt;Essentially, it denotes an expectation of the best possible average correlation that the random labels have with any function present in the hypothesis class $G$. Therefore, a higher Rademacher complexity would imply that the function class $G$ is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class $G$ is, higher is the probability that it would have some $g$ which correlates well with random noise.&lt;/p&gt;

&lt;p&gt;The generalization error $R(h)$ can be written in terms of R.C. as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{R}_m(H) + \sqrt{\frac{\log \frac{1}{\delta}}{2m}}, $$&lt;/p&gt;

&lt;p&gt;where $\hat{R}(h)$ is the empirical error, $\delta$ is the confidence, and $m$ is the number of training samples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VC dimension:&lt;/strong&gt; It is the size of the largest set that can be fully shattered by $G$. By shattering, we mean that $G$ can classify the given set in all possible ways. As such, higher the VC-dimension, more is the capacity of the hypothesis class. We can bound the generalization error in terms of the VC-dimension of the hypothesis class as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{O}\left( \sqrt{\frac{\log(m/d)}{m/d}} \right) $$&lt;/p&gt;

&lt;p&gt;Although these metrics are well established in learning theory, they fail for deep neural networks since they are usually equally vacuous, i.e, the upper bound is greater than 1. This means that the bounds are so large that they are meaningless, since error can never exceed 1, and in practice the generalization error of the networks is many orders of magnitude less than these bounds.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;deep-networks-have-excess-capacity&#34;&gt;Deep networks have &amp;ldquo;excess capacity&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;As mentioned earlier, deep neural networks generalize surprisingly well despite having a huge number of parameters. They can be shown by the dotted red line (figure taken from tutorial slides) in the following popular figure which is often found in textbooks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/generalize.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Other learning models with a &amp;ldquo;high capacity&amp;rdquo; would follow the general trend and fail to generalize well, which may be evidence that somehow, the large number of parameters in deep networks is not necessarily translating to a high capacity. For a long time, it was believed that a combination of stochastic gradient descent and regularization eliminates the &amp;ldquo;excess capacity&amp;rdquo; of the neural network.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But this belief is wrong!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In their &lt;a href=&#34;https://arxiv.org/abs/1611.03530&#34; target=&#34;_blank&#34;&gt;ICLR &amp;lsquo;17 paper&lt;/a&gt; (which I have previously discussed in &lt;a href=&#34;https://desh2608.github.io/post/best-papers-at-iclr-17/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;), Zhang et. al., in a series of well-designed experiments, showed that deep networks do retain this excess capacity. From &lt;a href=&#34;http://www.offconvex.org/2017/12/08/generalization1/&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s blog post&lt;/a&gt; on the subject: &amp;ldquo;Their main experimental finding is that if you take a classic convnet architecture, say Alexnet, and train it on images with random labels, then you can still achieve very high accuracy on the training data. (Furthermore, usual regularization strategies, which are believed to promote better generalization, do not help much.) Needless to say, the trained net is subsequently unable to predict the (random) labels of still-unseen images, which means it doesn‚Äôt generalize.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/iclr-17.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An interesting (and provable) guarantee that the paper contains is the following theorem: &lt;em&gt;There exists a two-layer neural network with ReLU activations and $2n+d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&#34;https://arxiv.org/abs/1802.01396&#34; target=&#34;_blank&#34;&gt;related paper&lt;/a&gt; published recently, it was shown that the &amp;ldquo;excess capacity&amp;rdquo; is not just limited to deep networks, since even linear models possess this feature. Furthermore, when it comes to fitting noise, there are some interesting similarities between Laplacian kernel machines and ReLU networks. But before we get to that, I will briefly define Laplacian and Gaussian kernels. (For an overview of several kernel functions, check out &lt;a href=&#34;http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/&#34; target=&#34;_blank&#34;&gt;this article&lt;/a&gt;.)&lt;/p&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;kernel-methods&#34;&gt;Kernel Methods&lt;/h4&gt;

&lt;p&gt;Kernel methods map the data into higher-dimensional spaces, in the hope that in this higher-dimensional space the data could become more easily separated or better structured. However, when we talk about transforming data to a higher dimension, called a $z$-space, an actual transformation would involve paying computation costs. To avoid this, we need to look at what we actually want from the $z$-space.&lt;/p&gt;

&lt;p&gt;Support Vector Machines (SVMs), which are among the most popular kernel-based methods for classification, involve solving for the following Lagrangian.&lt;/p&gt;

&lt;p&gt;$$ \mathcal{L}(\alpha) = \sum_{n=1}^N \alpha_n - \frac{1}{2}\sum_{n=1}^N \sum_{m=1}^M y_n y_m \alpha_n \alpha_m z_n^T z_m $$&lt;/p&gt;

&lt;p&gt;under the constraints $\alpha_n \geq 0 \forall n$ and $\sum_{n=1}^N \alpha_n y_n = 0$. On solving this, we get the boundary as&lt;/p&gt;

&lt;p&gt;$$ g(x) = \text{sgn}(w^T z + b) $$&lt;/p&gt;

&lt;p&gt;where $w = \sum_{z_n \in SV} \alpha_n y_n z_n$.&lt;/p&gt;

&lt;p&gt;We can see from this that the only value we need from the $z$-space is the inner product $z^T z^{\prime}$. If we can show that obtaining this inner product is possible without actually going to the $z$-space, we are done.&lt;/p&gt;

&lt;p&gt;It turns out that this is indeed possible, and there are several such functions, known as &lt;strong&gt;kernel functions&lt;/strong&gt;, which can be written as the inner product in some space. The only constraint on the $z$-space is that it should exist. Interestingly, kernels such as the radial basis function (RBF) kernel exist in an $\infty$-dimensional space. Furthermore, in order for the problem to be convex and have a unique solution, it is important to select a positive semi-definite kernel, i.e., whose kernel matrix contain only non-negative eigenvalues. Such a kernel is said to obey &lt;a href=&#34;https://en.wikipedia.org/wiki/Mercer&#39;s_theorem&#34; target=&#34;_blank&#34;&gt;Mercer&amp;rsquo;s theorem&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we have some idea what kernels are, let us look at Laplacian and Gaussian kernels.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Laplacian kernel:&lt;/strong&gt; It is mathematically defined as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ K(x,y) = \exp \left( - \frac{\lVert x-y \rVert}{\sigma} \right). $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gaussian kernel:&lt;/strong&gt; Its mathematical formulation is&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ K(x,y) = \exp \left( - \frac{\lVert x-y \rVert^2}{2\sigma^2} \right). $$&lt;/p&gt;

&lt;p&gt;Both the Laplacian and Gaussian kernels are examples of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Radial_basis_function&#34; target=&#34;_blank&#34;&gt;radial basis function&lt;/a&gt; kernels. The difference lies only in the parameter $\sigma$. Since the Gaussian depends on the square of this parameter, it is more sensitive to changes in $\sigma$ than the Laplacian.&lt;/p&gt;

&lt;p&gt;The authors found in their empirical evaluations that Laplacian kernels were much more adept at fitting random labels than Gaussian kernels. This property may be attributed to the inherent non-smoothness of Laplacians as opposed to the Gaussians being smooth. This discontinuity in derivative is reminiscent of that for ReLU units, which, as we saw above, were found to fit random labels exceptionally well. As such, the conjecture is that the radial structure of the kernels, as opposed to the specifics of optimization, plays a key role in ensuring strong classification performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/laplace.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Another take-away from this paper is that they establish stronger bounds for classification performance of kernel methods. If understanding kernels can indeed lead to a better understanding of deep learning, then maybe these bounds will lead to tighter bounds for the effective capactity of deep neural networks.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;other-notions-of-generalizability&#34;&gt;Other notions of generalizability&lt;/h3&gt;

&lt;p&gt;We now look at 2 other concepts that seek to explain why deep neural networks generalize well: flat minima, and noise stability.&lt;/p&gt;

&lt;h4 id=&#34;flat-minima&#34;&gt;Flat minima&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/minima.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/899-simplifying-neural-nets-by-discovering-flat-minima.pdf&#34; target=&#34;_blank&#34;&gt;Hochreiter and Schmidhuber&lt;/a&gt; first conjectured that the flatness of the local minima found by the stochastic gradient descent may be an indicator of its generalization performance. Sharpness of a minimizer can be characterized by the magnitude of the eigenvalues of $\nabla^2 f(x)$, but since the computation of this quantity is expensive, &lt;a href=&#34;https://arxiv.org/pdf/1609.04836.pdf&#34; target=&#34;_blank&#34;&gt;Keskar et. al.&lt;/a&gt; defined a new metric for sharpness that is easier to compute.&lt;/p&gt;

&lt;p&gt;Given $x \in \mathbb{R}^n$, $\epsilon &amp;gt; 0$, and $A \in \mathbb{R}^{n \times p}$, the $(C_{\epsilon},A)$-sharpness of $f$ at $x$ is defined as&lt;/p&gt;

&lt;p&gt;$$ \phi_{x,f}(\epsilon,A) = \frac{(\max_{y\in C_{\epsilon}} f(x+Ay))-f(x)}{1+f(x)}\times 100 $$&lt;/p&gt;

&lt;p&gt;The metric is based on exploring a small neighborhood of a solution and computing the largest value that $f$ can attain in that neighborhood. We use that value to measure the sensitivity of the training function at the given local minimizer.&lt;/p&gt;

&lt;p&gt;Intuitively, flat minima have lower description lengths (since less information is required to represent a flat surface), and consequently, fewer number of models are possible with this length. The effective capactiy thus becomes less, and so the hypothesis is able to generalize well.&lt;/p&gt;

&lt;p&gt;However, &lt;a href=&#34;https://arxiv.org/abs/1703.04933&#34; target=&#34;_blank&#34;&gt;recent research&lt;/a&gt; suggests that flatness is sensitive to reparametrizations of the neural network: we can reparametrize a neural network without changing its outputs while making sharp minima look arbitrarily flat and vice versa. As a consequence the flatness alone cannot explain or predict good generalization.&lt;/p&gt;

&lt;p&gt;As Prof. Arora pointed out in his talk, most of the existing theory that tries to explain generalization is only doing a &amp;ldquo;postmortem analysis&amp;rdquo;. This means that they look at some property $\phi$ that is seemingly possessed by a few neural networks that generalize well, and they argue that the generalization is due to this property. The notion of &amp;ldquo;flat minima&amp;rdquo; is a prime example of this. However, &lt;em&gt;correlation is not causation.&lt;/em&gt; Instead of such a qualitative check, the theoretical approach would be to use the property $\phi$ to compute an upper bound on the number of possible neural networks that would generalize well with this property. This computation is very nontrivial and is therefore ignored.&lt;/p&gt;

&lt;h4 id=&#34;noise-stability&#34;&gt;Noise stability&lt;/h4&gt;

&lt;p&gt;While flat minima was an old concept, the notion of noise stability is a very recent formalization for the same, proposed in &lt;a href=&#34;https://arxiv.org/abs/1802.05296&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s ICML&amp;rsquo;18 paper&lt;/a&gt;. Essentially, it means that if we add some zero-mean Gaussian noise at an intermediate output of a neural network, the noise gets attenuated as the signal moves to higher layers. Therefore, the capacity of a network to fit random noise can be measured by adding a Gaussian noise at an intermediate layer and measuring the change in output at higher layers.&lt;/p&gt;

&lt;p&gt;This is also biologically inspired, since neurologists believe that single neurons are extremely susceptible to errors. However, the fact that we still function well suggests that there must be some mechanism to attenuate these errors.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Noise stability implies compressibility.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, what is meant by compression of a neural network? Given a network $C$ with $N$ parameters and some training loss, compression means obtaining a new network $C^{\prime}$ containing $N^{\prime}$ parameters ($N^{\prime} &amp;lt; N$), such that the training loss effectively remains the same. From the generalization claim proved earlier, this compression would mean better generalization capability for the network $C^{\prime}$.&lt;/p&gt;

&lt;p&gt;Now, let us consider a depth-2 network consisting only of linear transformations. This network can be represented by some matrix $M$, which transforms input $x$ to output $Mx$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/compression.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the above figure, $\eta$ is a zero-mean Gaussian noise that is added to the input. We say that the matrix $M$ is noise stable, i.e. $M(x+\eta)\approx Mx$. This means that $\frac{|Mx|}{|x|} &amp;gt;&amp;gt; \frac{|M\eta|}{|\eta|}$. Here, the value $\frac{|Mx|}{|x|}$ is at most equal to the largest singular value of $M$, which we denote by $\sigma_{\max}(M)$. The RHS is approximately $\frac{(\sum_i (\sigma_i (M))^2)^{\frac{1}{2}}}{\sqrt{n}}$ where $\sigma_i(M)$ is the $i$th singular value of $M$ and $n$ is dimension of $Mx$. The reason is that gaussian noise divides itself evenly across all directions, with variance in each direction $1/n$. Thus,&lt;/p&gt;

&lt;p&gt;$$ (\sigma_{max}(M))^2 \gg \frac{1}{h} \sum_i (\sigma_i(M)^2) $$&lt;/p&gt;

&lt;p&gt;The ratio of the LHS to the RHS in the above inequality is known as the &lt;em&gt;stable rank&lt;/em&gt;. Higher the stable rank, more uneven is the distribution of singular values in the matrix. This is easily seen since the highest singular value is much larger than the RMS of all the singular values, something similar to the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/22/singular.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The actual signal $x$ is usually correlated with the eigenvectors corresponding to the larger singular values, and as such, the other directions can be ignored without any loss in performance. This is similar to feature selection by a principal component analysis approach.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;nonvacuous-bounds-for-true-capacity&#34;&gt;Nonvacuous bounds for true capacity&lt;/h3&gt;

&lt;p&gt;We have earlier seen that most of the classical metrics used for bounding the generalization error in learning systems prove to be vacuous in case of deep neural networks. The following blog posts by Prof. Arora discuss this issue in some detail and also introduce a new generalization bound based on the compressibility of neural networks explained in the previous section.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2017/12/08/generalization1/&#34; target=&#34;_blank&#34;&gt;Generalization theory and deep nets, an introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2018/02/17/generalization2/&#34; target=&#34;_blank&#34;&gt;Proving generalization of deep nets via compression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this section, I will discuss two approaches for computing nonvacuous bounds for deep networks. The first is from &lt;a href=&#34;https://arxiv.org/pdf/1703.11008.pdf&#34; target=&#34;_blank&#34;&gt;Dziugaite and Roy&lt;/a&gt;, and the second is from &lt;a href=&#34;https://arxiv.org/pdf/1802.05296.pdf&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s ICML&amp;rsquo;18 paper&lt;/a&gt; mentioned previously.&lt;/p&gt;

&lt;p&gt;As discussed earlier, a common framework for addressing this problem would involve showing under certain assumptions that either SGD performs implicit regularization, or that it finds a solution with some known structure connected to regularization. Once this is found, a nonvacuous bound for the generalization error of such models would have to be determined.&lt;/p&gt;

&lt;h4 id=&#34;1-pac-bayes-approach&#34;&gt;1. PAC-Bayes approach&lt;/h4&gt;

&lt;p&gt;The first question is how to identify structure in the solutions found by SGD? For this, we again turn to the old notion of flat minima. If SGD finds a flat minima, it means that the solution is surrounded by a large volume of solutions that are nearly as good. If we then represent these nearby solutions by some distribution and pick an average classifier from this distribution, it would be very likely that its generalization error is very close to that of the true solution.&lt;/p&gt;

&lt;p&gt;This concept is very similar to the PAC-Bayes theorem, which informally bounds the expected error of a classifier chosen from a distribution $Q$ in terms of its KL divergence from a priori fixed distribution $P$. But first, &lt;em&gt;what is KL divergence?&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;h5 id=&#34;kullback-leibler-divergence&#34;&gt;Kullback-Leibler divergence&lt;/h5&gt;

&lt;p&gt;It is a metric that compares the similarity between two probability distributions. Mathematically, it is the expectation of the log difference between the probability of data in the original distribution $p$ and the approximating distribution $q$.&lt;/p&gt;

&lt;p&gt;$$ \begin{align} KL(p||q) &amp;amp;= \mathbb{E}(\log p(x) - \log q(x)) \\\ &amp;amp;= \sum_{i=1}^N p(x_i)(\log p(x_i) - \log q(x_i)) \end{align}$$&lt;/p&gt;

&lt;p&gt;In information theory, the most important notion is that of &lt;strong&gt;entropy&lt;/strong&gt;, which represents the minimum number of bits required to encode some information, and is mathematically represented as&lt;/p&gt;

&lt;p&gt;$$ H = -\sum_{i=1}^N p(x_i)\log p(x_i). $$&lt;/p&gt;

&lt;p&gt;As such, the KL divergence can be seen to compute how many bits of information will be lost in approximating a distribution $p$ with another distribution $q$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The PAC-Bayes bound is given as&lt;/p&gt;

&lt;p&gt;$$ KL(\hat{e}(Q,S_m)||e(Q)) \leq \frac{KL(Q||P)+\log \frac{m}{\delta}}{m-1}, $$&lt;/p&gt;

&lt;p&gt;where $\hat{e}(Q,S_m)$ is the empirical loss of $Q$ w.r.t some i.i.d sample $S_m$, and $e(Q)$ is the expected loss. If we now find a $Q$ that minimizes this value, we are likely to find a minima that generalizes well and has a nonvacuous bound. This is exactly what is proposed in the paper.&lt;/p&gt;

&lt;p&gt;On a binary variant of MNIST, the computed PAC-Bayes bounds on the test error are in the range 16-22%. While this is a loose bound (actual bounds are around 3%), it is still surprising to find a non-trivial numerical bound for a model with such a large capacity on so few training examples. The authors comment that these are, in all likelihood, &amp;ldquo;the first explicit and nonvacuous numerical bounds computed for trained neural networks in the deep learning regime&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;2-compressibility-approach&#34;&gt;2. Compressibility approach&lt;/h4&gt;

&lt;p&gt;Although the PAC-Bayes bound is nonvacuous, it is still looser than actual sample complexity bounds computed empirically. Instead, Arora et al. introduce a new &lt;em&gt;compression framework&lt;/em&gt; to address this problem. Earlier while discussing noise stability, we have already seen that if we can compress a classifier $f$ without decreasing the empirical loss, it becomes much more generalizable according to the fundamental theorem proved earlier.&lt;/p&gt;

&lt;p&gt;We say that $f$ is $(\gamma,S)$-compressible using helper string $s$ if there exists some other classifier $g_{A,s}$ on a class of parameters $A$ such that the classification loss of $f$ on every $x \in S$ differs from that of $g_{A,s}$ by at most $\gamma$. Here, $s$ is fixed before looking at the training sample, and is often just for randomization.&lt;/p&gt;

&lt;p&gt;Then, the main theorem in the paper is as follows: If $f$ is $(\gamma,S)$-compressible using helper string $s$, then with high probability,&lt;/p&gt;

&lt;p&gt;$$ L_0 (g_A) \leq \hat{L}_{\gamma}(f) + \mathcal{O}\left( \sqrt{\frac{q \log r}{m}} \right), $$&lt;/p&gt;

&lt;p&gt;where $A$ is a set of $q$ parameters each having at most $r$ discrete values, $L_0 (g_A)$ is the generalization loss of compressed classifier, and $\hat{L}_{\gamma}(f)$ is the empirical estimate of the marginal loss of original classifier. Note that the bound is for the compressed classifier, but the same is also true for earlier works (like the PAC-Bayes approach). The proof is very elementary and uses just simple concentration inequalities.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; First, using Hoeffding&amp;rsquo;s inequality, we can write&lt;/p&gt;

&lt;p&gt;$$ P(L_0 (g_A) - \hat{L}_0 (g_A) \geq \epsilon) \leq 2\exp(-2m\epsilon^2). $$&lt;/p&gt;

&lt;p&gt;Taking $\epsilon = \sqrt{\frac{q \log r}{m}}$, we get, with probability at least $1 - \exp(-2q\log r)$,&lt;/p&gt;

&lt;p&gt;$$ L_0 (g_A) \leq \hat{L}_0 (g_A) + \mathcal{O}\left( \sqrt{\frac{q \log r}{m}} \right). $$&lt;/p&gt;

&lt;p&gt;Next, by definition of $(\gamma,S)$-compressibility, we can write&lt;/p&gt;

&lt;p&gt;$$ \lvert f(x)[y] - g_A(x)[y] \rvert \leq \gamma. $$&lt;/p&gt;

&lt;p&gt;This means that as long as the original function has margin at least $\gamma$, the new function classifies the example correctly. Therefore,&lt;/p&gt;

&lt;p&gt;$$ \hat{L}_0 (g_A) \leq \hat{L}_{\gamma}(f). $$&lt;/p&gt;

&lt;p&gt;Combining this with the earlier inequality, we immediately get the result.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to providing a tighter generalization bound for fully connected networks, the paper also proposes some theory for convolutional nets, which have been notoroiusly difficult to theorize. For details, readers are suggested to refer to the paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Optimization</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-1/</link>
      <pubDate>Thu, 26 Jul 2018 11:15:18 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-1/</guid>
      <description>

&lt;p&gt;I only just got around to watching the ICML 2018 tutorial on &amp;ldquo;&lt;a href=&#34;http://unsupervised.cs.princeton.edu/deeplearningtutorial.html&#34; target=&#34;_blank&#34;&gt;Toward a Theory for Deep Learning&lt;/a&gt;&amp;rdquo; by &lt;a href=&#34;https://www.cs.princeton.edu/~arora/&#34; target=&#34;_blank&#34;&gt;Prof. Sanjeev Arora&lt;/a&gt;. In this and the next few posts, I will discuss the subject in some detail, including the referenced papers and blogs. Very conveniently, the talk itself was divided into 5 parts, and I will structure this series accordingly.&lt;/p&gt;

&lt;p&gt;At the outset, we should understand that a number of important concepts in deep learning are already shaped by optimization theory. Backpropagation, for instance, is basically just a linear time dynamic programming algorithm to compute gradient. Recent methods for gradient descent, such as momentum, Adagrad, etc. (see &lt;a href=&#34;https://desh2608.github.io/post/short-note-sgd-algorithms/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; for a quick overview) are obtained from convex optimization techniques. However, over the last decade, the deep learning community has come up with several models based on intuition mostly, that do not have any theoretical support yet.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The goal, then, is to find theorems that support these intuitions, leading to new insights and concepts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this first part of the series, we will try to understand why (and how) deep learning almost always finds decent solutions to problems that are highly nonconvex.&lt;/p&gt;

&lt;h3 id=&#34;possible-goals-for-optimization&#34;&gt;Possible goals for optimization&lt;/h3&gt;

&lt;p&gt;Any neural network essentially tries to minimize a loss function. However, in almost all cases, this loss function is highly nonconvex (and sometimes NP-hard), which means that no provably polytime algorithm exists for its optimization. Even so, deep networks are quite adept at finding an approximately good solution.&lt;/p&gt;

&lt;p&gt;Whenever the gradient $\nabla$ is non-zero, there exists a descent direction. As such, a possible goal for the network may be any of the following (in increasing order of difficulty):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Finding a critical point, i.e. $\nabla = 0$.&lt;/li&gt;
&lt;li&gt;Finding a local optimum, i.e. $\nabla = 0$ and $\nabla^2$ is positive semi-definite.&lt;/li&gt;
&lt;li&gt;Finding a global optimum.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Furthermore, this descent may be from several possible initializations, namely all points, random points, or specially-chosen points. Now, if there are $d$ parameters (weights) to be optimized, we say that the problem is in $\mathbb{R}^d$ space. It is usually visualized by the following sea-urchin figure (or a $d$-urchin figure, according to Prof. Arora).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/21/high-dim-space.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In $\mathbb{R}^d$ space, there exit exp($d$) directions which can be explored to find the optimal solution, which makes the naive approach infeasible. Also, we cannot use non black box approaches to prune the number of explorations, since there is no clean mathematical formulation for the problem.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But what does this mean?&lt;/em&gt; This means that problems in deep learning are usually of the kind where, given pixels of an image, you have to label it as a cat or a dog. Such an $(x_i,y_i)$ has no mathematical meaning. This means that we do not understand the inherent landscape of the problem we are trying to solve, and so no special pruning can be done.&lt;/p&gt;

&lt;p&gt;This, combined with the nonconvex nature of the loss function, also means that it becomes infeasible to find a global optimum for the optimization problem. As such, we have to settle for goals 1 and 2, i.e. a critical point or a local optimum.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;finding-critical-points&#34;&gt;Finding critical points&lt;/h3&gt;

&lt;p&gt;The update function for a parameter $\theta$ is given as&lt;/p&gt;

&lt;p&gt;$$ \theta_{t+1} = \theta_t - \eta \nabla f(\theta_t) $$&lt;/p&gt;

&lt;p&gt;If the second derivative $\nabla^2$ is high, $\nabla f(\theta_t)$ will vary a lot, and we may miss the actual critical point. To prevent this, it is advisable to take &lt;em&gt;small&lt;/em&gt; steps.&lt;/p&gt;

&lt;p&gt;But how do we quantify small? In other words, &lt;em&gt;how do we determine a good learning rate for the optimization problem&lt;/em&gt;? For this, we again look at $\nabla^2$, which will determine the smoothness of the function. Suppose there exists a $\beta$ such that the Hessian $-\beta I \leq \nabla^2 f(\theta) \leq \beta I$, where $I$ is the identity matrix. Essentially, a higher $\beta$ means that $\nabla^2$ varies more, and so the learning rate should be lower. From this understanding, we can prove the following claim.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Claim (&lt;a href=&#34;https://rd.springer.com/book/10.1007%2F978-1-4419-8853-9&#34; target=&#34;_blank&#34;&gt;Nesterov 1998&lt;/a&gt;):&lt;/strong&gt; If we choose $\eta = \frac{1}{2\beta}$, we can achieve $|\nabla f|&amp;lt;\epsilon$ in number of steps proportional to $\frac{\beta}{\epsilon^2}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; See the proof of Lemma 2.8 &lt;a href=&#34;https://ee227c.github.io/notes/ee227c-notes.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; (see Definition 2.7). So a single update reduces the function value by at least $\frac{\epsilon^2}{2\beta}$. Therefore, it would take $\mathcal{O}(\frac{\beta}{\epsilon^2})$ steps to arrive at a critical point.&lt;/p&gt;

&lt;h4 id=&#34;evading-saddle-points&#34;&gt;Evading saddle points&lt;/h4&gt;

&lt;p&gt;While we have a theoretical upper limit for the time taken for convergence at a critical point, this is still problematic since it may be a saddle point, i.e., the function value is minimum in $d-1$ directions but maximum in one direction. Such a surface literally looks like a saddle as follows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://desh2608.github.io/img/21/saddle-point.png&#34; alt=&#34;Saddle point&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An important question, then, is how to evade saddle points while looking for critical points. This question is explored in a series of papers and corresponding blog posts on &lt;a href=&#34;www.offconvex.org&#34; target=&#34;_blank&#34;&gt;Prof. Arora&amp;rsquo;s blog&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2016/03/22/saddlepoints/&#34; target=&#34;_blank&#34;&gt;Polynomial time guarantee for GD to escape saddle points&lt;/a&gt; (based on &lt;a href=&#34;http://proceedings.mlr.press/v40/Ge15.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2016/03/24/saddles-again/&#34; target=&#34;_blank&#34;&gt;Random initialization for asymptotically avoiding saddle points&lt;/a&gt; (based on &lt;a href=&#34;https://arxiv.org/pdf/1602.04915.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.offconvex.org/2017/07/19/saddle-efficiency/&#34; target=&#34;_blank&#34;&gt;Perturbing gradient descent&lt;/a&gt; (based on &lt;a href=&#34;https://arxiv.org/pdf/1703.00887.pdf&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here I will try to summarize these discussions in several bullet points.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Most learning problems have exponentially many saddle points.&lt;/em&gt; Learning problems usually involve searching for $k$ components, for example clustering, $k$-node hidden layer in a neural network, etc. Suppose $(x_1,x_2,\ldots,x_k)$ is an optimal solution. Then, $(x_2,x_1,\ldots,x_k)$ is also an optimal solution, but the mean of these is not an optimal solution. This suffices to show that the learning problem is nonconvex, since for a convex function, the average of optimal solutions is also optimal. Furthermore, we can keep swapping the $k$ components to obtain exponential optimal solutions. Saddle points lie on the paths joining these isolated solutions, and hence, are exponential in number themselves.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Hessians can be used to evade saddle points.&lt;/em&gt; Consider the second order Taylor expansion given below. If there exists a direction where $\frac{1}{2}(y-x)^T \nabla^2 f(x)(y-x)$ is significantly less than 0, then using this update rule can avoid saddle points. Such saddle points are called &amp;ldquo;strict,&amp;rdquo; and for these, methods such as trust region algorithms and cubic regularization can find the local optimum.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ f(y) = f(x) + &amp;lt;\nabla f(x), y-x&amp;gt; + \frac{1}{2}(y-x)^T \nabla^2 f(x)(y-x) $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Noisy gradient descent converges to local optimum in polynomial number of steps.&lt;/em&gt; Although the Hessian method provides a theoretical way to escape saddle points, the computation of $\nabla^2$ is still expensive. Suppose we put a ball on a saddle point. Then, giving it only a slight push will move it away from the saddle. This intuition leads to the notion of &amp;ldquo;noisy&amp;rdquo; GD, i.e., $y = x - \eta \nabla f(x) + \epsilon$, where $\epsilon$ is a zero-mean error, which is often cheaper to compute than the true gradient. The authors in also prove the theorem in &lt;a href=&#34;http://proceedings.mlr.press/v40/Ge15.pdf&#34; target=&#34;_blank&#34;&gt;the paper&lt;/a&gt;, but it is very non-trivial.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;It is hard to converge to a saddle point.&lt;/em&gt; Furthermore, a random initialization of GD will asymptotically converge to a local minimum, rather than other stationary points. In (2), &lt;a href=&#34;http://people.eecs.berkeley.edu/~brecht/&#34; target=&#34;_blank&#34;&gt;Ben Recht&lt;/a&gt; emphasized that &amp;ldquo;even simple algorithms like gradient descent with constant step sizes can‚Äôt converge to saddle points unless you try really hard.&amp;rdquo; To prove this, they use the Stable Manifold Theorem, taking $x^{\ast}$ to be an arbitrary saddle point and showing that this measure was &lt;em&gt;always&lt;/em&gt; zero.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;The Stable Manifold theorem is concerned with fixed point operations of the form $x^{(k+1)}=\psi(x^{(k)})$. It quantifies that the set of points that locally converge to a fixed point $x^{\ast}$ of such an iteration have measure zero whenever the Jacobian of $\psi$ at $x^{\ast}$ has eigenvalues bigger than 1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In fact, it has been &lt;a href=&#34;https://www.math.upenn.edu/~pemantle/papers/nonconvergence.pdf&#34; target=&#34;_blank&#34;&gt;shown long back&lt;/a&gt; that additive Gaussian noise is sufficient to prevent convergence to saddles, without even assuming the &amp;ldquo;strictness&amp;rdquo; criteria of (1).&lt;/p&gt;

&lt;p&gt;Now that it is clear that GD can avoid saddle points almost certainly, it remains to be seen whether it is &lt;em&gt;efficient&lt;/em&gt; in doing so. The paper (1), although it did show a poly-time convergence for the noisy GD, was still inefficient because its polynomial dependency on the dimension $n$ and the smallest eigenvalue of the Hessian are impractical. The paper (3) further improves this aspect of the problem.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;A perturbed form of GD, under an additional Hessian-Lipschitz condition, converges to a second-order stationary point in almost the same time required for GD to converge to a first-order stationary point.&lt;/em&gt; Furthermore, the dimensional dependence is only polynomial in $\log(d)$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finally, recent work definitely shows that &lt;em&gt;PGD is much better than GD with random initialization&lt;/em&gt;, since the latter can be slowed down by saddle points, taking exponential time to escape. This is because if there are a sequence of closely-spaced saddle points, GD gets closer to the later ones, and takes $e^i$ iterations to escape the $i^{th}$ saddle point. PGD, on the other hand, escapes each saddle point in a small number of steps regardless of history.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Although most learning problems have exponentially many saddle points, they are hard to converge to, and even random initializations can escape them. They take a long time for this escape though, which is why using perturbations is more efficient, and actually as efficient as GD for first-order stationary points. Therefore, using information from Hessians is not necessary to escape saddle points efficiently.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;second-order-methods-for-local-optimum&#34;&gt;Second-order methods for local optimum&lt;/h3&gt;

&lt;p&gt;Although we have established that Hessians are unnecessary for finding the local optimum, it would still be enlightening to look at some approaches for the same.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.03943.pdf&#34; target=&#34;_blank&#34;&gt;Agarwal et. al &amp;lsquo;17&lt;/a&gt; proposed LiSSA, or Linear (time) Stochastic Second-order Algorithm. The basic update rule is&lt;/p&gt;

&lt;p&gt;$$ x_{t+1} = x_t - \eta [\nabla^2 f(x)]^{-1}\nabla f(x), $$&lt;/p&gt;

&lt;p&gt;i.e. the gradient is scaled by the inverse of the Hessian, which intuitively makes sense as discussed earlier. Although backpropagation can compute the Hessian itself in linear time, we require the inverse. In this paper, the LiSSA algorithm uses the idea that $(\nabla^2)^{-1} = \sum_{i=1}^{\infty}(I - \nabla^2)^i$, but with finite truncation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.00756.pdf&#34; target=&#34;_blank&#34;&gt;Carmon et al. &amp;lsquo;17&lt;/a&gt; further improved upon the $\mathcal{O}(\frac{1}{\epsilon^2})$ guarantee provided by gradient descent for $\epsilon$-first-order convergence, without any need for Hessian computation. They use two competing techniques for this purpose. The first has already been discussed above:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If the problem is locally non-convex, the Hessian must have a negative eigenvalue. In this case, under the assumption that the Hessian is Lipschitz continuous, moving in the direction of the corresponding eigenvector must make progress on the objective.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second technique is more novel. They show that if the Hessian&amp;rsquo;s smallest eigenvalue is at least $-\gamma$, we can apply &lt;a href=&#34;https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf&#34; target=&#34;_blank&#34;&gt;proximal point techniques&lt;/a&gt; and accelerated gradient descent to a carefully constructed regularized problem to obtain a faster running time.&lt;/p&gt;

&lt;p&gt;While their approach is asymptotically faster than first-order methods, it is still empirically slower. Furthermore, it doesn&amp;rsquo;t seem to find better quality neural networks in practice.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;understanding-the-landscape-matrix-completion&#34;&gt;Understanding the landscape: Matrix completion&lt;/h3&gt;

&lt;p&gt;Very early on in this post, we established that in deep learning problems, the landscape is unknown, i.e. the problem does not have a meaningful mathematical formulation. In this vein, we now look at a &lt;a href=&#34;https://arxiv.org/pdf/1704.00708.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; that develops a new framework to capture the landscape. In particular, we will approach this problem in the context of matrix completion. (Interestingly, this paper is again from &lt;a href=&#34;https://users.cs.duke.edu/~rongge/index.html&#34; target=&#34;_blank&#34;&gt;Rong Ge&lt;/a&gt;, who first showed polytime convergence to local minimum for noisy GD.)&lt;/p&gt;

&lt;p&gt;But first, what is matrix completion. Matrix completion is a learning problem wherein the objective is to recover a low-rank matrix from partially observed entries. The mathematical formulation of the problem is:&lt;/p&gt;

&lt;p&gt;$$ \min_{X} \text(rank)(X) \quad \text{subject to} \quad X_{ij} = M_{ij} ~~ \forall i,j \in E $$&lt;/p&gt;

&lt;p&gt;where $E$ is the set of observed entries. Most approaches to solve this problem represent it in the form of the following nonconvex objective.&lt;/p&gt;

&lt;p&gt;$$ f(X) = \frac{1}{2}\sum_{i,j\in E}[M_{i,j}-(XX^T)_{i,j}]^2 +R(X) $$&lt;/p&gt;

&lt;p&gt;Here, $R(X)$ is a regularization term which ensures that no single row of $X$ becomes too large, otherwise most observed entries will be 0.&lt;/p&gt;

&lt;p&gt;Ge showed in &lt;a href=&#34;https://arxiv.org/pdf/1605.07272.pdf&#34; target=&#34;_blank&#34;&gt;an earlier paper&lt;/a&gt; that in case of matrix completion (others have shown the same result for other problems like tensor decomposition and dictionary learning), all local minima are also global minima.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For matrix completion, all local minima are also global minima.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the present paper, the authors proposed the new insight that for the case of the matrix completion objective as defined above, the function $f$ is quadratic in $X$, which means that its Hessian w.r.t $X$ is constant. Furthermore, any saddle point has at least one strictly negative eigenvalue in its Hessian. Together, these ensure that simple local search algorithms can find the desired low rank matrix from an arbitrary starting point in polynomial time with high probability.&lt;/p&gt;

&lt;p&gt;These advances, while mathematically involved, show that characterizing the various stationary points of the learning objective can be helpful in providing theoretical guarantees for learning algorithms. While I have avoided proof details for the several important theorems here, I will try to understand and explain them lucidly in some later post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 2</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-2/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:45 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-2/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://desh2608.github.io/post/intro-learning-theory-1/&#34; target=&#34;_blank&#34;&gt;first part&lt;/a&gt; of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used ‚Äî Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.&lt;/p&gt;

&lt;h4 id=&#34;rademacher-complexity&#34;&gt;Rademacher complexity&lt;/h4&gt;

&lt;p&gt;Given a family of functions, one of the ways to measure its complexity is to see how well it can fit a random assignment of labels. A more complex hypothesis set would be able to fit a random noise better, and vice versa. For this purpose, we define $m$ random variables $\sigma_i$, called Rademacher variables. We then define the &lt;em&gt;empirical&lt;/em&gt; Rademacher complexity as&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathcal{R}_S}(G) = \mathbb{E}_{\sigma}[\text{sup}_{g\in G}\frac{1}{m}\sigma_i g(z_i)] $$&lt;/p&gt;

&lt;p&gt;Here the summation term is essentially the inner product of the vector of noise (Rademacher variables) and the labels with some $g \in G$. Intuitively, this term can be taken to represent the correlation between the actual assignment and the random assignment. On taking the supremum over all $g \in G$, we are computing how well the function class $G$ correlates with random noise on $S$. The expectation of this term over all random noise distributions measures the average correlation.&lt;/p&gt;

&lt;p&gt;Therefore, a higher Rademacher complexity would imply that the function class $G$ is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class $G$ is, higher is the probability that it would have some $g$ which correlates well with random noise.&lt;/p&gt;

&lt;p&gt;However, this is just the empirical R.C. since we are computing the mean on the given sample set. The actual R.C. is obtained by taking the expectation of this value by sampling $S$ from a distribution $D$ consisting of sample sets of size $m$. Having thus defined the R.C., we can obtain an upper bound on the expected value of an error function $g$ taken from a family of functions $G$.&lt;/p&gt;

&lt;p&gt;$$ \mathbb{E}[g(z)] \leq \frac{1}{m} \sum_{i=1}^m g(z_i) + 2\mathcal{R}_m(G) + \sqrt{\frac{\log \frac{1}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;Note that if we take the first term on RHS to LHS, the LHS becomes the maximum difference between the empirical and general loss (function value if function is binary-valued). We have access to the empirical values, but not the expectation. So we take 2 sample sets A and B which differ at only 1 point, so that we can use the McDiarmid‚Äôs inequality.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The McDiarmid‚Äôs inequality bounds the probability that the actual mean and expected mean of a function differ by more than a fixed quantity, given that the function does not deviate by a large amount on perturbing a single element.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The actual proof then becomes simply manipulating the expectation and supremum using Jensen‚Äôs inequality (function of an expectation is at most expectation of the function, if the function itself is convex). I do not go into the details of the proof here since it is readily available.&lt;/p&gt;

&lt;p&gt;Till now, we have only computed the bounds on the expectation of the set of loss functions $G$. We actually need to compute bounds on the general loss on the hypothesis class $H$, which assigns binary values to given samples. For this, we use the following lemma which is simple to prove.&lt;/p&gt;

&lt;p&gt;$$ \hat{\mathcal{R}_S} (G) = \frac{1}{2}\hat{\mathcal{R}_{S_X}}(G) $$&lt;/p&gt;

&lt;p&gt;From this and the earlier result, we easily arrive at an upper bound on the generalization error of the hypothesis class in terms of its Rademacher complexity.&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{R}_m(H) + \sqrt{\frac{\log \frac{1}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;Here, computing the empirical loss is simple, but computing the R.C. for some hypothesis sets may be hard (since it is equivalent to an empirical risk minimization problem). Therefore, we need some complexity measures which are easier to compute.&lt;/p&gt;

&lt;h4 id=&#34;growth-function&#34;&gt;Growth function&lt;/h4&gt;

&lt;p&gt;The growth function of a hypothesis class $H$ for sample size $m$ denotes the number of distinct ways that $H$ can classify the sample. A more complex hypothesis class would be able to have a larger number of possible combinations for any sample size $m$. However, unlike R.C., this measure is purely combinatorial, and independent of the underlying distributions in $H$.&lt;/p&gt;

&lt;p&gt;The Rademacher complexity and the growth function are related by Massart‚Äôs lemma as&lt;/p&gt;

&lt;p&gt;$$ \mathcal{R}_m(G) \leq \sqrt{\frac{2\log \prod_G (m) }{m}} $$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Massart‚Äôs lemma bounds the expected correlation of a given vector taken from a set with a vector of random noise, in terms of the size of the set, dimensionality of the set, and the maximum L2-norm of the set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As soon as we see ‚Äúexpected correlation,‚Äù we should think of the Rademacher complexity. To introduce the growth function, we use the term for the size of the set, since it essentially denotes the size of set containing all possible assignments for a sample.&lt;/p&gt;

&lt;p&gt;Using this relation in the earlier obtained upper bound, we can bound the generalization error in terms of the growth function.&lt;/p&gt;

&lt;p&gt;Although it is a combinatorial quantity, the growth function still depends on the sample size $m$, and thus would require repeated calculations for all values $m&amp;gt;1$. Instead, we turn to the third and most popular complexity measure for hypothesis sets.&lt;/p&gt;

&lt;h4 id=&#34;vc-dimension&#34;&gt;VC-dimension&lt;/h4&gt;

&lt;p&gt;The VC-dimension of a hypothesis class is the size of the largest set that can be fully shattered by it. By shattering, we mean that $H$ can classify the given set in all possible ways. Formally,&lt;/p&gt;

&lt;p&gt;$$ VCdim(H) = \max{ m:\prod_H (m) = 2^m } $$&lt;/p&gt;

&lt;p&gt;It is important to understand 2 things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If $VCdim(H) = d$, then there exists a set of size $d$ that can be fully shattered. This does not mean that all sets of size $d$ or less are fully shattered by $H$.&lt;/li&gt;
&lt;li&gt;Also, in this case, no set of size greater than $d$ can ever be shattered by $H$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To relate VC-dimension with the growth function, we use the Sauer‚Äôs lemma:&lt;/p&gt;

&lt;p&gt;$$ \prod_H(m) \leq \sum_{i=0}^m {m\choose i} $$&lt;/p&gt;

&lt;p&gt;Here, the LHS, which is the growth function, represents the number of possible behaviors that $H$ can have on a set of size $m$. The RHS is the number of small subsets that are completely shattered by $H$. For a detailed proof, I highly recommend &lt;a href=&#34;https://www.youtube.com/watch?v=LHIwWeQhhk4&#34; target=&#34;_blank&#34;&gt;this lecture&lt;/a&gt; (Actually, I would highly recommend the entire course).&lt;/p&gt;

&lt;p&gt;Using some manipulations on the combinatorial, we arrive at&lt;/p&gt;

&lt;p&gt;$$ \prod_H(m) \leq  \left( \frac{em}{d} \right)^d = \mathcal{O}(m^d) $$&lt;/p&gt;

&lt;p&gt;Now we can use this relation with the earlier results to bound the generalization error in terms of the VC-dimension of the hypothesis class.&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \mathcal{O}\left( \sqrt{\frac{\log(m/d)}{m/d}} \right) $$&lt;/p&gt;

&lt;p&gt;where $m$ is the sample size and $d$ is the VC-dimension.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Here is a quick recap:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rademacher complexity ‚Äî ability to fit random labels (using correlation)&lt;/li&gt;
&lt;li&gt;Growth function ‚Äî number of distinct behaviors on $m$&lt;/li&gt;
&lt;li&gt;VC-dimension ‚Äî largest set size that can be fully shattered&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This blog post is loosely based on notes made from Chapter 3 ‚ÄúRademacher complexity and VC-Dimension‚Äù of &lt;em&gt;Foundations of Machine Learning.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 1</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-1/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:43 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-1/</guid>
      <description>

&lt;p&gt;One of the most significant take-aways from NIPS 2017 was the &lt;a href=&#34;https://syncedreview.com/2017/12/12/lecun-vs-rahimi-has-machine-learning-become-alchemy/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;alchemy&amp;rdquo; debate&lt;/a&gt; spearheaded by &lt;a href=&#34;https://www.linkedin.com/in/ali-rahimi-a85104/&#34; target=&#34;_blank&#34;&gt;Ali Rahimi&lt;/a&gt;. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.&lt;/p&gt;

&lt;p&gt;One of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set. For this, we require the hypothesis class $H$ to approximate the concept class $C$ which determines the labels for the distribution $D$. Since both $C$ and $D$ are unknown, we try to model $H$ based on the known sample set $S$ and its labels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generalization error:&lt;/strong&gt; The generalization error of a hypothesis $h$ is the expectation of the error on a sample $x$ picked from the distribution $D$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Empirical error:&lt;/strong&gt; This is the mean of the error of hypothesis $h$ on the sample $S$ of size $m$.&lt;/p&gt;

&lt;p&gt;Having defined the generalization error and empirical error thus, we can state the objective of learning as follows.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The objective of learning is to have the empirical error approximate the generalization error with high probability.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This kind of a learning framework is known as &lt;strong&gt;PAC-learning&lt;/strong&gt; (Probably Approximately Correct). Formally, a concept class $C$ is PAC-learnable if there is some algorithm A for which the generalization error on a sample $S$ derived from the distribution $D$ is very low (less than $\epsilon$) with high probability (greater than $1- \delta$). In other words, we can say that for a PAC-learnable class, the accuracy is high with good confidence.&lt;/p&gt;

&lt;h3 id=&#34;guarantees-for-finite-hypothesis-sets&#34;&gt;Guarantees for finite hypothesis sets&lt;/h3&gt;

&lt;p&gt;The PAC-learning framework provides strong guarantees for finite hypothesis sets (i.e., where the size of $H$ is finite). Again, this falls in two categories ‚Äî the consistent case, and the inconsistent case. A hypothesis class is said to be &lt;em&gt;consistent&lt;/em&gt; if it admits no error on the training sample, i.e., the training accuracy is 100%.&lt;/p&gt;

&lt;h4 id=&#34;consistent-hypothesis&#34;&gt;Consistent hypothesis&lt;/h4&gt;

&lt;p&gt;Let us consider a finite hypothesis set $H$. We want the generalization error to be less than some $\epsilon$, so we will take a consistent hypothesis $h \in H$, and bound the probability that its error is more than $\epsilon$, i.e., we are calculating the probability that there exists some $h \in H$, such that $h$ is consistent and its generalization error is more than $\epsilon$. This is simply the union of all $h \in H$ such that it follows the said constraints. By the union bound, this
probability will be less than the sum of the individual probabilities i.e.,&lt;/p&gt;

&lt;p&gt;$$ \sum_{h\in H}Pr[\hat{R}(h)=0 \wedge R(h) &amp;gt; \epsilon] $$&lt;/p&gt;

&lt;p&gt;From the definition of conditional probability, we can write&lt;/p&gt;

&lt;p&gt;$$ Pr(A \cap B) = Pr(A|B)Pr(B) \leq Pr(A|B) $$&lt;/p&gt;

&lt;p&gt;which bounds the required probability $P$ as&lt;/p&gt;

&lt;p&gt;$$ P \leq \sum_{h\in H} Pr[\hat{R}(h) =0| R(h) &amp;gt; \epsilon] $$&lt;/p&gt;

&lt;p&gt;The condition says that the expectation of error of $h$ on any sample is at least $\epsilon$, so it would correctly classify a sample with probability at most $1-\epsilon$. Hence, to correctly classify $m$ training samples with $|H|$ hypotheses, the total probability is given as&lt;/p&gt;

&lt;p&gt;$$ P \leq |H|(1-\epsilon)^m \leq |H|\exp(-m\epsilon) $$&lt;/p&gt;

&lt;p&gt;On setting the RHS of the inequality to $\delta$, we obtain the generalization bound of the finite, consistent hypothesis class as&lt;/p&gt;

&lt;p&gt;$$ R(h_S) \leq \frac{1}{m}\left( \log |H| + \log \frac{1}{\delta} \right) $$&lt;/p&gt;

&lt;p&gt;As expected, the generalization error decreases with a larger training set. However, to arrive at a consistent algorithm, we may have to increase the size of the hypothesis class, which results in an increase in generalization error.&lt;/p&gt;

&lt;h4 id=&#34;inconsistent-hypothesis&#34;&gt;Inconsistent hypothesis&lt;/h4&gt;

&lt;p&gt;In practical scenarios, it is very restrictive to always require a consistent hypothesis class to bound the generalization error. In this section, we look at a more general case where empirical error is non-zero. For this derivation, we use the &lt;strong&gt;Hoeffding‚Äôs inequality&lt;/strong&gt; which provides an upper bound on the probability that the mean of independent variables in an interval $[0,1]$ deviates from its expected value by more than a certain amount.&lt;/p&gt;

&lt;p&gt;$$ P(\bar{X} - \mathbb{E}\bar{X} \geq t) \leq \exp(-2nt^2) $$&lt;/p&gt;

&lt;p&gt;If we take the errors as the random variable, their mean is the empirical error and the expectation is the generalization error. We can then get an upper bound for the generalization error of a single hypothesis $h$ as&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \sqrt{\frac{\log \frac{2}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;However, this is still not the general case since the hypothesis $h$ returned by the learning algorithm is not fixed. Similar to the consistent case, we will try to obtain an upper bound on the generalization error for an inconsistent (but finite) hypothesis, i.e., we need to compute the probability that there exists some hypothesis $h \in H$ such that the generalization error of $h$ differs from its empirical error by a value greater than $\epsilon$. Again, using the union bound, we get&lt;/p&gt;

&lt;p&gt;$$ P \leq \sum_{h \in H}Pr[|\hat{R}(h)-R(h)|&amp;gt;\epsilon] $$&lt;/p&gt;

&lt;p&gt;Using the Hoeffdieng‚Äôs inequality, this becomes&lt;/p&gt;

&lt;p&gt;$$ P \leq 2|H|\exp(-2m\epsilon^2) $$&lt;/p&gt;

&lt;p&gt;Now equating the RHS with $\delta$, we can arrive at the result&lt;/p&gt;

&lt;p&gt;$$ R(h) \leq \hat{R}(h) + \sqrt{\frac{\log |H| + \log \frac{2}{\delta}}{2m}} $$&lt;/p&gt;

&lt;p&gt;Here it is interesting to note that for a fixed $|H|$, to attain the same guarantee as in the consistent case, a quadratically larger labeled sample is required. Let us now analyze the role of the size of hypothesis class. If we have a smaller $H$, the second term is reduced but the empirical error may increase, and vice versa. However, for the same empirical error, it is always better to go with the smaller hypothesis class, i.e., the famous &lt;em&gt;Occam‚Äôs Razor&lt;/em&gt; principle.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this article, we looked at some generalization bounds in case of a finite hypothesis, using the PAC learning framework. In the next part, I will discuss some measures for infinite hypotheses, namely the Rademacher complexity, growth function, and the VC dimension.&lt;/p&gt;

&lt;p&gt;This blog post is loosely based on notes made from Chapter 2 &amp;ldquo;The PAC Learning Framework&amp;rdquo; of &lt;em&gt;Foundations of Machine Learning&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
