<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learning theory on Desh Raj</title>
    <link>https://desh2608.github.io/tags/learning-theory/</link>
    <description>Recent content in Learning theory on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 30 Jul 2018 23:39:37 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/tags/learning-theory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Theory of Deep Learning: Generative Models</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-4/</link>
      <pubDate>Mon, 30 Jul 2018 23:39:37 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-4/</guid>
      <description>Till now, in this series based on the ICML 2018 tutorial on &amp;ldquo;Toward a Theory for Deep Learning&amp;rdquo; by Prof. Sanjeev Arora, we have limited our discussion to the theory of supervised discriminative neural models, i.e., those models which learn the conditional probability $P(y|x)$ from a set of given $(x_i,y_i)$ samples. In particular, we saw how deep networks find good solutions, why they generalize well despite being overparametrized, and what role depth plays in all of this.</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Role of Depth</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-3/</link>
      <pubDate>Sat, 28 Jul 2018 23:00:20 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-3/</guid>
      <description>In the previous posts of this series, we have looked at how stochastic gradient descent is able to find a good solution despite the nonconvex objective, and why overparametrized neural networks generalize so well. In this post, we will look at the titular property of deep networks, namely depth, and what role they play in the learning ability of the model.
An ideal result in this regard would be if we can show that there exists a class of natural learning problems (recall the idea of a &amp;ldquo;natural&amp;rdquo; problem from the first post) which cannot be solved with depth $d$ neural networks, but are solvable with at least one model of depth $d+1$.</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Generalization</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-2/</link>
      <pubDate>Fri, 27 Jul 2018 13:45:11 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-2/</guid>
      <description>In Part 1 of this series, based on the ICML 2018 tutorial on &amp;ldquo;Toward a Theory for Deep Learning&amp;rdquo; by Prof. Sanjeev Arora, we looked at several aspects of optimization of the nonconvex objective function that is a part of most deep learning models. In this article, we will turn our attention to another important aspect, namely generalization.
A distinguishing feature of most modern deep learning architectures is that they generalize to test cases exceptionally well, even though the number of parameters is far greater than the number of training samples.</description>
    </item>
    
    <item>
      <title>Theory of Deep Learning: Optimization</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-1/</link>
      <pubDate>Thu, 26 Jul 2018 11:15:18 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-1/</guid>
      <description>I only just got around to watching the ICML 2018 tutorial on &amp;ldquo;Toward a Theory for Deep Learning&amp;rdquo; by Prof. Sanjeev Arora. In this and the next few posts, I will discuss the subject in some detail, including the referenced papers and blogs. Very conveniently, the talk itself was divided into 5 parts, and I will structure this series accordingly.
At the outset, we should understand that a number of important concepts in deep learning are already shaped by optimization theory.</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 2</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-2/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:45 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-2/</guid>
      <description>In the first part of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used â€” Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 1</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-1/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:43 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-1/</guid>
      <description>One of the most significant take-aways from NIPS 2017 was the &amp;ldquo;alchemy&amp;rdquo; debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.
One of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set.</description>
    </item>
    
  </channel>
</rss>