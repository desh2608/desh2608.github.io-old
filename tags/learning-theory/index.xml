<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learning theory on Desh Raj</title>
    <link>https://desh2608.github.io/tags/learning-theory/</link>
    <description>Recent content in Learning theory on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 26 Jul 2018 11:15:18 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/tags/learning-theory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Theory of Deep Learning: Optimization</title>
      <link>https://desh2608.github.io/post/deep-learning-theory-1/</link>
      <pubDate>Thu, 26 Jul 2018 11:15:18 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-theory-1/</guid>
      <description>I only just got around to watching the ICML 2018 tutorial on &amp;ldquo;Toward a Theory for Deep Learning&amp;rdquo; by Prof. Sanjeev Arora. In this and the next few posts, I will discuss the subject in some detail, including the referenced papers and blogs. Very conveniently, the talk itself was divided into 5 parts, and I will structure this series accordingly.
At the outset, we should understand that a number of important concepts in deep learning are already shaped by optimization theory.</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 2</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-2/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:45 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-2/</guid>
      <description>In the first part of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used â€” Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 1</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-1/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:43 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-1/</guid>
      <description>One of the most significant take-aways from NIPS 2017 was the &amp;ldquo;alchemy&amp;rdquo; debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.
One of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set.</description>
    </item>
    
  </channel>
</rss>