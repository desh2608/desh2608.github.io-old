<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learning theory on Desh Raj</title>
    <link>https://desh2608.github.io/tags/learning-theory/</link>
    <description>Recent content in Learning theory on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 15 Jan 2018 13:39:45 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/tags/learning-theory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introduction to Learning Theory - Part 2</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-2/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:45 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-2/</guid>
      <description>In the first part of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used â€” Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 1</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-1/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:43 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-1/</guid>
      <description>One of the most significant take-aways from NIPS 2017 was the &amp;ldquo;alchemy&amp;rdquo; debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.
One of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set.</description>
    </item>
    
  </channel>
</rss>