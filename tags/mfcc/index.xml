<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mfcc on Desh Raj</title>
    <link>https://desh2608.github.io/tags/mfcc/</link>
    <description>Recent content in mfcc on Desh Raj</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Fri, 26 Jul 2019 13:18:50 -0400</lastBuildDate>
    
	    <atom:link href="https://desh2608.github.io/tags/mfcc/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A note on MFCCs and delta features</title>
      <link>https://desh2608.github.io/post/delta-feats/</link>
      <pubDate>Fri, 26 Jul 2019 13:18:50 -0400</pubDate>
      
      <guid>https://desh2608.github.io/post/delta-feats/</guid>
      <description>

&lt;p&gt;In this post, I will briefly describe the MFCC features for ASR systems, and the use of delta coefficients. I then talk about how this is approximated in &lt;a href=&#34;https://github.com/kaldi-asr/kaldi&#34; target=&#34;_blank&#34;&gt;Kaldi&lt;/a&gt; using an LDA-like transform, and finally mention some recent experimental results to replace the LDA with traditional delta features without losing out on performance in terms of WER.&lt;/p&gt;

&lt;h3 id=&#34;what-are-mfccs-and-how-are-they-computed&#34;&gt;What are MFCCs and how are they computed?&lt;/h3&gt;

&lt;p&gt;Feature extraction is the first step in any automatic speech recognition (ASR) pipeline. The objective is to compute features from speech waveforms which contain relevant information about the linguistic content of the speech, and ignore information about the background noise, emotions, etc. This problem has been extensively studied since early days in ASR research, and several feature extraction methods have been proposed. Among these, the most well-known and widely used are Mel Frequency Cepstral Coefficients (MFCCs).&lt;/p&gt;

&lt;p&gt;Since MFCCs are very well known, I will only briefly describe their computation in this post. Most of this is taken from &lt;a href=&#34;http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/&#34; target=&#34;_blank&#34;&gt;this blog&lt;/a&gt;, which explains them in some detail. The key steps for computing MFCCs are described below.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;First, the entire waveform is divided into shorter segments of 20-40 ms each. The assumption is that in this short segment, the signal is statistically stationary, and so features can be assumed to be constant inside this window. In Kaldi and most major ASR systems, windows are 25 ms in length and at 10 ms intervals apart, i.e., they are overlapping.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In order to recognize the frequencies present in this short segment, the power spectrum (or the &lt;a href=&#34;https://en.wikipedia.org/wiki/Periodogram&#34; target=&#34;_blank&#34;&gt;periodogram&lt;/a&gt; estimate) is computed. This is done using discrete-time Fourier transforms.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It is difficult to distinguish individual frequencies in the raw power spectrum, especially in the high frequency range. To solve this problem, the spectrum is convolved with several (20-40, in general) triangular Mel filters, called a filterbank. These filters are narrow at low frequency and get wider as frequency increases, in accordance with the human cochlea. Furthermore, a log transform is applied since humans don&amp;rsquo;t perceive loudness on a linear scale.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Since filterbank energies are correlated and cannot be used directly with a Gaussian mixture with diagonal covariance, we apply a &lt;a href=&#34;https://en.wikipedia.org/wiki/Discrete_cosine_transform&#34; target=&#34;_blank&#34;&gt;discrete cosine transform (DCT)&lt;/a&gt; to decorrelate them.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There is some debate in the community regarding the use of the DCT, instead of directly using the log Mel fiterbank features, particularly for deep neural network based acoustic models. Some research groups, like Google, use filterbanks (fbanks) while Kaldi mostly uses MFCCs, especially in its TDNN chain models. Here is &lt;a href=&#34;https://www.danielpovey.com/&#34; target=&#34;_blank&#34;&gt;Dan Povey&lt;/a&gt;&amp;rsquo;s take on this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The reason we use MFCC is because they are more easily compressible, being decorrelated; we dump them to disk
with compression to 1 byte per coefficient.  But we dump all the coefficients, so it&amp;rsquo;s equivalent to filterbanks times a full-rank matrix, no information is lost.&lt;/p&gt;

&lt;p&gt;(Source: &lt;a href=&#34;https://groups.google.com/forum/#!topic/kaldi-help/_7hB74HKhC4&#34; target=&#34;_blank&#34;&gt;kaldi-help&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;delta-and-delta-delta-features&#34;&gt;Delta and delta-delta features&lt;/h3&gt;

&lt;p&gt;The idea behind using delta (differential) and delta-delta (acceleration) coefficients is that in order to recognize speech better, we need to understand the dynamics of the power spectrum, i.e., the trajectories of MFCCs over time. The delta coeffients are computed using the following formula.&lt;/p&gt;

&lt;p&gt;$$ d_t = \frac{\sum_{n=1}^N n (c_{t+n} - c_{t-n})}{2 \sum_{n=1}^N n^2}, $$
where $d_t$ is a delta coefficient from frame $t$ computed in terms of the static coefficients $c_{t-n}$ to $c_{t+n}$. $n$ is usually taken to be 2. The acceleration coefficients are computed similarly, but using the differential instead of the static coefficients.&lt;/p&gt;

&lt;h3 id=&#34;the-lda-transform-in-kaldi&#34;&gt;The LDA transform in Kaldi&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;For a comprehensive reference on LDA, readers are advised to refer to &lt;a href=&#34;https://sebastianraschka.com/Articles/2014_python_lda.html#what-is-a-good-feature-subspace&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The latest TDNN-based chain models in Kaldi (see, for example, &lt;a href=&#34;https://github.com/kaldi-asr/kaldi/blob/06442e1870996486cb052fdd89d63aac44144b87/egs/wsj/s5/local/chain/tuning/run_tdnn_1g.sh#L188&#34; target=&#34;_blank&#34;&gt;this recipe&lt;/a&gt;) do not use differential and acceleration features (hereby refered to as &amp;ldquo;delta features&amp;rdquo; for convenience). Instead, they employ an LDA-like transformation which is essentially an affine transformation of the spliced input. Here is a sample from the xconfig of a typical Kaldi TDNN model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input dim=100 name=ivector
input dim=40 name=input
# please note that it is important to have input layer with the name=input
# as the layer immediately preceding the fixed-affine-layer to enable
# the use of short notation for the descriptor
fixed-affine-layer name=lda input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) affine-transform-file=$dir/configs/lda.mat
# the first splicing is moved before the lda layer, so no splicing here
relu-batchnorm-dropout-layer name=tdnn1 $tdnn_opts dim=1024
tdnnf-layer name=tdnnf2 $tdnnf_opts dim=1024 bottleneck-dim=128 time-stride=1
tdnnf-layer name=tdnnf3 $tdnnf_opts dim=1024 bottleneck-dim=128 time-stride=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This splicing can be over 1 or 2 frames on either side of the central frame, i.e. &lt;code&gt;Append(-1,0,1)&lt;/code&gt; or &lt;code&gt;Append(-2,-1,0,1,2)&lt;/code&gt;. Additionally, &lt;a href=&#34;https://ieeexplore.ieee.org/document/5545402&#34; target=&#34;_blank&#34;&gt;i-vectors&lt;/a&gt; are appended with the spliced input before the LDA. Although Kaldi itself has an &lt;a href=&#34;https://kaldi-asr.org/doc/transform.html#transform_lda&#34; target=&#34;_blank&#34;&gt;implementation of the LDA transform&lt;/a&gt; available, the transformation here simply multiplies the spliced input with a full-rank matrix. This is why this is called an &amp;ldquo;LDA-like&amp;rdquo;, and not an LDA transform.&lt;/p&gt;

&lt;h3 id=&#34;some-new-results&#34;&gt;Some new results&lt;/h3&gt;

&lt;p&gt;In some sense, this LDA-like transform is a generalization of using the delta features, since it can apply arbitrary scaling to each coefficient, and this matrix is learned in the training stage. However, this means having to additionally learn $(k \times n+d)^2$ parameters, where $k$ is the splicing window, $n$ is the MFCC size, and $d$ is the i-vector dimensionality. For typical values of $k$, $n$, and $d$, this is in the range of 50000 to 90000 parameters. While this is not a &amp;ldquo;huge&amp;rdquo; number compared to the size of modern deep networks (a typical TDNN model in Kaldi may have up to 10 million parameters), we would still like to see if this is disposable.&lt;/p&gt;

&lt;p&gt;I replaced the LDA transform with simple delta features. In the context of our input, the differential is simply $c_{t+1} - c_{t-1}$, and the acceleration is $c_{t-2} + c_{t+2} - 2\times c_t$. This is implemented using a new &lt;code&gt;xconfig&lt;/code&gt; layer called &lt;code&gt;delta-layer&lt;/code&gt; as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class XconfigDeltaLayer(XconfigLayerBase):
    &amp;quot;&amp;quot;&amp;quot;This class is for parsing lines like
     &#39;delta-layer name=delta input=idct&#39;
    which appends the central frame with the delta features
    (i.e. -1,0,1 since scale equals 1) and delta-delta features 
    (i.e. 1,0,-2,0,1), and then applies batchnorm to it.
    Parameters of the class, and their defaults:
      input=&#39;[-1]&#39;             [Descriptor giving the input of the layer]
      dim=120                  [Final dimension of combined features]
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, first_token, key_to_value, prev_names=None):
        XconfigLayerBase.__init__(self, first_token, key_to_value, prev_names)

    def set_default_configs(self):
        self.config = {&#39;input&#39;: &#39;[-1]&#39;}

    def check_configs(self):
        pass

    def output_name(self, auxiliary_output=None):
        assert auxiliary_output is None
        return self.name

    def output_dim(self, auxiliary_output=None):
        assert auxiliary_output is None
        input_dim = self.descriptors[&#39;input&#39;][&#39;dim&#39;]
        return (3*input_dim)

    def get_full_config(self):
        ans = []
        config_lines = self._generate_config()

        for line in config_lines:
            for config_name in [&#39;ref&#39;, &#39;final&#39;]:
                # we do not support user specified matrices in this layer
                # so &#39;ref&#39; and &#39;final&#39; configs are the same.
                ans.append((config_name, line))
        return ans

    def _generate_config(self):
        # by &#39;descriptor_final_string&#39; we mean a string that can appear in
        # config-files, i.e. it contains the &#39;final&#39; names of nodes.
        input_desc = self.descriptors[&#39;input&#39;][&#39;final-string&#39;]
        input_dim = self.descriptors[&#39;input&#39;][&#39;dim&#39;]
        output_dim = self.output_dim()

        configs = []
        line = (&#39;component name={0}_scale-1 type=NoOpComponent dim={1}&#39;.format(
            input_desc, input_dim))
        configs.append(line)
        line = (&#39;component-node name={0}_scale-1 component={0}_scale-1&#39;
            &#39; input=Scale(-1.0, {0})&#39;.format(input_desc))
        configs.append(line)
        line = (&#39;component name={0}_scale-2 type=NoOpComponent dim={1}&#39;.format(
            input_desc, input_dim))
        configs.append(line)
        line = (&#39;component-node name={0}_scale-2 component={0}_scale-2&#39;
            &#39; input=Scale(-2.0, {0})&#39;.format(input_desc))
        configs.append(line)

        line = (&#39;component name={0}_2 type=NoOpComponent dim={1}&#39;.format(
            input_desc, output_dim))
        configs.append(line)
        line = (&#39;component-node name={0}_2 component={0}_2 input=Append(Offset({0},0),&#39;
            &#39; Sum(Offset({0}_scale-1,-1), Offset({0},1)), Sum(Offset({0},-2), Offset({0},2),&#39; 
            &#39; Offset({0}_scale-2,0)))&#39;.format(input_desc))
        configs.append(line)

        line = (&#39;component name={0} type=BatchNormComponent dim={1}&#39;.format(
            self.name, output_dim))
        configs.append(line)
        line = (&#39;component-node name={0} component={0} input={1}_2&#39;.format(
            self.name, input_desc))
        configs.append(line)
        return configs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following are some experimental results on &lt;code&gt;mini_librispeech&lt;/code&gt;, &lt;code&gt;wsj&lt;/code&gt; (Wall Street Journal), and &lt;code&gt;swbd&lt;/code&gt; (Switchboard). The i-vector scale was reduced for &lt;code&gt;mini_librispeech&lt;/code&gt; since the delta features are computed on top of a &lt;a href=&#34;https://arxiv.org/abs/1904.08779&#34; target=&#34;_blank&#34;&gt;SpecAugment&lt;/a&gt; layer, which itself includes batch normalization. Therefore, using an i-vector scale of 1.0 would overpower the MFCCs.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Setup&lt;/th&gt;
&lt;th&gt;Test set&lt;/th&gt;
&lt;th&gt;IDCT&lt;/th&gt;
&lt;th&gt;SpecAugment&lt;/th&gt;
&lt;th&gt;i-vector scale&lt;/th&gt;
&lt;th&gt;LDA&lt;/th&gt;
&lt;th&gt;Delta&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;mini_librispeech&lt;/td&gt;
&lt;td&gt;dev_clean2&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;7.54&lt;/td&gt;
&lt;td&gt;7.66&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;wsj&lt;/td&gt;
&lt;td&gt;eval92&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;N&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;2.39&lt;/td&gt;
&lt;td&gt;2.41&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;swbd&lt;/td&gt;
&lt;td&gt;rt03&lt;/td&gt;
&lt;td&gt;N&lt;/td&gt;
&lt;td&gt;N&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;15.0&lt;/td&gt;
&lt;td&gt;15.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;These results are for a particular test set for these setups, and for a specific decoder, but the general trend of results is found to be the same across all test set and decoder combinations. Without significant loss in performance, we can eliminate the need of an LDA transform in the network. Work on a &lt;a href=&#34;https://github.com/kaldi-asr/kaldi/pull/3490/files&#34; target=&#34;_blank&#34;&gt;pull request&lt;/a&gt; for this setup is in progress.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
