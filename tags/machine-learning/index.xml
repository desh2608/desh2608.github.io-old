<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-learning on Desh Raj</title>
    <link>https://desh2608.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine-learning on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 23 Apr 2018 13:41:31 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An Introduction to Speech Recognition using WFSTs</title>
      <link>https://desh2608.github.io/post/intro-speech-recognition-wfst/</link>
      <pubDate>Mon, 23 Apr 2018 13:41:31 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-speech-recognition-wfst/</guid>
      <description>Until now, all of my blog posts have been about deep learning methods or their application to NLP. Since the last couple of weeks, however, I have started learning about Automatic Speech Recognition (ASR)1. Therefore, I will also include speech-related articles in this publication now.
The ASR logic is very simple (it’s just Bayes rule, like most other things in machine learning). Essentially, given a speech waveform, the objective is to transcribe it, i.</description>
    </item>
    
    <item>
      <title>Sparsity in Online Learning with Lasso Regularization</title>
      <link>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</link>
      <pubDate>Sat, 24 Feb 2018 13:40:42 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</guid>
      <description>Sparse vectors have become popular recently for 2 reasons:
 Sparse matrices require much less storage since they can be stored using various space-saving methods. Sparse vectors are much more interpretable than dense vectors. For instance, the non-zero non-negative components of a sparse word vector may be taken to denote the weights for certain features. In contrast, there is no interpretation for a value like $-0.1347$.  Sparsity is often induced through the use of L1 (or Lasso) regularization.</description>
    </item>
    
    <item>
      <title>A Short Note on Stochastic Gradient Descent Algorithms</title>
      <link>https://desh2608.github.io/post/short-note-sgd-algorithms/</link>
      <pubDate>Thu, 08 Feb 2018 13:40:25 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/short-note-sgd-algorithms/</guid>
      <description>I just finished reading Sebastian Ruder’s amazing article providing an overview of the most popular algorithms used for optimizing gradient descent. Here I’ll make very short notes on them primarily for purposes of recall.
Momentum The update vector consists of another term which has the previous update vector (weighted by $\gamma$). This helps it to move faster downhill — like a ball.
$$ v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta) $$</description>
    </item>
    
    <item>
      <title>Uncertain Fuzzy Self-organization based Clustering: Interval Type-2 Approach to Adaptive Resonance Theory</title>
      <link>https://desh2608.github.io/publication/infosc-17-art/</link>
      <pubDate>Mon, 15 Jan 2018 15:02:35 +0530</pubDate>
      
      <guid>https://desh2608.github.io/publication/infosc-17-art/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>