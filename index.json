[{"authors":["admin"],"categories":null,"content":"I am a second-year PhD student at Johns Hopkins University, working in the Center for Language and Speech Processing (CLSP), advised by Dan Povey and Sanjeev Khudanpur. My research interests are in applied machine learning, particularly deep learning methods for speech recognition and language modeling. I am currently working on speaker adaptation methods for ASR systems.\nI graduated from IIT Guwahati in 2017 with a major in Computer Science. My bachelor thesis was on deep learning methods for relation extraction in clinical text, supervised by Prof. Ashish Anand.\nWhen I’m not doing ML, I like to work out, play guitar, and read fiction.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1564014817,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://desh2608.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a second-year PhD student at Johns Hopkins University, working in the Center for Language and Speech Processing (CLSP), advised by Dan Povey and Sanjeev Khudanpur. My research interests are in applied machine learning, particularly deep learning methods for speech recognition and language modeling. I am currently working on speaker adaptation methods for ASR systems.\nI graduated from IIT Guwahati in 2017 with a major in Computer Science. My bachelor thesis was on deep learning methods for relation extraction in clinical text, supervised by Prof.","tags":null,"title":"Desh Raj","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1564014817,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://desh2608.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://desh2608.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://desh2608.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":[],"content":" I did not attend ACL 2020 in Florence, Italy. I did, however, go through several videos (all the videos of oral presentations are available here), and here are some notes from the ones I found interesting or relevant to my research. My comments are in red italic. Since I work on speech recognition, most of the work related to NLP tasks is not relevant to me, so this post is definitely biased that way. Here are the broad categories:\n End-to-end speech translation Language modeling Analysis of attention models  \nEnd-to-end speech translation 1. Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation\nElizabeth Salesky, Matthias Sperber, Alan W Black\n[Paper] [Video]\n The task is end-to-end speech translation, i.e., given speech in one language, say Spanish, get translated text in a different language, say English. The problem is that the feature sequence is usually too long, e.g. a few thousand frames. Traditionally, like in the LAS model, a pyramidal encoder is used. Can we use linguistic information? Yes! Divide input sequence into segments corresponding to phoneme boundaries and then average these segments. Advantages: better BLEU score + shorter input. How to get the phoneme boundaries? Use an ASR system (need not be very good, or even from the same language, since we just need boundaries). Results: Gives consistent improvements in both low-resource and high-resource situations. But this is still not as good as a cascaded speech translation system (i.e. ASR+MT). May be interesting to see if this method works for end-to-end ASR systems in general.  2. Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation\nMatthias Sperber, Graham Neubig, Jan Niehues, Alex Waibel\n[Paper] [Video]\n Lots of ASR and MT parallel data are available, but not so much for speech translation, so using a \u0026ldquo;direct model\u0026rdquo; may not be ideal. Auxiliary data can be incorporated, e.g., by using an encoder from an ASR system and decoder from an MT system. But this results in poor data efficiency. Two-stage model: similar to cascaded model but trained in end-to-end manner. So error propagation is an issue. Proposed: attention-passing model. Idea: instead of passing hidden decoder state, pass the attention context vector. Results: better than other models, and does not reduce much even if end-to-end data is reduced.  \nLanguage modeling 3. Interpolated Spectral N-gram Language Models\nAriadna Quattoni, Xavier Carreras\n[Paper] [Video]\n Revisit spectral learning with 2 novelties: (i) capture long range dependencies and (ii) matching task evaluation metric with RNNs. Wait, but what is spectral learning? A method to learn the underlying weighted automata that generated any given set of sequences. How is it used in LM?  Create the Hankel matrix from the training set. Compute SVD of this matrix $H = P\\cdot S$. (computational bottleneck) Recover WA: $ A_{\\sigma} = P^+ H_{\\sigma} S^+$.  How to capture long-range dependencies? Use bigger n-grams. But this would explode the Hankel matrix! Solution: the basis selection algorithm in this paper (from the same group). But this is still worse than vanilla RNN. Issue: mismatch between training loss (L2 loss over Hankel reconstruction) and evaluation metric! RNNs optimize conditional cross-entropy which matches perplexity. Solution: train interpolation weights using a log-linear model:  $$ g\\left(x_{1 : n}, \\sigma\\right)=\\exp \\left[ \\sum_{j=0}^{n-1} w_{\\sigma, j} \\log f\\left(x_{n-j : n} \\cdot \\sigma\\right) \\right] $$\n Currently in ASR systems, RNNLM rescoring is done since neural LMs are difficult to incorporate in the WFST-based system directly. Perhaps spectral n-gram models in the decoder can avoid the need for rescoring?  4. What Kind of Language Is Hard to Language-Model?\nSebastian J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, Jason Eisner\n[Paper] [Video]\n What factors in a language make it harder to model? Wait, but first, what is difficulty?  Surprisal, i.e. negative log probability of the string. But this is not fair - we are comparing sentences with different contents/style/topic. So we need to work on parallel sentences.  Work on 69 languages from 13 language families. Two rules:  Open vocabulary - because using UNKs is cheating. Total bits as score; no normalization per word or character.  How to aggregate scores across the whole set?  Just take average. Problem: in missing data cases. \u0026ldquo;Mixed effects model\u0026rdquo;: $y_{2,de} \\sim n_2\\cdot \\exp d_{de}$, where $y_{2,de}$ is the score for string 2 on German, $n_2$ is the average score of string 2 for all languages, and $d_{de}$ is the average score of German for all strings. Any probabilistic model can be used for this, but MAP works well enough.  BPE-RNNLM model used. How many merges is optimum? Answer: language-dependent. But 40% (average optimum across the board) turns out to works well enough. What correlates with difficulty? Several factors (morphology, subject/verb order, average dependency length) tried, but no visible correlation. Very simple heuristics are predictive. E.g. raw sequence length predicts char-RNNLM difficulty, raw vocabulary size predicts BPE-RNNLM difficulty. Translationese is not any easier than the native language.  \nAnalysis of attention models 5. Is Attention Interpretable?\nSofia Serrano, Noah A. Smith\n[Paper] [Video]\n Usually in an attention-based classifier, we take the attention weights to find which words were considered important by the classifier to predict the output. What can go wrong here?  Overemphasizes influence of few representations Worse than other orderings of importance Not necessarily where decision was made  Interpretability: How well do attention weights represent the importance of inputs given to attention layer for the model output? Where does this problem come from? Disconnect between attention weights criteria and model output criteria. Method: zero out attention weights from highest to lowest, and renormalize. Do this until decision changes. Compute % of attention weights zeroed. Model: Single-layer bidirectional GRU with attention, followed by linear classifier. Result: \u0026gt;90% need to be zeroed before decision changes! What about random order of zeroing? Almost 100% need to be zeroed, so attention is actually doing something. Alternative orderings: (i) by gradient of decision function, (ii) gradient weighted by attention weight magnitude. Result: \u0026lt;50% need to be zeroed. Effect of encoder structure: For CNN encoder, and no encoder (word embedding fed directly into attention layer), % of weights required to be zeroed is much lower (\u0026lt; 25% even for max-to-min ordering). This last result is not out of the blue. RNN encoders tie the input representations to the attention layer more strongly, so zeroing out one weight would have lower impact.  6. Analyzing Multi-Head Self-Attention: Specialized Heads do the Heavy Lifting, the Rest can be Pruned\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov\n[Paper] [Video]\n How important are different attention heads in a multi-head self-attention model? What are their individual roles? Task: MT. Method:  Layer-wise relevance propagation. Head relevance is sum of relevance of neurons, where relevance is the attention weight Confidence: maximum attention weight of an attention head.  Result: only a few heads are more important in terms of relevance and confidence. Roles of heads? 1. Syntactic 2. Positional 3. Rare tokens Syntactic heads are present in most languages. In \u0026gt; 50% cases, at least one head gives maximum attention to least frequent (rare) tokens. How to prune heads which are not important? While concatenating attention heads, multiply them by a scalar gate. Ideally, we would like to have L0-regularization on these, but it is not differentiable. Solution: use a stochastic approximation. Even on pruning to 25% of original number, the roles are still alive. Problem: can prune to small number, but cannot start from this configuration.  In particular, I find the work on spectral learning for LMs very interesting and I hope to discuss it in more detail in upcoming blogs. Several of the papers analyzing attention models may be relevant for people working in end-to-end ASR models as well.\n","date":1564682171,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564682171,"objectID":"127dc936ed45904814ac7e9382726078","permalink":"https://desh2608.github.io/post/acl2020/","publishdate":"2019-08-01T13:56:11-04:00","relpermalink":"/post/acl2020/","section":"post","summary":"I did not attend ACL 2020 in Florence, Italy. I did, however, go through several videos (all the videos of oral presentations are available here), and here are some notes from the ones I found interesting or relevant to my research. My comments are in red italic. Since I work on speech recognition, most of the work related to NLP tasks is not relevant to me, so this post is definitely biased that way.","tags":["nlp","acl"],"title":"ACL 2020: Notes from an ASR perspective","type":"post"},{"authors":[],"categories":[],"content":" In this post, I will briefly describe the MFCC features for ASR systems, and the use of delta coefficients. I then talk about how this is approximated in Kaldi using an LDA-like transform, and finally mention some recent experimental results to replace the LDA with traditional delta features without losing out on performance in terms of WER.\nWhat are MFCCs and how are they computed? Feature extraction is the first step in any automatic speech recognition (ASR) pipeline. The objective is to compute features from speech waveforms which contain relevant information about the linguistic content of the speech, and ignore information about the background noise, emotions, etc. This problem has been extensively studied since early days in ASR research, and several feature extraction methods have been proposed. Among these, the most well-known and widely used are Mel Frequency Cepstral Coefficients (MFCCs).\nSince MFCCs are very well known, I will only briefly describe their computation in this post. Most of this is taken from this blog, which explains them in some detail. The key steps for computing MFCCs are described below.\n First, the entire waveform is divided into shorter segments of 20-40 ms each. The assumption is that in this short segment, the signal is statistically stationary, and so features can be assumed to be constant inside this window. In Kaldi and most major ASR systems, windows are 25 ms in length and at 10 ms intervals apart, i.e., they are overlapping.\n In order to recognize the frequencies present in this short segment, the power spectrum (or the periodogram estimate) is computed. This is done using discrete-time Fourier transforms.\n It is difficult to distinguish individual frequencies in the raw power spectrum, especially in the high frequency range. To solve this problem, the spectrum is convolved with several (20-40, in general) triangular Mel filters, called a filterbank. These filters are narrow at low frequency and get wider as frequency increases, in accordance with the human cochlea. Furthermore, a log transform is applied since humans don\u0026rsquo;t perceive loudness on a linear scale.\n Since filterbank energies are correlated and cannot be used directly with a Gaussian mixture with diagonal covariance, we apply a discrete cosine transform (DCT) to decorrelate them.\n  There is some debate in the community regarding the use of the DCT, instead of directly using the log Mel fiterbank features, particularly for deep neural network based acoustic models. Some research groups, like Google, use filterbanks (fbanks) while Kaldi mostly uses MFCCs, especially in its TDNN chain models. Here is Dan Povey\u0026rsquo;s take on this:\n The reason we use MFCC is because they are more easily compressible, being decorrelated; we dump them to disk with compression to 1 byte per coefficient. But we dump all the coefficients, so it\u0026rsquo;s equivalent to filterbanks times a full-rank matrix, no information is lost.\n(Source: kaldi-help)\n Delta and delta-delta features The idea behind using delta (differential) and delta-delta (acceleration) coefficients is that in order to recognize speech better, we need to understand the dynamics of the power spectrum, i.e., the trajectories of MFCCs over time. The delta coeffients are computed using the following formula.\n$$ d_t = \\frac{\\sum_{n=1}^N n (c_{t+n} - c_{t-n})}{2 \\sum_{n=1}^N n^2}, $$ where $d_t$ is a delta coefficient from frame $t$ computed in terms of the static coefficients $c_{t-n}$ to $c_{t+n}$. $n$ is usually taken to be 2. The acceleration coefficients are computed similarly, but using the differential instead of the static coefficients.\nThe LDA transform in Kaldi  For a comprehensive reference on LDA, readers are advised to refer to this post.\n The latest TDNN-based chain models in Kaldi (see, for example, this recipe) do not use differential and acceleration features (hereby refered to as \u0026ldquo;delta features\u0026rdquo; for convenience). Instead, they employ an LDA-like transformation which is essentially an affine transformation of the spliced input. Here is a sample from the xconfig of a typical Kaldi TDNN model:\ninput dim=100 name=ivector input dim=40 name=input # please note that it is important to have input layer with the name=input # as the layer immediately preceding the fixed-affine-layer to enable # the use of short notation for the descriptor fixed-affine-layer name=lda input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) affine-transform-file=$dir/configs/lda.mat # the first splicing is moved before the lda layer, so no splicing here relu-batchnorm-dropout-layer name=tdnn1 $tdnn_opts dim=1024 tdnnf-layer name=tdnnf2 $tdnnf_opts dim=1024 bottleneck-dim=128 time-stride=1 tdnnf-layer name=tdnnf3 $tdnnf_opts dim=1024 bottleneck-dim=128 time-stride=1  This splicing can be over 1 or 2 frames on either side of the central frame, i.e. Append(-1,0,1) or Append(-2,-1,0,1,2). Additionally, i-vectors are appended with the spliced input before the LDA. Although Kaldi itself has an implementation of the LDA transform available, the transformation here simply multiplies the spliced input with a full-rank matrix. This is why this is called an \u0026ldquo;LDA-like\u0026rdquo;, and not an LDA transform.\nSome new results In some sense, this LDA-like transform is a generalization of using the delta features, since it can apply arbitrary scaling to each coefficient, and this matrix is learned in the training stage. However, this means having to additionally learn $(k \\times n+d)^2$ parameters, where $k$ is the splicing window, $n$ is the MFCC size, and $d$ is the i-vector dimensionality. For typical values of $k$, $n$, and $d$, this is in the range of 50000 to 90000 parameters. While this is not a \u0026ldquo;huge\u0026rdquo; number compared to the size of modern deep networks (a typical TDNN model in Kaldi may have up to 10 million parameters), we would still like to see if this is disposable.\nI replaced the LDA transform with simple delta features. In the context of our input, the differential is simply $c_{t+1} - c_{t-1}$, and the acceleration is $c_{t-2} + c_{t+2} - 2\\times c_t$. This is implemented using a new xconfig layer called delta-layer as follows.\nclass XconfigDeltaLayer(XconfigLayerBase): \u0026quot;\u0026quot;\u0026quot;This class is for parsing lines like 'delta-layer name=delta input=idct' which appends the central frame with the delta features (i.e. -1,0,1 since scale equals 1) and delta-delta features (i.e. 1,0,-2,0,1), and then applies batchnorm to it. Parameters of the class, and their defaults: input='[-1]' [Descriptor giving the input of the layer] \u0026quot;\u0026quot;\u0026quot; def __init__(self, first_token, key_to_value, prev_names=None): XconfigLayerBase.__init__(self, first_token, key_to_value, prev_names) def set_default_configs(self): self.config = {'input': '[-1]'} def check_configs(self): pass def output_name(self, auxiliary_output=None): assert auxiliary_output is None return self.name def output_dim(self, auxiliary_output=None): assert auxiliary_output is None input_dim = self.descriptors['input']['dim'] return (3*input_dim) def get_full_config(self): ans = [] config_lines = self._generate_config() for line in config_lines: for config_name in ['ref', 'final']: # we do not support user specified matrices in this layer # so 'ref' and 'final' configs are the same. ans.append((config_name, line)) return ans def _generate_config(self): # by 'descriptor_final_string' we mean a string that can appear in # config-files, i.e. it contains the 'final' names of nodes. input_desc = self.descriptors['input']['final-string'] input_dim = self.descriptors['input']['dim'] output_dim = self.output_dim() configs = [] line = ('dim-range-node name={0}_copy1 input-node={0} dim={1} dim-offset=0'.format( input_desc, input_dim)) configs.append(line) line = ('dim-range-node name={0}_copy2 input-node={0} dim={1} dim-offset=0'.format( input_desc, input_dim)) configs.append(line) line = ('component name={0}_2 type=NoOpComponent dim={1}'.format( input_desc, output_dim)) configs.append(line) line = ('component-node name={0}_2 component={0}_2 input=Append(Offset({0},0),' ' Sum(Offset(Scale(-1.0,{0}_copy1),-1), Offset({0},1)), Sum(Offset({0},-2), Offset({0},2),' ' Offset(Scale(-2.0,{0}_copy2),0)))'.format(input_desc)) configs.append(line) line = ('component name={0} type=BatchNormComponent dim={1}'.format( self.name, output_dim)) configs.append(line) line = ('component-node name={0} component={0} input={1}_2'.format( self.name, input_desc)) configs.append(line) return configs  The following are some experimental results on mini_librispeech, wsj (Wall Street Journal), and swbd (Switchboard). The i-vector scale was reduced for mini_librispeech since the delta features are computed on top of a SpecAugment layer, which itself includes batch normalization. Therefore, using an i-vector scale of 1.0 would overpower the MFCCs.\n   Setup Test set IDCT SpecAugment i-vector scale LDA Delta     mini_librispeech dev_clean2 Y Y 0.5 7.54 7.66   wsj eval92 Y N 1.0 2.39 2.41   swbd rt03 N N 1.0 15.0 15.0    These results are for a particular test set for these setups, and for a specific decoder, but the general trend of results is found to be the same across all test set and decoder combinations. Without significant loss in performance, we can eliminate the need of an LDA transform in the network. Work on a pull request for this setup is in progress.\n","date":1564161530,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564235270,"objectID":"3e7c69a0f610968dacee042218349fe6","permalink":"https://desh2608.github.io/post/delta-feats/","publishdate":"2019-07-26T13:18:50-04:00","relpermalink":"/post/delta-feats/","section":"post","summary":"In this post, I will briefly describe the MFCC features for ASR systems, and the use of delta coefficients. I then talk about how this is approximated in Kaldi using an LDA-like transform, and finally mention some recent experimental results to replace the LDA with traditional delta features without losing out on performance in terms of WER.\nWhat are MFCCs and how are they computed? Feature extraction is the first step in any automatic speech recognition (ASR) pipeline.","tags":["speech-recognition","kaldi","mfcc"],"title":"A note on MFCCs and delta features","type":"post"},{"authors":["Ashish Arora","Chun Chieh Chang","Babak Rekabdar","Daniel Povey","David Etter","**Desh Raj**","Hossein Hadian","Jan Trmal","Paola Garcia","Shinji Watanabe","Vimal Manohar","Yiwen Shao","Sanjeev Khudanpur"],"categories":null,"content":"","date":1558483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"5258e45c65b1d1a2b2f34dc5a8c92d74","permalink":"https://desh2608.github.io/publication/icdar-19-using/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/icdar-19-using/","section":"publication","summary":"","tags":null,"title":"Using ASR methods for OCR","type":"publication"},{"authors":null,"categories":[],"content":" Recently, I came across this paper which compares several sequence discriminative training criteria based on the popular lattice-free MMI (LF-MMI) objective, and concludes that \u0026ldquo;boosted\u0026rdquo; LF-MMI outperforms others consistently. Since I couldn\u0026rsquo;t find the code publicly available, I set out to implement it myself in Kaldi. The idea was that even if the claim turned out to be false, this would give me a hands-on experience with C++ level implementations in Kaldi.\nOn first look, the implementation seems trivial if you already have a LF-MMI (also called the \u0026ldquo;chain\u0026rdquo; model in Kaldi) implementation available. However, there are several tricks used in Kaldi which are worth pointing out. In this article, I start with giving an overview of LF-MMI and its implementation in the chain models, and then talk about how I implemented boosted LF-MMI. The majority of the theory here is based on this paper which introduced LF-MMI and this doc on chain model.\nMMI \u0026ndash; a background Maximum mutual information, or MMI, is a sequence discriminative training criteria popular in ASR. \u0026ldquo;Sequence\u0026rdquo; means that the objective takes into account the utterance as a whole instead of \u0026ldquo;frame-level\u0026rdquo; objectives like cross-entropy. \u0026ldquo;Discriminative\u0026rdquo; loosely means using an objective function which supposedly optimizes some criteria associated with the task, and then minimizing that objective directly using gradient-based methods. Discriminative training for LVCSR was made popular in Dan Povey\u0026rsquo;s thesis. Formally, the MMI objective for ASR is written as\n$$ F_{MMI}(\\lambda) = \\sum_{r=1}^R \\log \\frac{P_{\\lambda}(O_r|M_{w_r})P(w_r)}{\\sum_{\\hat{w}}P_{\\lambda}(O_r|M_{\\hat{w}})P(\\hat{w})}, $$\nwhere $M_w$ is the HMM corresponding to the transcription $w$. As you can see, the objective function considers the log-probability of the whole utterance in the numerator, and normalizes it by dividing with the log-probability of all possible utterances in the denominator.\nHowever, computing the sum in the denominator means summing over an exponentially large number of word sequences, which is not practically feasible. To remedy this, we approximate the sum with either of two methods:\n N-best list: This is computed once and used for all utterances. However, this approximation is less used since it is too crude.\n Lattice structure: This may be word/phone based. A path through the lattice represents a possible word/phone sequence. One limitation with using a lattice is that it requires initialization with a trained model, and usually cross-entropy trained systems are used for this purpose. The older nnet setups in Kaldi used this approach.\n  With the advent of end-to-end models, such a requirement of a trained system to initialize the lattice comes across as a major drawback of lattice-based MMI. How can we avoid using a lattice?\nLattice-free MMI First proposed in this paper from Dan Povey, lattice-free MMI is \u0026ldquo;purely sequence trained\u0026rdquo; in the sense that no cross-entropy training is required to initialize, since it does not use a lattice. So how does it approximate the sum in the denominator? Simply put, it does not \u0026ldquo;approximate\u0026rdquo; it \u0026mdash; it computes this sum exactly.\nThe key idea is that if we represent the denominator as a graph and somehow manage to fit this graph in the GPU, then computation can be performed efficiently. In the manner that it is formalized, the denominator graph cannot be fit into the GPU. To fix this, two major modifications are applied:\n A phone LM is used instead of a word LM. The number of possible phones is much smaller than the number of possible words, which makes the size of graph for phone LM significantly smaller.\n DNN outputs are computed at one-third the standard frame rate, which means that we now have 3 times fewer outputs to compute for any utterance. This is achieved by setting the frame shift to 30 ms instead of the traditional 10 ms.\n  This reduced frame rate also means that now we cannot use the standard 3-state left-to-right HMM topology that is common in ASR, since we want to traverse the entire HMM in a single frame. Instead, we use an HMM which can emit symbols in the set ab*.\nTo train such a system according to the MMI objective, we need a way to efficiently compute the objective itself and its derivative. In Kaldi, the numerator and denominator are represented as FSTs (corresponding to the HMMs) and the overall objective function is simply the difference of these in log-space. As such, we need a way to efficiently represent these FSTs and perform forward-backward on them.\nThe denominator and numerator FSTs Let us start with the denominator FST since it is much more expensive. The process of creating the denominator FST is very similar to the decoding graph creation. The key idea, as in traditional ASR using WFSTs (see Mohri\u0026rsquo;s well-known paper), is to have separate FSTs for H (HMM state graph), C (context-dependency), L (the lexicon), and G (the language model), and use WFST composition algorithms to get the final graph, with the exception that since we are using phones instead of words, we don\u0026rsquo;t need the L graph. So our final graph is actually an HCP instead of an HCLG, where P denotes the phone LM.\nAt this point, I would like to point out some Kaldi specifics. The phone LM P is created in stage -6 by calling the function create_phone_lm(). The denominator FST is created in the stage -5 within the train.py script, which internally makes a call to the binary chain-make-den-fst. The denominator graph is specificied in chain-den-graph.cc. It uses the files $dir/tree (the tree) and $dir/0.trans_mdl (the transition model), which correspond to the C and H components, and the phone LM that was created in the previous stage.\nThe phone LM P is constructed so that the overall size of the graph is minimized. It is a 4-gram with no backoff lower than 3-gram so that triphones not seen in training cannot be generated. The number of states is limited by completely removing low-count 4-gram states.\nOnce we have the composed graph HCP, a different kind of minimization technique is used, which consists of performing the following operations thrice in a row.\n Push the weights Minimize the graph Reverse the arcs and swap initial and final states.  Another trick used to reduce the size of the denominator FST for training on the GPU is to train on chunks of 1-1.5 seconds, instead of the entire utterance. However, to do this, we would also need to break up the transcript, and 1-second chunks may not coincide with word boundaries. How do we solve this?\nRecall that the numerator FST is defined to be utterance-specific, and encodes alternative pronunciations of the transcript of the original utterance. This lattice is turned into an FST that constrains at what time the phones can appear, with an error window of 0.05s from their position in the lattice. This is then processed into an FST whose labels are pdf-ids (neural net outputs). We extract fixed size chunks from this FST for training chunks in the denominator FST.\nAnother issue associated with chunk-level FSTs is that the initial probabilities are now different. We approximate this by running the HMM for a few iterations and then averaging the probabilities to use as the initial probability of any state. This is a crude approximation but it seems to work. We then call this the normalization FST.\nThe numerator FST is much easier since it just contains the lattice for one utterance, broken into chunks of fixed length. The only point worth mentioning here (and this will be important when we talk about boosted LF-MMI later) is that the numerator FST is composed with the normalization FST. This is done for two reasons.\n It ensures that the objective function value is always negative, which makes it easier to interpret. It also ensures that the numerator FST does not contain sequences that are not allowed by the denominator (or normalization) FST. This happens since the sum of the overall path weights for such sequences will be dominated by the normalization FST part.  Forward-backward computations Again, since the numerator FST is much smaller, its forward and backward computations are performed on CPU (the process is outlined in chain-numerator.h), while those for the denominator FST (outlined in chain-denominator.h) are performed on the GPU.\nThe basic forward and backward algorithm are the same as well known in literature, and a pseudocode is also given in the extended comments in chain-denominator.h. However, this algorithm is susceptible to numeric overflow and underflow. To avoid this, we multiply the emission probability of the frame with a normalizing factor $\\frac{1}{alpha(t)}$ where $alpha(t) = \\sum_{i} \\alpha_i (t)$. This is also called an \u0026ldquo;arbitrary scale\u0026rdquo; since in principle it can be allowed to be any value and doesn\u0026rsquo;t affect the posterior. However, we do need to add a quantity $\\sum_{t=0}^{T-1} \\log alpha(t)$ to the final log probability obtained to make it equal to the actual log probability. This \u0026ldquo;arbitrary scaling\u0026rdquo; is used in both the forward and backward computations.\nThe actual objective function computation is implemented in ComputeChainObjfAndDeriv() defined in chain-training.cc. There are two Kaldi-specific things I must point out here.\n The forward-backward computation for the denominator FST in the GPU is not done in the log domain, since computing log several times makes things slower. However, this also means that the objective function values can occasionally become \u0026ldquo;bad\u0026rdquo;. To fix this, the PenalizeOutOfRange() function is used to encourage the objective to be within the [-30,30] range.\n The denominator computation is performed before the numerator, so as to reduce the maximum memory usage. I am not sure how this is, but it is important to remember this detail as we move to the implementation of boosted LF-MMI.\n  Implementing boosted LF-MMI First, what is boosted LF-MMI? It is the same as LF-MMI, except that now we optimize the following objective function.\n$$ F_{bMMI}(\\lambda) = \\sum_{r=1}^R \\log \\frac{P_{\\lambda}(O_r|M_{W_r})P(W_r)}{\\sum_{\\hat{w}}P_{\\lambda}(O_r|M_{\\hat{w}})P(\\hat{w})e^{-bA(M_{w_r},M_{\\hat{w}})}}, $$\nwhere $b$ is the boosting factor and $A(M_{w_r},M_{\\hat{w}})$ is the accuracy function which measures the number of matching labels between the reference and hypothesis sequences. My Kaldi implementation for LF-bMMI can be found in this branch. You may note that most of the changes are cosmetic and only serve to pass the new argument $b$ from the training script to the actual implementation, which is in the function ComputeBoostedChainObjfAndDeriv().\nIn our implementation, the only change is that in the computation for num_logprob_weighted, we subtract from numerator.forward() by a term b * num_seq * frames_per_seq. This might seem weird at first, since in the expression of the objective function, we actually subtract the denominator by this term. However, recall that the numerator FST is composed with the normalization FST, so that this modification will result in the same result as the objective function above.\nOn trying out LF-bMMI for mini-Librispeech, I found it to be slightly worse than regular LF-MMI (11.86 vs 11.74 WER), and consultation with Vimal Manohar revealed that he had tried LF-bMMI and LF-SMBR along with Hossein Hadian last year to similar results.\n","date":1558453752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558471871,"objectID":"40c1bf9f7cc639d4d7103783fe2ad64f","permalink":"https://desh2608.github.io/post/chain/","publishdate":"2019-05-21T11:49:12-04:00","relpermalink":"/post/chain/","section":"post","summary":"Recently, I came across this paper which compares several sequence discriminative training criteria based on the popular lattice-free MMI (LF-MMI) objective, and concludes that \u0026ldquo;boosted\u0026rdquo; LF-MMI outperforms others consistently. Since I couldn\u0026rsquo;t find the code publicly available, I set out to implement it myself in Kaldi. The idea was that even if the claim turned out to be false, this would give me a hands-on experience with C++ level implementations in Kaldi.","tags":["kaldi","chain"],"title":"On lattice free MMI and Chain models in Kaldi","type":"post"},{"authors":null,"categories":null,"content":"","date":1556925300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"87a215ad88c48a9f2668c308248e9e23","permalink":"https://desh2608.github.io/talk/joint_ctc_attention/","publishdate":"2019-05-02T17:15:00-30:00","relpermalink":"/talk/joint_ctc_attention/","section":"talk","summary":"","tags":null,"title":"Joint CTC-Attention for ASR using Multi-task Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":1556640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"36f77fb9f4e4f5ef10ca5348f3c8e2e1","permalink":"https://desh2608.github.io/talk/cpc/","publishdate":"2019-04-29T10:00:00-30:00","relpermalink":"/talk/cpc/","section":"talk","summary":"","tags":null,"title":"Contrastive Predictive Coding","type":"talk"},{"authors":null,"categories":null,"content":"","date":1555610400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"598b71b623b7a587c24cace2585737c9","permalink":"https://desh2608.github.io/talk/dataset_shift/","publishdate":"2019-04-17T12:00:00-30:00","relpermalink":"/talk/dataset_shift/","section":"talk","summary":"","tags":null,"title":"Dataset Shift in NLP","type":"talk"},{"authors":null,"categories":[],"content":" Since I started working on speech recognition, I have only ever evaluated results using Word Error Rate (WER) metrics. Often, when some method performed differently on different datasets, senior researchers in my lab have told me it is \u0026ldquo;too noisy\u0026rdquo;, or has \u0026ldquo;different channel characteristics\u0026rdquo;. I have learnt about read speech and conversational speech, and how an ASR system needs to be robust to several variations in the utterance.\nHowever, I have not once actually listened to these datasets to understand how complex the task is. I think many amateur researchers make this error starting out, since the data and recipes (in Kaldi, at least) are so nicely constructed, that you just tend to think of the speech data as a bunch of MFCC vectors.\nIn this article, I put samples of several popular corpora used in ASR research, along with their characteristics and my impression of how they sound. The resource descriptions, in most cases, are taken from their respective Kaldi recipes.\nDisclaimer: Some of these datasets are proprietary, and I don\u0026rsquo;t really know if sharing a small sample is permitted under the license. If you know about the particulars, please send me a mail at draj@cs.jhu.edu and I will update the post.\nResource Management (RM) Clean speech in a medium-vocabulary task consisting of commands to a (presumably imaginary) computer system. About 3 hours of training data.\nYour browser does not support the audio element.  Here is a transcription of the above sample: SHOW THE GRIDLEY+S TRACK IN BRIGHT ORANGE WITH HORNE+S IN DIM RED.\nThe sample sounds distinctly synthetic, and does not have any noise. As such, it is among the easiest datasets for ASR, and is often used for a Hello World equivalent for ASR systems.\nBabel The first version of Babel was recorded speech in 5 European languages (as mentioned on the Wiki), but the current version in Kaldi consists of 9 non-European languages like Assamese, Cantonese, Turkish, etc. Here is a sample from the Babel Bengali.\nYour browser does not support the audio element.  It sounds like conversational speech recorded at one end of a telephone conversation. There are several silences, which would correspond to the other person speaking (which is not recorded). There is a static noise in the background as well.\nSwitchboard This is telephonic speech in English released in 1997, collected as a 2-channel, 8kHz-sampled data. Here is a sample.\nYour browser does not support the audio element.  It sounds similar to the Babel corpus, except that it is entirely in English and a little cleaner.\nFisher English This is conversational telephone speech collected as 2-channel, 8kHz-sampled data. The data is similar to Switchboard but the transcription was mostly done in a \u0026ldquo;faster\u0026rdquo;, lower-quality way. Here is a sample.\nYour browser does not support the audio element.  Librispeech LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned. There is a smaller version called mini_librispeech created for the purpose of regression testing. Here is a sample.\nYour browser does not support the audio element.  The transcription for the above sample: as you know and as i have given you proof i have the greatest admiration in the world for one whose work for humanity has won such universal recognition i hope that we shall both forget this unhappy morning and that you will give me an opportunity of rendering to you in person\nAs is evident, this is read speech, and would probably be much easier to transcribe than the earlier datasets.\nTED-LIUM The TED-LIUM corpus is English-language TED talks, with transcriptions, sampled at 16kHz. It contains about 118 hours of speech. Here is a sample.\nYour browser does not support the audio element.  We can see that the difficulty of transcription may be between a read speech and a conversational speech. There are very few silences, unlike the telephonic speech corpora, but there is still some background noise and reverberation since these are recorded talks.\nTIMIT The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Here is a sample.\nYour browser does not support the audio element.  Wall Street Journal (WSJ) This is a corpus of read sentences from the Wall Street Journal, recorded under clean conditions. The vocabulary is quite large. About 80 hours of training data. Here is a sample.\nYour browser does not support the audio element.  As is clear, TIMIT and WSJ sound very similar, but the spoken content differs greatly.\n","date":1555027835,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555081792,"objectID":"bdcd00e6ffc5c56fb4298642d8a1012d","permalink":"https://desh2608.github.io/post/asr-data/","publishdate":"2019-04-11T20:10:35-04:00","relpermalink":"/post/asr-data/","section":"post","summary":"Since I started working on speech recognition, I have only ever evaluated results using Word Error Rate (WER) metrics. Often, when some method performed differently on different datasets, senior researchers in my lab have told me it is \u0026ldquo;too noisy\u0026rdquo;, or has \u0026ldquo;different channel characteristics\u0026rdquo;. I have learnt about read speech and conversational speech, and how an ASR system needs to be robust to several variations in the utterance.","tags":[],"title":"What do speech datasets sound like?","type":"post"},{"authors":null,"categories":[],"content":" This is a regularly updated post on some tips and tricks for working with Kaldi.\nList of contents:\n How to stop training mid-way and decode using last trained stage About Sum() and Append() in Kaldi xconfig Checking training logs Converting between FV and FM types Number of epochs in Kaldi  \nHow to stop training mid-way and decode using last trained stage In the Kaldi chain model, suppose you are training for 4 epochs (which is close to 1000 iterations in the usual run of the TED-LIUM recipe). During training, suppose you decide to stop midway and check the decoding result.\nNow, the training can be stopped and resumed simply by supplying the arguments --stage and --train-stage, where the input to stage is the stage where the train.py is called, and train-stage is the stage from where you want to continue training.\nBut if you stop at, say, stage 239, and want to decode, you first have to prepare the model for testing. This is so that dropout and batchnorm aren\u0026rsquo;t performed at test time. For this, first run\nnnet3-am-copy --prepare-for-test=true \u0026lt;dir\u0026gt;/239.mdl \u0026lt;dir\u0026gt;/final.mdl  This creates a testing model called final.mdl which the decode.sh script uses for decoding. Instead of using the default name final, you can create any test copy name, say 239-final.mdl. To use this mdl file for decoding, pass this as argument to the --iter argument in decode.sh.\n\nAbout Sum() and Append() in Kaldi xconfig If you have worked with Kaldi xconfig, it is pretty easy to define layer inputs and outputs, using something called Descriptors. They act as a glue between components and can also perform easy operations like append, sum, scale, round, etc. So, for instance, you can have the following xconfig:\ninput name=ivector dim=100 input dim=40 name=input relu-batchnorm-layer name=tdnn1 dim=1280 input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) linear-component name=tdnn2l dim=256 input=Append(-1,0) relu-batchnorm-layer name=tdnn2 input=Append(0,1) dim=1280 linear-component name=tdnn3l dim=256 relu-batchnorm-layer name=tdnn3 dim=1280 input=Sum(tdnn3l,tdnn2l)  This network does not make too much sense and is only for purpose of representation. At some point, you may require to do something of the sort Sum(Append(x,y),z), i.e., append two inputs and add it to a third input. This operation, however, isn\u0026rsquo;t allowed in the xconfig.\nThis is because Sum() takes 2 \u0026lt;sum-descriptor\u0026gt; types, while the output of Append() is a \u0026lt;descriptor\u0026gt; type which is a super class of \u0026lt;sum-descriptor\u0026gt;, and as such, there is an argument type mismatch. This can be easily solved:\nno-op-component name=noop1 input=Append(x,y) relu-batchnorm-layer name=tdnn3 dim=1280 input=Sum(noop1,tdnn2l)  Similarly, a Scale() outputs a \u0026lt;fwd-descriptor\u0026gt; while a Sum() expects a \u0026lt;sum-descriptor\u0026gt;, so to use Scale() inside Sum() we first have to pass it through a no-op-component.\n\nChecking training logs When you are training any chain model in Kaldi, it is important to know if the parameters are getting updated well and if the objective function is improving. All such information is stored in the log directories in Kaldi, but since there is so much information in there, it may be difficult to find what you are looking for.\nSuppose your working directory is something like exp/chain/tdnn_1a/. Then, first go to the log directory by\ncd exp/chain/tdnn_1a/log  Now, to check the objective functions for all the training iterations, do\nls -lt train* | grep -r 'average objective' .  This will print something like this, for all the iterations.\nLOG (nnet3-chain-train[5.5.103~1-34cc4e]:PrintTotalStats():nnet-training.cc:348) Overall average objective function for 'output' is -0.100819 over 505600 frames. LOG (nnet3-chain-train[5.5.103~1-34cc4e]:PrintTotalStats():nnet-training.cc:348) Overall average objective function for 'output-xent' is -1.17531 over 505600 frames.  Here, our actual objective is \u0026lsquo;output\u0026rsquo;. The other objective is the cross-entropy regularization term. To avoid printing it, you can replace 'average objective' with \u0026quot;average objective function for 'output'\u0026quot; in the previous command. Look at the values. If the model is learning well, the objective should be increasing (since it is the log-likelihood).\nYou may also want to see if your parameters are updating how you want them to be. For this, do\nls -lt progress* | grep -r 'Relative parameter differences' .  Usually, the relative parameter differences are close to the learning rate.\n\nConverting between FM and FV types Kaldi has two major types: Matrix and Vector. As such, features are often stored in one of these two file types. For instance, when you extract i-vectors, they are stored as a matrix of floats (FM) and if you extract x-vectors, they are stored as vectors of float (FV). Often it may be required to convert features stored as FV to FM and vice-versa.\nAlthough there is no dedicated Kaldi binary to perform this conversion, we can leverage the fact that the underlying text format for both these types is the same and use this as an intermediate for the conversion. For example, to convert from FV to FM:\ncopy-vector --binary=false scp:exp/xvectors/xvector.scp ark,t:- | \\ copy-matrix ark,t:- ark,scp:exp/xvectors/xvector_mat.ark,exp/xvectors/xvector_mat.scp  Similarly, to convert from FM to FV:\ncopy-matrix --binary=false scp:exp/ivectors/ivector.scp ark,t:- | \\ copy-vector ark,t:- ark,scp:exp/ivectors/ivector_vec.ark,exp/ivectors/ivector_vec.scp  \nNumber of epochs in Kaldi This is borrowed directly from Dan\u0026rsquo;s reply in a kaldi-help Google Group post.\n A few of the reasons we use relatively few epochs in Kaldi are as follows:\n We actually count epochs after augmentation, and with a system that has frame-subsampling-factor of 3 we separately train on the data shifted by -1, 0 and 1 and count that all as one epoch. So for 3-fold augmentation and frame-subsampling-factor=3, each \u0026ldquo;epoch\u0026rdquo; actually ends up seeing the data 9 times.\n Kaldi uses natural gradient, which has better convergence properties than regular SGD and allows you to train with larger learning rates; this might allow you to reduce the num-epochs by at least a factor of 1.5 or 2 versus what you\u0026rsquo;d use with normal SGD.\n We do model averaging at the end\u0026ndash; averaging over the last few iterations of training (an iteration is an interval of usually a couple minutes\u0026rsquo; training time). This allows us to use relatively large learning rates at the end and not worry too much about the added noise; and it allows us to use relatively high learning rates at the end, which further decreases the training time. This wouldn\u0026rsquo;t work without the natural gradient; the natural gradient stops the model from moving too far in the more important directions within parameter space.\n We start with aligments learned from a GMM system, so the nnet doesn\u0026rsquo;t have to do all the work of figuring out the alignments\u0026ndash; i.e. it\u0026rsquo;s not training from a completely uninformed start.\n  So supposing we say we are using 5 epochs, we are really seeing the data more like 50 times, and if we didn\u0026rsquo;t have those tricks (NG, model averaging) that might have to be more like 100 or 150 epochs, and without knowing the alignments, maybe 200 or 300 epochs.\n ","date":1553702741,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562266733,"objectID":"49c6f08474ebf1637a34d7dc9702b40e","permalink":"https://desh2608.github.io/post/kaldi-tricks/","publishdate":"2019-03-27T12:05:41-04:00","relpermalink":"/post/kaldi-tricks/","section":"post","summary":"This is a regularly updated post on some tips and tricks for working with Kaldi.\nList of contents:\n How to stop training mid-way and decode using last trained stage About Sum() and Append() in Kaldi xconfig Checking training logs Converting between FV and FM types Number of epochs in Kaldi  \nHow to stop training mid-way and decode using last trained stage In the Kaldi chain model, suppose you are training for 4 epochs (which is close to 1000 iterations in the usual run of the TED-LIUM recipe).","tags":["Kaldi"],"title":"Some Kaldi Things","type":"post"},{"authors":null,"categories":null,"content":"","date":1552406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"5962fc9d77e3265e6be19b6788e9c744","permalink":"https://desh2608.github.io/talk/attention_model_asr/","publishdate":"2019-03-11T10:00:00-30:00","relpermalink":"/talk/attention_model_asr/","section":"talk","summary":"","tags":null,"title":"Attention-based Models for ASR","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://desh2608.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":[],"content":" Recently, I was reading a paper on language model adaptation, which used an optimization technique called Generalized Iterative Scaling (GIS). Having no idea what the method was, I sought out the first paper which proposed it, but since the paper is from 1972, and I am not a pure math guy, I found it difficult to follow. After some more looking around, I chanced upon this lucid JMLR\u0026rsquo;10 paper from Chih-Jen Lin: Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models. In this post, I will summarize the ideas in the paper, primarily the discussion about a unified framework for Iterative Scaling (IS) and Coordinate Descent (CD) methods, and how each particular technique is derived from this general framework.\nThe General Framework Iterative Scaling (IS) and Coordinate Descent (CD) are methods used to optimize maximum entropy (maxent) models. What is a maxent model? Given a sequence $x$, a maxent model predicts the label sequence $y$ with maximal probability. It is discriminatively trained by modeling the conditional probability\n$$ P_{\\mathbf{w}}(y|x) = \\frac{S_{\\mathbf{w}}(x,y)}{T_{\\mathbf{w}}(x)}, $$\nwhere $S_{\\mathbf{w}}(x,y) = \\exp(\\sum_t w_t f_t(x,y))$ and $T_{\\mathbf{w}}(x) = \\sum_y S_{\\mathbf{w}}(x,y)$.\nNote that each of the $f_t$ are features which can be defined arbitrarily with the sole constraint that they must be non-negative. Each $f_t$ has a corresponding weight $w_t$ which needs to be estimated. IS and CD methods do this estimation by iterating over all the $w_t$\u0026rsquo;s, either sequentially or in parallel. Based on the above conditional probability, we can define an objective function by taking the log of the probability and adding an L2-regularization term to it as\n$$ \\text{min}_{\\mathbf{w}} L(\\mathbf{w}) \\equiv \\text{min}_{\\mathbf{w}} \\sum_x \\tilde{P}(x) \\log T_{\\mathbf{w}}(x) - \\sum_t w_t \\tilde{P}(f_t) + \\frac{1}{2\\sigma^2}\\sum_t w_t^2. $$\nHere, $\\tilde{P}(x) = \\sum_y \\tilde{P}(x,y),$ where $\\tilde{P}(x,y)$ is the empirical distribution, and $\\tilde{P}(f_t)$ is the expected value of $f_t(x,y)$. The log-likelihood itself (without regularization) is convex, but adding the regularization term makes it strictly convex, and it can also be shown that this objective function has a unique global minima.\nIf we update our weights (either in parallel or in sequence), after one such iteration of updation, we change our objective function from $L(\\mathbf{w})$ to $L(\\mathbf{w}+\\mathbf{z})$, where $\\mathbf{z}$ si the update made to the weights. Each such iteration can be written as a subproblem which we need to solve, i.e.\n$$ A(\\mathbf{z}) \\leq L(\\mathbf{w}+\\mathbf{z}) - L(\\mathbf{w}).$$\nIn addition, if we have $A(0) = 0$, this implies that $L$ decreases with every update. Let us now expand the RHS in the above equation. We have\n$$ \\begin{align} L(\\mathbf{w}+\\mathbf{z}) - L(\\mathbf{w}) \u0026amp;= \\sum_x \\tilde{P}(x) \\log T_{\\mathbf{w}+\\mathbf{z}}(x) - \\sum_t w_t \\tilde{P}(f_t) + \\frac{1}{2\\sigma^2}\\sum_t (w_t+z_t)^2 \\\\\\ \u0026amp; - \\sum_x \\tilde{P}(x) \\log T_{\\mathbf{w}}(x) + \\sum_t w_t \\tilde{P}(f_t) - \\frac{1}{2\\sigma^2}\\sum_t w_t^2 \\\\\\ \u0026amp;= \\sum_x \\tilde{P}(x) \\log \\frac{T_{\\mathbf{w}+\\mathbf{z}}(x)}{T_{\\mathbf{w}}(x)} + \\sum_t Q_t (z_t) \\end{align} $$\nwhere $Q_t(z_t) \\equiv \\frac{2w_tz_t + z_t^2}{2\\sigma^2} - z_t \\tilde{P}(f_t)$. Further, the ratio in the log term can be simplified as\n$$ \\frac{T_{\\mathbf{w}+\\mathbf{z}}(x)}{T_{\\mathbf{w}}(x)} = \\sum_y P_{\\mathbf{w}}(y|x)e^{\\sum_t z_t f_t(x,y)}. $$\nThis is the general overview of the problem that all IS and CD methods solve. The difference is in how this function is minimized. Let us look at each of the methods and how they build upon this general framework.\nCoordinate Descent CD solves the exact problem without any approximation, i.e., the subproblem is\n$$ A(\\mathbf{z}) = L(\\mathbf{w}+\\mathbf{z}) - L(\\mathbf{w}) $$\nThis then leads to the subproblem be exactly equal to as derived above. This has an advantage and a limitation.\n Since $A(\\mathbf{z})$ here is the maximum possible decrement in any iteration, the convergence requires the least number of steps out of all possible approximations of $A(\\mathbf{z})$.\n Because of the presence of the log term in the objective function, there is no closed form solution, and so every iteration must solve an optimization problem using the Newton method.\n  In practice, the Newton optimization at each step overshadows any gain due to fewer iterations till convergence, so that CD takes more time to converge than IS methods which approximate $A(\\mathbf{z})$.\nGeneralized IS (GIS) and Sequential Conditional GIS (SC-GIS) GIS and SC-GIS use the approximation $\\log \\alpha \\leq \\alpha -1$ to get\n$$ \\begin{align} L(\\mathbf{w}+\\mathbf{z}) - L(\\mathbf{w}) \u0026amp;\\leq \\sum_t Q_t (z_t) + \\sum_x \\tilde{P}(x) (\\sum_y P_{\\mathbf{w}}(y|x) e^{\\sum_t z_t f_t(x,y)} - 1) \\\\\\ \u0026amp;= \\sum_t Q_t (z_t) + \\sum_{x,y} \\tilde{P}(x) (P_{\\mathbf{w}}(y|x)e^{\\sum_t z_t f_t(x,y)} - 1) \\end{align} $$\nDefine $f^{\\#}(x,y) = \\sum_t f_t(x,y)$ and $f^{\\#}=\\text{max}_{x,y}(f^{\\#}(x,y))$. We can then use Jensen\u0026rsquo;s inequality to upper bound the exponential term in the above inequality. GIS is a parallel update method, i.e., all the $w_t$\u0026rsquo;s are updated simultaneously, which means that we can use $f^{\\#}$ to bound the exponential terms. On the contrary, SC-GIS is a sequential method, which means we can only use $f_t^{\\#}$ to get this bound, where $f_t^{\\#} \\equiv \\text{max}_{x,y}f_t(x,y)$. Finally, the subproblems can be written as\n$$ A_t^{GIS}(z_t) = Q_t (z_t) + \\frac{e^{z_t f^{\\#}}-1}{f^{\\#}}\\sum_{x,y} \\tilde{P}(x) P_{\\mathbf{w}}(y|x)f_t(x,y) $$\n$$ A_t^{SC-GIS}(z_t) = Q_t (z_t) + \\frac{e^{z_t f_t^{\\#}}-1}{f_t^{\\#}}\\sum_{x,y} \\tilde{P}(x) P_{\\mathbf{w}}(y|x)f_t(x,y) $$\nImproved IS (IIS) A problem with bounding in terms of $f^{\\#}$ as done in GIS is that $f^{\\#}$ can be too large even if one of the $(x,y)$ pairs has a large value of $f^{\\#}(x,y)$. This would cause the subproblem to be very small, similar to the issue of small learning rates in gradient-based optimization. To remedy this, we can bound in terms of $f^{\\#}(x,y)$, although in that case we the term cannot be taken out of the summation. This is what is done in IIS, and this gives the following definition of the subproblem.\n$$ A_t^{IIS}(z_t) = Q_t (z_t) + \\sum_{x,y} \\tilde{P}(x) P_{\\mathbf{w}}(y|x)f_t(x,y)\\frac{e^{z_t f^{\\#}(x,y)}-1}{f^{\\#}(x,y)} $$\nKey points  Iterative scaling and coordinate descent methods have provably linear convergence.\n However, the time complexity of solving each subproblem is key in choosing which method to use for optimization.\n GIS and SC-GIS have closed form solutions for the subproblems, which makes it $\\mathcal{O}(1)$ to solve each iteration.\n Although CD and IIS need Newton optimization for each subproblem, the authors propose a fast CD method which performs only 1 update in later iterations. This is because it is empirically observed that a single update is enough to update the weight sufficiently in later stages.\n  ","date":1544459371,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544544224,"objectID":"0217cfef315d61ac94184f1da835b747","permalink":"https://desh2608.github.io/post/iterative-scaling-coordinate-descent/","publishdate":"2018-12-10T11:29:31-05:00","relpermalink":"/post/iterative-scaling-coordinate-descent/","section":"post","summary":"Recently, I was reading a paper on language model adaptation, which used an optimization technique called Generalized Iterative Scaling (GIS). Having no idea what the method was, I sought out the first paper which proposed it, but since the paper is from 1972, and I am not a pure math guy, I found it difficult to follow. After some more looking around, I chanced upon this lucid JMLR\u0026rsquo;10 paper from Chih-Jen Lin: Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models.","tags":["machine-learning","optimization","maxent"],"title":"Iterative Scaling and Coordinate Descent","type":"post"},{"authors":null,"categories":[],"content":" Think about tasks such as machine translation (MT), automatic speech recognition (ASR), or handwriting recognition (HWR). While these appear very distinct, on abstraction they share the same pipeline wherein given an input signal, we are required to predict some text. The difference only lies in the form of the input signal - it is a piece of text, a sound wave, or a line image, in the case of MT, ASR, and HWR, respectively.\nIn all of these tasks, OOV words are a major source of nuisance. What is an OOV word? Simply put, these are those words in the test dataset which are not seen in the training data, and as such, not present in the vocabulary - hence the name \u0026ldquo;out of vocabulary\u0026rdquo;. Even if the training vocabulary is very large (in fact, the name Large Vocabulary ASR is very common), the test data may still have words which were never seen before, for instance, names of people, places, or organizations.\nA crude way of dealing with such OOV words may be to simply predict a special token \u0026lt;UNK\u0026gt; whenever they are encountered. However, this would lead to severe information loss, especially when all new names are replaced by the special token. This is where subwords come into the picture.\n Subwords are smaller units that comprise words. They may be a single character, or even entire words.\n For example, suppose our training vocabulary consists of just 2 words {\u0026lsquo;speech\u0026rsquo;,\u0026lsquo;processing\u0026rsquo;}. If our language model is trained on word-level, we would only be able to predict these 2 words, and nothing else. So while testing, if we are required to predict the phrase \u0026ldquo;he sings in a choir\u0026rdquo;, our model would fail miserably. However, if we had trained on a subword-level (say, character level), we have a non-zero chance of predicting the phrase since all the characters are seen in the training. This provides sufficient motivation for using subwords in these tasks.\nTraditionally, in ASR, subwords have been modeled using information from phonemes (distinct sound units), such that a subword corresponds to a phoneme unit. The intuition is that at test time, any new word can only be formed using phonemes of the language. However, this requires considerable domain knowledge, and even still, variations in accent or speaker can greatly affect test-time performance.\nIn MT, subwords first came into limelight with this popular paper from Seinrich and Haddow at the University of Edinburgh. They used a simple but effective Byte Pair Encoding (BPE) based approach to identify subword units in the text. The summary of their method is as follows:\n Fix a vocabulary size V according to your total data size. Separate all the characters in all the words. Merge the most frequent bigram into one token and add it to the vocabulary. Perform V such merge operations to get the final vocabulary.  This simple method performs extremely well in practice, and the authors were able to get improvements of about 1.1 BLEU points on an English to German translation task.\nI was recently working on an HWR task which required similar subword modeling for OOV word recognition, and the remainder of this article is about the methods used and their performance.\nTowards a likelihood-based model The first method we tried was the BPE-based approach, and it gave improvements on the word-error rate (WER) over the word-based model. However, BPE is constrained in the sense that it is a deterministic technique. Once you have fixed the training vocabulary, every string can only be segmented in a specific way. This may hint at a loss of modeling power, and so our first hypothesis is that a probabilistic segmentation technique may perform better.\nOn further investigation, I found a recent paper which proposes a technique known as \u0026ldquo;subword regularization\u0026rdquo; for MT. The method consists of two parts: vocabulary learning, and subword sampling.\nVocabulary learning Similar to the BPE-based technique, we start with all the characters distinct in every word, and merge until we reach the desired vocabulary size V. However, while BPE used the metric of most frequent bigram, the Unigram SR method ranks all subwords according to the likelihood reduction on removing the subword from the vocabulary. The top 80% of these are retained and the rest are discarded. Once this phase is over, we can now obtain the likelihood of observing a subword sequence given any string (sentence).\nSubword sampling We choose the top-k segmentations based on the likelihood, and then model them as a multinomial distribution $P(x_i | X) = \\frac{P(x_i)^{\\alpha}}{\\sum_l P(x_i)^{\\alpha}}$, where $\\alpha$ is a smoothing hyperparameter. A smaller $\\alpha$ leads to a more uniform distribution, while a larger $\\alpha$ leads to Viterbi sampling (i.e., selection of the best segmentation).\nThe idea behind this method is \u0026ldquo;regularization by noise\u0026rdquo;. This means that the algorithm is expected to generalize well since we are now training it with some added noise by selecting several different segmentation candidates for any word, and so the model sees a wider variety of subwords during training.\nFor implementation, we used Google\u0026rsquo;s sentencepiece library, which is also the official code of the paper linked above, and integrated it in our Kaldi-based pipeline (see here). While the method supposedly performed well in MT, we didn\u0026rsquo;t obtain the same performance improvements in the HWR task. A top-1 (deterministic) sampler gave similar results as BPE, but a top-5 sampler performed worse, which hinted that probabilstic sampling may not necessarily be the best suited option for our task.\nFor further analysis, I looked at the frequency of different subword lengths learned by the two methods for the same total vocabulary size.\nIt turns out that the unigram method learns several \u0026ldquo;longer\u0026rdquo; subwords than BPE, which may give us some idea about the poorer performance. This suggested that if we somehow put a constraint on the lengths of the learned subwords while keeping the probabilistic sampling, we might get the best of both worlds.\nDigression - The Morfessor tool Readers familiar with linguistics (or morphology in particular) would have heard about (or used) the Morfessor tool, which provides an unsupervised technique for morpheme recognition. Morphemes, in a crude sense, are essentially subword units which are self-contained in meaning. Interestingly, the first Morfessor paper proposed a technique which is very similar to the likelihood-based subword modeling in the unigram SR paper (although the author does not seem to be aware of this). Additionally, they also proposed a minimum description length (MDL) based approach which added the subword lengths as a cost in the objective function, and therefore penalized longer subwords. Empirically, they found that the MDL technique outperformed the likelihood based method, and this further reinforced my belief that a subword length constraint would prove beneficial for the task.\nLZW-based subword modeling In an Interspeech 2005 paper, a new subword modeling algorithm was presented which supposedly correlated strongly with syllables of a language. The method is based on the popular LZW compression technique (which is also used in the Unix compress utility). In the context of strings, the LZW method finds a set of prefix-free substrings to encode the given string. The authors of the paper further used subword length tables to keep track of how many times each such subword was called during training, and thus ranked them within the tables. The test-time segmentation was determined by computing the average rank of all the segmentation candidates in this tree traversal.\nIn our implementation, we further integrated the probabilistic sampling method from the unigram SR, and used memoization to make the tree traversal computationally efficient. The implementation for learning and applying the model can be found here and here, respectively.\nPerhaps the most critical segments of the implementation are the following:\ndef learn_subwords_from_word(word, tab_seqlen, tab_pos, max_subword_length): w = \u0026quot;\u0026quot; pos = 0 for i,c in enumerate(word): if (i == len(word) - 1): pos = 2 wc = w + c if (len(wc) \u0026gt; max_subword_length): wc = c if wc in tab_seqlen[len(wc)-1]: w = wc tab_seqlen[len(wc)-1][wc] += 1 else: tab_seqlen[len(wc)-1][wc] = 1 w = c if wc in tab_pos[pos]: w = wc tab_pos[pos][wc] += 1 i -= 1 else: tab_pos[pos][wc] = 1 w = c pos = min(i,1)  def compute_segment_scores(word, tab_seqlen, tab_pos, scores): if (len(word) == 0): return ([]) max_subword_length = len(tab_seqlen) seg_scores = [] for i in range(max_subword_length): if(i \u0026lt; len(word)): subword = word[:i+1] if subword in tab_seqlen[i]: other_scores = [] subword_score = float(tab_seqlen[i][subword][1]/(((i+1)**max_subword_length)*len(tab_seqlen[i]))) if (word[i+1:] in scores): other_scores = copy.deepcopy(scores[word[i+1:]]) else: other_scores = copy.deepcopy(compute_segment_scores(word[i+1:], tab_seqlen, tab_pos, scores)) if (len(other_scores) == 0): seg_scores.append(([subword],subword_score)) else: for j,segment in enumerate(other_scores): other_scores[j] = ([subword]+segment[0],subword_score+segment[1]) seg_scores += other_scores seg_scores = sorted(seg_scores, key=lambda item: item[1]) scores[word] = seg_scores return seg_scores  It may be noted here that the score of a segmentation candidate is calculated as the sum of the scores for all the subwords in that segmentation, where the score of subword $\\sigma_w$ is defined as\n$$ \\sigma_w = w \\times \\text{relative rank of subword in its table}$$\nHere, $w = \\left(\\frac{1}{|w|}\\right)^{\\max_w{|w|}}$. This score empirically gives subword lengths which correspond closely with the distribution of syllable lengths in English. It is a variation of the scoring scheme proposed in the original paper.\nAn analysis of the subword length frequencies obtained using this method reveals the following.\nAs expected, it produces more subwords of shorter lengths. A log-scale graph reveals further details about frequencies of longer subwords.\nFor higher lengths, LZW corresponds strongly with BPE, while unigram SR is nowhere close.\nHowever, in the actual task, the method performs worse than both BPE and unigram, and this further strengthed my belief that probabilistic sampling, while useful for MT, does not quite fit in this particular HWR dataset.\nConclusion While BPE seems like an ad-hoc technique for modeling subword units, it actually performs exceptionally well in practice. This, combined with its simplicity of implementation and low time complexity, makes it a great candidate for the task.\nHowever, I believe that if a subword model were informed by the grapheme units (for HWR), as early techniques for ASR were informed by phonemes, it might perform well on the task. This seems like an interesting direction for exploration.\n","date":1542904035,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542920287,"objectID":"011f96816d79401a09a74a58c13db380","permalink":"https://desh2608.github.io/post/subword-segmentation/","publishdate":"2018-11-22T11:27:15-05:00","relpermalink":"/post/subword-segmentation/","section":"post","summary":"Think about tasks such as machine translation (MT), automatic speech recognition (ASR), or handwriting recognition (HWR). While these appear very distinct, on abstraction they share the same pipeline wherein given an input signal, we are required to predict some text. The difference only lies in the form of the input signal - it is a piece of text, a sound wave, or a line image, in the case of MT, ASR, and HWR, respectively.","tags":["subword","machine-learning","speech-recognition"],"title":"Experiments with Subword Modeling","type":"post"},{"authors":null,"categories":[],"content":" I was trying to find a consolidated list of papers in machine learning (ICML, NIPS, AAAI, SIGIR) and natural language processing (ACL, EMNLP, NAACL) published after 2000, which are held in some regard, perhaps by winning prizes such as Test-of-time paper at these major conferences. However, there seems to be no such list, or if it is, it\u0026rsquo;s hidden too deep and it may just be quicker to prepare a similar list of my own. I will add the papers in reverse chronological order of their publication year.\n2009  A General, Abstract Model of Incremental Dialogue Processing. David Schlangen and Gabriel Skantze. EACL 2009. Honorable mention at NAACL 2018  2008  A unified architecture for natural language processing: deep neural networks with multitask learning. Ronan Collobert and Jason Weston. ICML 2008. Test-of-time award at ICML 2018\n Cheap and Fast—But is it Good?: Evaluating Non-Expert Annotations for Natural Language Tasks. Snow, O\u0026rsquo;Connor, Jurafsky, and Ng. EMNLP 2008. Honorable mention at NAACL 2018\n Modeling Local Coherence: An entity-based approach. Regina Barzilay and Mirella Lapata. Transactions of ACL (2008). Honorable mention at NAACL 2018\n  2007  Random features for large scale kernel machines. Ali Rahimi and Ben Recht. NIPS 2007. Test-of-time award at NIPS 2017\n Combining Online and Offline Knowledge in UCT. Sylvain Gelly and David Silver. ICML 2007. Test-of-time award at ICML 2017\n Pegasos: Primal estimated sub-gradient solver for SVM. Shalev-Shwartz et al. ICML 2007. Honorable mention at ICML 2017\n A Bound on the Label Complexity of Agnostic Active Learning. Steve Hanneke. ICML 2007. Honorable mention at ICML 2017\n An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems. Ehud Reiter and Anja Belz. Transactions of ACL 2009. Honorable mention at NAACL 2018\n Frustratingly Easy Domain Adaptation. Hal Daume III. ACL 2007. Honorable mention at NAACL 2018\n  2006  Dynamic topic models. David Blei and John Lafferty. ICML 2006. Test-of-time award at ICML 2016\n Improving web search ranking by incorporating user behavior information. Agichtein et al. SIGIR 2006. Test-of-time award at SIGIR 2018\n  2005  Learning to Rank Using Gradient Descent. Burges et al. ICML 2005. Test-of-time award at ICML 2015\n Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Wilson, Weibi, and Hoffman. EMNLP 2005. Honorable mention at NAACL 2018\n  2004  Multiple kernel learning, conic duality, and the SMO algorithm. Michael Jordan\u0026rsquo;s group. ICML 2004. 10 year paper award at ICML 2014\n A Linear Programming Formulation for Global Inference in Natural Language Tasks. Dan Roth and Wen-tau Yih. CoNLL 2004. Honorable mention at NAACL 2018\n Evaluating Content Selection in Summarization: The Pyramid Method. Ani Nenkova and Rebecca Passonneau. NAACL 2004. Honorable mention at NAACL 2018\n TextRank: Bringing Order into Texts. Rada Mihalcea and Paul Tarau. EMNLP 2004. Honorable mention at NAACL 2018\n Trainable sentence planning for complex information presentation in spoken dialog systems. Stent, Prasad, and Walker. ACL 2004. Honorable mention at NAACL 2018\n  2003  Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions. Zhu, Ghahramani, and Lafferty. ICML 2003. Classic paper prize at ICML 2013\n Online Convex Programming and Generalized Infinitesimal Gradient Ascent. Martin Zinkevich. ICML 2003. Classic paper prize at ICML 2013\n Anaphora and Discourse Structure. Webber et al. Computational Linguistics (2003). Honorable mention at NAACL 2018\n Minimum Error Rate Training In Statistical Machine Translation. Franz Och. ACL 2003. Honorable mention at NAACL 2018\n Probabilistic Text Structuring: Experiments with Sentence Ordering. Mirella Lapata. ACL 2003. Honorable mention at NAACL 2018\n Sentence Level Discourse Parsing using Syntactic and Lexical Information. Radu Soricut and Daniel Marcu. NAACL 2003. Honorable mention at NAACL 2018\n  2002  An Unsupervised Method for Word Sense Tagging using Parallel Corpora. Mona Diab and Philip Resnik. ACL 2002. Honorable mention at NAACL 2018\n BLEU: a Method for Automatic Evaluation of Machine Translation. Papineni et al. ACL 2002. Test-of-time award at NAACL 2018\n Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. Michael Collins. EMNLP 2002. Test-of-time award at NAACL 2018\n Thumbs up?: Sentiment Classification using Machine Learning Techniques. Pang, Lee, and Vaithyanathan. EMNLP 2002. Test-of-time award at NAACL 2018\n Unsupervised Discovery of Morphemes. Mathia Creutz and Krista Laguz. SIGPHON 2002. Honorable mention at NAACL 2018\n  2001 2000  Algorithms for non-negative matrix factorization. Daniel Lee and H. Sebastian Seung. NIPS 2000. Classic paper award at NIPS 2013\n Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers. Erin Allwein, Robert Schapire, and Yoram Singer. ICML 2000. Best 10 year paper award at ICML 2000\n PROMPT: Algorithm and Tool for Automated Ontology Merging and Alignment. Natalya Roy and Mark Musen. AAAI 2000. Classic paper award at AAAI 2018\n  Some random observations:\n NLP venues didn\u0026rsquo;t really have a classic paper section until this year\u0026rsquo;s NAACL, which is probably why so many papers were nominated. 2001 seems to have been a dismal year for NLP, with no good papers in the long run. By contrast, the community appears to have bounced back next year, with all 3 NAACL 2018 test-of-time awards given to papers from 2002. I have no idea why BLEU won. It was supposed to be an \u0026ldquo;understudy,\u0026rdquo; which is pretty clear from its name. The fact that it is still being used as an evaluation metric speaks more of a general failure to construct better metrics than of its strength. Since the papers are from before 2010, deep learning is conspicuous by its absence. In fact, Collobert and Weston\u0026rsquo;s ICML\u0026rsquo;08 paper on a unified architecture for language is the only such paper. Ali Rahimi\u0026rsquo;s \u0026ldquo;ML is alchemy\u0026rdquo; talk at NIPS\u0026rsquo;17 got a lot of attention, probably much more than his paper on random features.  Other similar lists  Best paper award winners in Computer Science  ","date":1535673464,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537624863,"objectID":"0cbc120e4369c9be223164e87962891b","permalink":"https://desh2608.github.io/post/classic-papers/","publishdate":"2018-08-30T19:57:44-04:00","relpermalink":"/post/classic-papers/","section":"post","summary":"I was trying to find a consolidated list of papers in machine learning (ICML, NIPS, AAAI, SIGIR) and natural language processing (ACL, EMNLP, NAACL) published after 2000, which are held in some regard, perhaps by winning prizes such as Test-of-time paper at these major conferences. However, there seems to be no such list, or if it is, it\u0026rsquo;s hidden too deep and it may just be quicker to prepare a similar list of my own.","tags":["machine-learning","natural-language-processing"],"title":"Award-winning classic papers in ML and NLP","type":"post"},{"authors":null,"categories":[],"content":" We have discussed several aspects of deep learning theory, ranging from optimization and generalization guarantees to role of depth and generative models. In this final post of this series, I will illustrate how theory can motivate simple solutions to problems, which can then outperform complex techniques. For this, we will consider a field where deep learning has done exceptionally well, namely, word and sentence embeddings.\nIf you need a refresher on word embeddings, I have previously explained them, along with the most popular methods, in this post. The distributional hypothesis forms the basis for all word embedding techniques used at present. Instead of naively taking the co-occurence matrix, though, almost all techniques use some low-rank approximation for the same. This gives rise to low-dimensional ($\\sim 300$) dense embeddings for text. An important question, then, is the following: How can low-dimensional embeddings represent the complex linguistic structure in text? We will first look at this question from a theoretical perspective, based on this ACL\u0026rsquo;16 paper from Arora et al.\nHow do low-dimensional embeddings approximate co-occurence matrices? Formally, we want to see why, for some low-dimensional vector representations $v$, we have\n$$ \\langle v_w,v_{w^{\\prime}} \\rangle \\approx \\text{PMI}(w,w^{\\prime}), $$\nwhere $\\text{PMI}(w,w^{\\prime})$ is the pointwise mutual information between $w$ and $w^{\\prime}$, defined as $\\log \\frac{P(w,w^{\\prime})}{P(w)P(w^{\\prime})}$, where the probabilities are computed empirically from the co-occurence matrix.\nFor this, the authors propose a generative model of language, as opposed to the usual discriminative model that is based on predicting the context words given a target word (i.e., multiclass classification). This is based on the random walk of a discourse vector $c_t \\in \\mathcal{R}^d$, which generates $t$th word in step $t$. Every word has a time-invariant latent vector $v_w \\in \\mathcal{R}^d$, and the word production model is given as\n$$ \\text{Pr}[w ~ \\text{emitted at time} ~ t|c_t] \\propto \\exp(\\langle c_t,v_w \\rangle). $$\nHere, random walk means that $c_{t+1}$ is obtained by adding a small random displacement vector to $c_t$. For a theoretic analysis, we make an isotropy assumption about the word vectors.\n Isotropy assumption: In the bulk, word vectors are distributed uniformly in the $\\mathcal{R}^d$ space.\n To generate such a dsitribution, we can just sample i.i.d from $v = s \\cdot v^{\\prime}$, where $s$ is a scalar random variable ($s \\leq \\kappa$), and $v^{\\prime}$ is obtained from a spherical Gaussian distribution. This is a simple Bayesian prior similar to the assumptions commonly used in statistics.\nLet us define $Z_c = \\sum_{w}\\exp(\\langle v_w,c \\rangle)$. This is like the normalization factor used with the above equation, but it is very difficult to compute. In the paper, the authors prove that this value is very close to some constant $Z$ for a fixed $c$. This allows us to remove this factor from consideration. Empirically, it has also been seen that some log-linear models have self-normalization properties, and this may be a reason for the observation. Let us now see how to prove this lemma.\nSince $Z_c$ is a sum of random variables, it may be tempting to use concentration inequalities to bound its value. However, we cannot do this since $Z_c$ is neither sub-Gaussian nor sub-exponential. We approach the problem it two parts. First we bound the mean and variance of $Z_c$, and then show that it is concentrated around its mean.\nPart 1: Suppose there are $n$ vectors in our space. Since they are identically distributed, we have\n$$ \\mathbb{E}[Z_c] = n\\mathbb{E}[\\exp(\\langle v_w,c \\rangle)] \\geq n\\mathbb{E}[1 + \\langle v_w,c \\rangle] = n. $$\nHere, we have used $\\mathbb{E}[\\langle v_w,c \\rangle] = 0$, since $v_w$\u0026rsquo;s are drawn from a scaled uniform spherical Gaussian. Now, suppose all the scalar variables $s_w$ are equal in distribution to $s$. Then, we can write\n$$ \\mathbb{E}[Z_c] = n\\mathbb{E}[\\exp(\\langle v_w,c \\rangle)] = n\\mathbb{E}\\left[ \\mathbb{E} [\\exp(\\langle v_w,c \\rangle)|s]\\right]. $$\nWe can compute the conditional expectation as\n$$ \\begin{align} \\mathbb{E} [\\exp(\\langle v_w,c \\rangle)|s] \u0026amp;= \\int_x \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left( -\\frac{x^2}{2\\sigma^2} \\right)\\exp(x) dx \\\\\\ \u0026amp;= \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left( -\\frac{(x-\\sigma^2)^2}{2\\sigma^2} + \\frac{\\sigma^2}{2}\\right) dx \\\\\\ \u0026amp;= \\exp(\\frac{\\sigma^2}{2}). \\end{align} $$\nHere, the standard deviation is equal to the scaling factor $s$, and so $\\sigma^2 = s^2$. It follows that\n$$ \\mathbb{E}(Z_c) = n\\exp(\\frac{s^2}{2}). $$\nSimilarly, we can show that the variance\n$$ \\mathbb{V}(Z_c) \\leq n\\mathbb{E}[\\exp(2s^2)]. $$\nSince $\\langle v_w,c \\rangle|s$ has a Gaussian distribution with variance $s^2 \\leq \\kappa^2$, we have using Chernoff bounds that\n$$ \\text{Pr}[|\\langle v_w,c \\rangle| \\geq \\eta \\log n |s] \\leq \\exp \\left( - \\frac{\\eta^2 \\log^2 n}{2\\kappa^2} \\right) = \\exp (-\\Omega(\\log^2 n)). $$\nHere we have removed $\\eta$ and $\\kappa$ since they are constants. We can now write the converse of this inequality, by taking expectation over all $s_w$, as\n$$ \\text{Pr}[|\\langle v_w,c \\rangle| \\leq \\frac{1}{2}\\log n] \\geq 1 - \\exp(-\\Omega(\\log^2 n)). $$\nThis means that, with high probability, $|\\langle v_w,c \\rangle| \\leq \\frac{1}{2}\\log n$, or equivalently, $\\exp(\\langle v_w,c \\rangle) \\leq \\sqrt{n}$. Now, let the random variable $X_w$ have the same distribution as $\\exp(\\langle v_w,c \\rangle)$ when the above holds.\nLet us take a minute to understand what we are doing here. We do not know how to bound the original $Z_c$, since $\\exp(\\langle v_w,c \\rangle)$ has no known concentration bounds. So we approximate it by a new random variable with high probability, so that we can compute bounds on the sum. Now, let $Z_{c}^{\\prime} = \\sum_{w}X_w$. We will now try to bound the mean and variance for this random variable.\nComputing the lower bound for the mean is simple since the mean of $\\exp(\\langle v_w,c \\rangle)$ is zero, and so $\\mathbb{E}[Z_c^{\\prime}] \\leq n$. We can similarly bound the variance as $\\mathbb{V}[Z_c^{\\prime}] \\leq 1.1 \\Lambda n$, where $\\Lambda$ is a constant. Now, using Bernstein\u0026rsquo;s inequality, we get\n$$ \\text{Pr}\\left[ | Z_c^{\\prime} - \\mathbb{E}[Z_c^{\\prime}] | \\geq \\epsilon n \\right] \\leq \\exp(-\\Omega(\\log^2 n)). $$\nSince $Z_c$ has the same distribution as $Z_c^{\\prime}$, the above inequality also holds for the former. This means that the probability of $Z_c$ deviating from its mean is very low, and so we can say with high probability that\n$$ (1-\\epsilon_z)Z \\leq Z_c \\leq (1+\\epsilon_z)Z. $$\nThe above proof was just to remove the normalization factor as a constant from the original problem, so that analysis becomes easier. We now come to the main result itself. Suppose $c$ and $c^{\\prime}$ are consecutive discourse vectors and $w$ and $w^{\\prime}$ are words generated from them. We have\n$$ \\begin{align} p(w,w^{\\prime}) \u0026amp;= \\mathbb{E}_{c,c^{\\prime}}[\\text{Pr}[w,w^{\\prime}|c,c^{\\prime}]] \\\\\\ \u0026amp;= \\mathbb{E}_{c,c^{\\prime}}[p(w|c)p(w^{\\prime}|c^{\\prime})] \\\\\\ \u0026amp;= \\mathbb{E}_{c,c^{\\prime}}\\left[ \\frac{\\exp(\\langle v_w,c \\rangle)}{Z_c}\\right] \\frac{\\exp(\\langle v_{w^{\\prime}},c^{\\prime} \\rangle)}{Z_{c^{\\prime}}}. \\end{align} $$\nAs proved above, we can approximate the denominators to $Z$ and take them out of the expectation. This gives\n$$ \\begin{align} p(w,w^{\\prime}) \u0026amp;= \\frac{1}{Z^2}\\mathbb{E}_{c,c^{\\prime}}[\\exp(\\langle v_w,c \\rangle)\\exp(\\langle v_{w^{\\prime}},c^{\\prime} \\rangle))] \\\\\\ \u0026amp;= \\frac{1}{Z^2}\\mathbb{E}_c [\\exp(\\langle v_w,c \\rangle)\\mathbb{E}_{c^{\\prime}|c}[\\exp(\\langle v_{w^{\\prime}},c^{\\prime} \\rangle)]]. \\end{align}. $$\nWe can compute the internal expectation term as\n$$ \\begin{align} \\mathbb{E}_{c^{\\prime}|c}[\\exp(\\langle v_{w^{\\prime}},c^{\\prime} \\rangle)] \u0026amp;= \\mathbb{E}_{c^{\\prime}|c}[\\exp(\\langle v_{w^{\\prime}},c^{\\prime} - c + c \\rangle)] \\\\\\ \u0026amp;= \\mathbb{E}_{c^{\\prime}|c}[\\exp(\\langle v_{w^{\\prime}},c^{\\prime} -c \\rangle)]\\exp(\\langle v_{w^{\\prime}},c \\rangle) \\\\\\ \u0026amp;\\approx \\exp(\\langle v_{w^{\\prime}},c \\rangle). \\end{align}$$\nHere, the last approximation can be done because we have assumed that our random walk has small steps, i.e., $|c^{\\prime} - c|$ is small. Using this in above, we get\n$$ p(w,w^{\\prime}) = \\frac{1}{Z^2}\\mathbb{E}[\\exp(\\langle v_w + v_{w^{\\prime}},c \\rangle)]. $$\nSince $c$ has uniform distribution over the sphere, the above resembles a Gaussian centered at 0 and variance $\\frac{\\lVert v_w + v_{w^{\\prime}} \\rVert^2}{d}$. Since $\\mathbb{E}[\\exp(X)] = \\exp(\\frac{\\sigma^2}{2})$ for $X \\sim \\mathcal{N}(0,\\sigma^2)$, we get the closed form expression as\n$$ p(w,w^{\\prime}) = \\frac{1}{Z^2}\\exp\\left( \\frac{\\lVert v_w + v_{w^{\\prime}} \\rVert^2}{2d} \\right), $$\nwhich is the desired result. Note that I have ignored some technicalities for error bounds in this proof. We have now shown the original result that we wanted, but how did dimensionality help?\nThe answer lies in the isotropy assumption that we made at the very beginning. Having $n$ vectors be isotropic in $d$ dimensions requires $d \u0026lt;\u0026lt; n$, which is indeed what is observed empirically. Hence, theory justifies experimental findings.\nAn algorithm for sentence embeddings In a previous part of this series, I echoed Prof. Arora\u0026rsquo;s concern that theoretical analysis at present is like a postmortem analysis, where we try to find properties of the model that can explain certain empirical findings. The ideal scenario would be where we can use this understanding to guide future learning models. In this section, I will look at this paper from ICLR\u0026rsquo;17 which uses the understanding from the previous section to build simple but strong word embeddings.\nSuppose we want to obtain the vector for a piece of text, say, a sentence. From our generative model defined in the previous section, it would be reasonable to say that this can be approximated by a max a priori (MAP) estimate of the discourse vector that generated the sentence, i.e.,\n$$ \\text{Pr}[w ~ \\text{emitted in sentence} ~ s | c_s] = \\frac{\\exp(\\langle c_s,v_w \\rangle)}{Z_{c_s}}, $$\nwhere $c_s$ is the discourse vector that remains approximately constant for the sentence. However, we need to modify this slightly to account for two real situations.\n Some words often appear out of context, and some stop words appear regardless of discourse. To approximate this, we add a term $\\alpha p(w)$ to the log-linear model, where $p(w)$ is the unigram probability of the word. This makes probability of appearance of some words high even if they have low correlation with the discourse vector. Generation of words depends not just on current sentence, but on entire history of discourse. To model this, we use discourse vector $\\tilde{c}_s = \\beta c_0 + (1-\\beta)c_s$, where $c_0$ is the common discourse vector.  Finally, the modified log-linear objective is as follows.\n$$ \\text{Pr}[w ~ \\text{emitted in sentence} ~ s | c_s] = \\alpha p(w) + (1-\\alpha) \\frac{\\exp(\\langle \\tilde{c}_s,v_w \\rangle)}{Z_{\\tilde{c}_s}} $$\nAfter the word embeddings have been trained using this objective, we can model the likelihood for obtaining sentence $s$ given discourse vector $c_s$ as\n$$ p[s|c_s] = \\prod_{w\\in s}p(w|c_s) = \\prod_{w\\in s}\\left[ \\alpha p(w) + (1-\\alpha) \\frac{\\exp(\\langle \\tilde{c}_s,v_w \\rangle)}{Z} \\right]. $$\nHere, we have taken $Z_{\\tilde{c}_s} = Z$, in accordance with the result we proved earlier. To maximize this expression, we just need to maximize the term inside the product. Taking $f_w(\\tilde{c}_s)$ to denote the term inside the product, we can easily compute its derivative, and then use Taylor expansion, $f_w(\\tilde{c}_s) = f_w(0) + \\nabla f_w(\\tilde{c}_s)^T \\tilde{c}_s$, to get an expression for $f_w(\\tilde{c}_s)$. Finally, we have\n$$ \\text{arg}\\max\\sum_{w\\in s}f_w(\\tilde{c}_s) \\propto \\sum_{w\\in s}\\frac{a}{p(w)+a}v_w, $$\nwhere $a = \\frac{1-\\alpha}{\\alpha Z}$. If we analyze this expression, this is simply a weighted sum of the word vectors in the sentence, which is one of the most common bag-of-words technique to obtain sentence embeddings. Furthermore, the weight is low if the unigram frequency of the word is high. This is similar to Tf-idf weighting of words. Now, this theory gives rise to the following algorithm, taken from the original paper.\nThis is a striking illustration of how rigorously developed theoretical results can guide construction of simple algorithms in deep learning.\nFinal note: This series was based on the ICML 2018 tutorial on \u0026ldquo;Toward a Theory for Deep Learning\u0026rdquo; by Prof. Sanjeev Arora, which is why the discussion revolved mostly around the work done by his group. The papers themselves are not very trivial to understand, but the blog posts are more beginner friendly, and highly recommended. Several people criticize deep learning for being purely intuition-based, but I believe that will change soon, given that so much good research is being done to develop a theory for it.\n","date":1533042915,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533044156,"objectID":"188749be570315bbdca48b0248510cbf","permalink":"https://desh2608.github.io/post/deep-learning-theory-5/","publishdate":"2018-07-31T18:45:15+05:30","relpermalink":"/post/deep-learning-theory-5/","section":"post","summary":"We have discussed several aspects of deep learning theory, ranging from optimization and generalization guarantees to role of depth and generative models. In this final post of this series, I will illustrate how theory can motivate simple solutions to problems, which can then outperform complex techniques. For this, we will consider a field where deep learning has done exceptionally well, namely, word and sentence embeddings.\nIf you need a refresher on word embeddings, I have previously explained them, along with the most popular methods, in this post.","tags":["deep learning","learning theory","representation learning"],"title":"Theory of Deep Learning: An Illustration with Embeddings","type":"post"},{"authors":null,"categories":[],"content":" Till now, in this series based on the ICML 2018 tutorial on \u0026ldquo;Toward a Theory for Deep Learning\u0026rdquo; by Prof. Sanjeev Arora, we have limited our discussion to the theory of supervised discriminative neural models, i.e., those models which learn the conditional probability $P(y|x)$ from a set of given $(x_i,y_i)$ samples. In particular, we saw how deep networks find good solutions, why they generalize well despite being overparametrized, and what role depth plays in all of this.\nWe now turn our attention towards the theory of unsupervised learning and generative models, with special emphasis on variational autoencoders and generative adversarial networks (GANs). But first, what is unsupervised learning?\n The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\n Evidently, unsupervised learning is much more abstract than its supervised counterpart. In the latter, our objective was essentially to find a function that approximates the original mapping of the distribution $\\mathcal{X}\\times\\mathcal{Y}$. In the unsupervised domain, there is no such objective. We are given input data, and we want to learn \u0026ldquo;structure\u0026rdquo;. The most obvious way to understand why this is more difficult is to realize that drawing a picture of a lion is much more difficult than identifying a lion in a picture.\nWhy is learning structures important? Creating large annotated datasets is an expensive task, and may even be infeasible for some problems such as parsing, which require significant domain knowledge. Let\u0026rsquo;s consider the simplest problem of image classification. The largest dataset for this problem, ImageNet, contains 14 million images, with 20000 distinct output labels. However, the number of images freely available online far exceeds 14 million, which means that we can probably learn something from them. This kind of transfer learning is the most important motivation for unsupervised learning.\nFor instance, while training a machine translation model, obtaining a parallel corpus may be difficult, but we always have access to unilateral text corpora in different languages. If we then try to learn some underlying structure present in these languages, it can assist the downstream translation task. In fact, recent advances in transfer learning for NLP have empirically proven that huge performance gains are possible using such a technique.\nRepresentation learning is perhaps the most widely studied aspect of unsupervised learning. A \u0026ldquo;good representation\u0026rdquo; often means one which disentangles factors of variation, i.e, each coordinate in the representation corresponds to one meaningful factor of variation. For example, if we consider word embeddings, an ideal vector representing a word would depict different features of the word along each dimension. However, this is easier said than done, since learning representations require an objective function, and it is still unknown how to translate these notions of \u0026ldquo;good representation\u0026rdquo; into training criteria. For this reason, representation learning is often criticized for getting too much attention for transfer learning. The essence of the criticism, taken from this post by Ferenc Huszár is this:\nIf we identified transfer learning as the primary task representation learning is supposed to solve, are we actually sure that representation learning is the way to solve it? One can argue that there may be many ways to transfer information from some dataset over to a novel task. Learning a representation and transferring that is just one approach. Meta-learning, for example, might provide another approach.\nIn the discussion so far, we have blindly assumed that the data indeed contains structures that can be learnt. This is not an oversight; it is actually based on the manifold assumption which we will discuss next.\nThe manifold assumption  A manifold is a topological space that locally resembles Euclidean space near each point.\n This means that globally, a manifold may not be a Euclidean space. The only requirement for an $n$-manifold, i.e., a manifold in $n$ dimensions, is that each point of the manifold must have a neighborhood that is homeomorphic to the Euclidean space of $n$ dimensions. There are three technicalities in this definition.\n A neighborhood of a point $p$ in $X$ is a $V \\subset X$ which contains an open set $U$ containing $p$, i.e., $p$ must be in the interior of $V$.\n A function $f: X \\rightarrow Y$ between two topological spaces $X$ and $Y$ is called a homeomorphism if it has the following properties:\n $f$ is a bijection, $f$ is continuous, $f^{-1}$ is continuous.  A Euclidean space is a topological space such that\n it is in 2 or 3 dimensions and obeys Euclidean postulates, or it is in any dimension such that points are given by coordinates and satisfy Euclidean distance.   Note that the dimension of a manifold may not always be the same as the dimension of the space in which the manifold is embedded. Dimension here simply means the degree of freedom of the underlying process that generated the manifold. As such, lines and curves, even if embedded in $\\mathbb{R}^3$, are one-dimensional manifolds.\nWith this definition in place, we can now state the manifold assumption. It hypothesizes that the intrinsic dimensionality of the data is much smaller than the ambient space in which the data is embedded. This means that if we have some data in $N$ dimensions, there must be an underlying manifold $\\mathcal{M}$ of dimension $n \u0026lt;\u0026lt; N$, from which the data is drawn based on some probability distribution $f$. The goal of unsupervised learning in most cases, is to identify such a manifold.\nIt is easy to see that the manifold assumption is, as the name suggests, just an assumption, and does not hold universally. Otherwise, applying the assumption consecutively, we would be able to represent any high-dimensional data using a one-dimensional manifold, which, of course, is not possible.\nThe task of manifold learning is modeled as approximating the joint probability density $p(x,z)$, where $x$ is the data point and $z$ is its underlying \u0026ldquo;code\u0026rdquo; on the manifold. Deep generative models have come to be accepted as the standard for estimating this probability, because of two reasons:\n Deep models promote reuse of features. We have already seen in the previous post that depth is analogous to composition whereas width is analogous to addition. Composition offers more representation capability than addition using the same number of parameters.\n Deep models are conjectured to lead to progressively more abstract features at higher levels of representation. An example of this is the commonly known phenomenon in training deep convolutional networks on image data, where it is found that the first few layers learn lines, blobs, and other local features, and higher level layers learn more abstract features. This is done explicitly using the pooling mechanism.\n  Theory of Variational Autoencoders Deep learning models often face some flak for being purely intution-based. Variational autoencoders (VAEs) are the practitioner\u0026rsquo;s answer to such criticisms, since they are rooted in the theory of Bayesian inference, and also perform well empirically. In this section, we will look at the theory that forms VAEs.\nFirst, we formalize the notion of the \u0026ldquo;code\u0026rdquo; that we mentioned earlier using the concept of a latent variable. These are those variables that are not directly observed but are inferred from the observable variables. For instance, if the model is drawing a picture of an MNIST digit, it would make sense to first have a variable choose a digit from $[0,\\ldots,9]$, and then draw the strokes corresponding to the digit.\nFormally, suppose we have a vector of latent variables $z$ in a high-dimensional space $\\mathcal{Z}$ which can be sampled using a probability distribution $P(z)$. Then, suppose we have a family of deterministic functions $f(z;\\theta)$ parametrized by $\\theta \\in \\Theta$, such that $f:\\mathcal{Z}\\times \\Theta \\rightarrow \\mathcal{X}$. The task, then, is to optimize $\\theta$ such that we can sample $z$ from $P(z)$ and with high probability, $f(z;\\theta)$ will be like the $X$\u0026rsquo;s in our dataset. As such, we can write the expression for the generated data as\n$$ X^{\\prime} = f(z;\\theta). $$\nNow, since we have no idea how to check if randomly generated images are \u0026ldquo;like\u0026rdquo; our dataset, we use the notion of \u0026ldquo;maximum likelihood\u0026rdquo;, i.e., if the model is likely to produce training set samples, then it is also likely to produce similar samples and unlikely to produce dissimilar ones. With this assumption, we want to maximize the probability of each $X$ in the training process. We can now replace $f(z;\\theta)$ by the conditional probability $P(X|z;\\theta)$, and we get\n$$ P(X) = \\int P(X|z;\\theta)P(z)dz. $$\nIn VAEs, we usually have $P(X|z;\\theta) = \\mathcal{N}(X|f(z;\\theta),\\sigma^2 I)$, which is a Gaussian. Using this formalism, we can use gradient descent to increase $P(X)$ by making $f(z;\\theta)$ approach $X$ for some $z$. So essentially, VAEs do the following steps:\n Sample $z$ from some known distribution. Feed $z$ into some parametrized function to get $X$. Tune the parameters of the function such that generated $X$ resemble those in dataset.  In this process, two questions arise:\nHow do we define $z$?\nVAEs simply sample $z$ from $\\mathcal{N}(0,I)$, where $I$ is the identity matrix. The motivation for this choice is that any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function. I do not prove this here, but the proof is based on taking the composition of the inverse cumulative distribution function (CDF) of the desired distribution with the CDF of a Gaussian.\nHow do we deal with $\\int dz$?\nWe need to understand that the space $\\mathcal{Z}$ is very large, and there are only few $z$ which generate realistic $X$, which makes it very difficult to sample \u0026ldquo;good\u0026rdquo; values of $z$ from $P(z)$ . Suppose we have a function $Q(z|X)$ which, given some $X$, gives a distribution over $z$ values that are likely to produce $X$. Now to compute $P(X)$, we need to:\n relate $P(X)$ with $\\mathbb{E}_{z\\sim Q}P(X|z)$, and estimate $\\mathbb{E}_{z\\sim Q}P(X|z)$.  For the first, we use KL-divergence (that we saw in the previous post) between the probability distribution estimated by $Q$ to the actual conditional probability distribution as follows.\n$$ \\begin{align} \u0026amp; \\mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \\mathbb{E}_{z\\sim Q}[\\log Q(z|X) - \\log P(z|X)] \\\\\\ \u0026amp;= \\mathbb{E}_{z\\sim Q}\\left[ \\log Q(z|X) - \\log \\frac{P(X|z)P(z)}{P(X)} \\right] \\\\\\ \u0026amp;= \\mathbb{E}_{z\\sim Q} [ \\log Q(z|X) - \\log P(X|z) - \\log P(z) ] + \\log P(X) \\\\\\ \\Rightarrow \u0026amp; \\log P(X) - \\mathcal{D}_{KL}[Q(z|X)||P(z|X)] = \\mathbb{E}_{z\\sim Q}[\\log P(X|z)] - \\mathcal{D}_{KL}[Q(z|X)||P(z)] \\end{align} $$\nIn the LHS of the above equation, we have an expression that we want to maximize, since we want $P(X)$ to be large and we want $Q$ to approximate the conditional probability distribution (this was our objective of using KL-divergence). If we use a sufficiently high-capacity model for $Q$, the $\\mathcal{D}_{KL}$ term will approximate $0$, in which case we will directly be optimizing $P(X)$.\nNow we are just left with finding some way to optimize the RHS in the equation. For this, we will have to choose some model for $Q$. An obvious (and usual) choice is to take the multivariate Gaussian, i.e., $Q(z|X) = \\mathcal{N}(z|\\mu(X),\\Sigma(X))$. Since $P(z) = \\mathcal{N}(0,I)$, the KL-divergence term on the RHS can now be written as\n$$ \\mathcal{D}_{KL}[\\mathcal{N}(\\mu(X),\\sum(X))||\\mathcal{N}(0,I)] = \\frac{1}{2}\\left( \\text{tr}(\\Sigma(X)) + (\\mu(X))^T (\\mu(X)) - k - \\log \\text{det}(\\Sigma(X)) \\right). $$\nTo estimate the first term on the RHS, we just compute the term for one sample of $z$, instead of iterating over several samples. This is because during stochastic gradient descent, different values of $X$ will automatically require us to sample $z$ several times. With this approximation, the optimization objective for a single sample $X$ becomes\n$$ J = \\log P(X|z) - \\mathcal{D}_{KL}[Q(z|X)||P(z)]. $$\nThis can be represented in the form of a feedforward network by the figure on the left below.\nThere is, however, a caveat. The network is not trainable using backpropagation because the red box is a stochastic step, which means that it is not differentiable. To solve this problem, we use the reparametrization trick as follows.\n$$ z = \\mu(X) + \\Sigma^{\\frac{1}{2}}(X) \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0,I) $$\nAfter this trick, we get the final network as shown in the right in the above figure. Furthermore, we must have $\\mathcal{D}_{KL}[Q(z|X)||P(z|X)]$ approximately equal $0$ in the LHS. Since we have taken $Q$ to be a Gaussian, this means that the original density function $f$ should be such that $P(z|X)$ is a Gaussian. It turns out that such a function, which maximizes $P(X)$ and satisfies the said criteria, provably exists.\nAlthough VAEs have strong theoretical support, they do not work very well in practice, especially in problems such as face generation. This is because the loss function used for training is log-likelihood, which ultimately leads to fuzzy face images which have high match with several $X$. Instead of using likelihood, we use the power of discriminative deep learning, which is where GANs come into the picture.\nGenerative adversarial networks: new insights GANs were proposed in 2014, and have become immensely popular in computer vision ever since. They are basically motivated from game theory, and I will not get into the details here since the tutorial by Ian Goodfellow is a excellent resource for the same.\nSince the prior learnt by the generator depends upon the discriminative process, an important issue with GANs is that of mode collapse. The problem is that since the discriminator only learns from a few samples, it may be unable to teach the generator to produce $\\mathcal{P}_{synth}$ with sufficiently large diversity. In the context of what we have already seen, this can be taken as the problem of generalization for GANs.\nIn this section, I will discuss three results from two important papers from Arora et al. which deal with mode collapse in GANs.\n Generalization and equilibrium in generative adversarial nets Do GANs learn the distribution? Some theory and empirics  For all our discussions in this section, we will consider the Wasserstein GAN objective instead of the usual minimax objective, which is as follows (and arguably more intuitive)\n$$ J = \\lvert \\mathbb{E}_{x\\in \\mathcal{P}_{real}}[D(x)] - \\mathbb{E}_{x\\in \\mathcal{P}_{synth}}[D(x)] \\rvert, $$\nwhere $D$ is the discriminator.\n1. Generalization depends on discriminator size  If the discriminator size is $n$, then there exists a generator supported on $\\mathcal{O}(n\\log n)$ images, which wins against all possible discriminators.\n This means that if we have a discriminator of size $n$, then the best possible generator training is possible using $Cn/\\epsilon^2 \\log n$ images from the full training set. Any more images will improve the training objective by at most $\\epsilon$. I will now give the proof (simplified from the actual proof in the paper).\nProof: Suppose $\\mu$ denotes the actual distribution learnt by the generator and $\\nu$ denotes the actual distribution of real images that the discriminator has access to. Let $\\tilde{\\mu}$ and $\\tilde{\\nu}$ be the empirical versions of the above distributions, i.e., the distributions that we actually use for training. Let $d(p,q)$ be some distance measure between the two distributions.\nIn the paper, the authors have defined an $\\mathcal{F}$-distance that has good generalization properties, but I will not get into the details of that here for sake of simplicity. For this discussion, just assume that the distance measure is $d$. From my earlier post on generalization error in supervised learning, we say that a model generalizes well when, for some $\\epsilon$,\n$$ |\\text{True error} - \\text{Empirical error}| \\leq \\epsilon. $$\nHere, we don\u0026rsquo;t really know the error, but we can use our distance measure to the same effect. If the size of discriminator is $p$, we want to compute the sample complexity $m$ in terms of $p$ and $\\epsilon$ such that the GAN generalizes. For that, we need a few approximations.\nFirst we approximate the parameter space $\\mathcal{V}$ using its $\\frac{\\epsilon}{8}$-net $\\mathcal{X}$. This means that for every $\\nu \\in \\mathcal{V}$, we can find a $\\nu^{\\prime}\\in \\mathcal{X}$ which is at a distance of at most $\\frac{\\epsilon}{8}$ from it. Assuming that the function computed by the discriminator $D$ is 1-Lipschitz, we can then say that $\\lvert \\mathbb{E}_{x\\sim \\mu}[D_{\\nu}(x)] - \\mathbb{E}_{x\\sim \\mu}[D_{\\nu^{\\prime}}(x)] \\rvert \\leq \\frac{\\epsilon}{8}$.\nThe $\\epsilon$-net is taken so that we can apply concentration inequalities in this continuous finite space. You can read more about them here. Now, we can use Hoeffding\u0026rsquo;s inequality to bound the difference between true and empirical errors on this space as\n$$ P\\left[ $\\lvert \\mathbb{E}_{x\\sim \\mu}[D_{\\nu}(x)] - \\mathbb{E}_{x\\sim \\tilde{\\mu}}[D_{\\nu}(x)] \\rvert \\geq \\frac{\\epsilon}{4} \\right] \\leq 2\\exp \\left( -\\frac{\\epsilon^2 m}{2} \\right). $$\nTaking union bound over all $p$ parameters, we get that when $m \\geq \\frac{Cp\\log (p/\\epsilon)}{\\epsilon^2}$, then the bound holds with high probability. Note that this sample complexity is $m = \\mathcal{p\\log p}$, which is what we wanted. Now we just need to show that this bound implies that the generalization error is bounded. Since we have taken the $\\frac{\\epsilon}{8}$-net approximation, we translate both the parameters in $\\mathcal{X}$ back to $\\mathcal{V}$, paying a cost of $\\frac{\\epsilon}{8}$ for each. Finally, we get, for every $D$,\n$$ \\lvert \\mathbb{E}_{x\\sim \\mu}[D_{\\nu}(x)] - \\mathbb{E}_{x\\sim \\tilde{\\mu}}[D_{\\nu}(x)] \\rvert \\leq \\frac{\\epsilon}{2}. $$\nWe can prove a similar upper bound for $\\nu$. Finally, with similar approximation arguments, and from the definition of our distance function, we get the desired result.\n2. Existence of equilibrium For GANs to be successful, they must find an equilibrium in the G-D game where the generator wins. In the context of the minimax equation, this means that switching min and max in the objective should not cause any change in the equilibrium. In the paper, the authors prove an $\\epsilon$-approximate equilibrium, i.e., one where such a switching affects the expression by at most $\\epsilon$.\n If a generator net is able to generate a Gaussian distribution, then there exists an $\\epsilon$-approximate equilibrium where the generator has capacity $\\mathcal{O}(n\\log n / \\epsilon^2)$.\n The proof of this result lies in a classical result in statistics, which says that any probability distribution can be approximated by a mixture of infinite Gaussians. For this, we just need to take the standard Gaussian $P(x)\\mathcal{N}(x,\\sigma^2)$ at every $x \\in \\mathcal{X}$ such that $\\sigma^2 \\rightarrow 0$, and take the mixture of all such Gaussians. The remaining proof is similar to the one done for the previous result, so I will not repeat it here.\n3. Empirically detecting mode collapse We have already seen that GAN training can be successful even if the generator has not learnt a good enough distribution, if the discriminator is small. But suppose we take a really large discriminator and then train our GAN to a minima. How do we still make sure that the generator distribution is good? It could well be the case that the generator has simply memorized the training data, due to which the discriminator cannot make a better guess than random. Researchers have proposed several qualitative checks to test this:\n Check the similarity of each generated image to the nearest image in the training set. If the seed formed by interpolating two seeds $s_1$ and $s_2$ that produce realistic images, also produces realistic images, then the learnt distribution probably has many realistic images. Check for semantically important directions in latent space, which cause predictable changes in generated image.  We will now see a new empirical measure for the support size of the trained distribution, based on the Birthday Paradox.\n The birthday paradox\nIn a room of just 23 people, there\u0026rsquo;s a 50% chance of finding 2 people who share their birthday.\n To see why, refer to this post. It is a simple problem of permutation and combination, followed by using the approximation for $e^x$.\nSince $23 \\approx \\sqrt{365}$, we can generalize this to mean that if a distribution has support $N$, we are likely to find a duplicate in a batch of about $\\sqrt{N}$ samples taken from this distribution. As such, finding the smallest batch size $s$ which ensures duplicate images with good probability almost guarantees that the distribution has support $s^2$. Let us formalize this guarantee.\nSuppose we have a probability distribution $P$ on a set $\\Omega$. Also, let $S \\subset \\Omega$ such that $\\sum_{s\\in S}P(s)\\geq \\rho$ and $|S|=N$. Then from calculations similar to the one done for birthday paradox, we can say that the probability of finding at least one collision on drawing $M$ i.i.d samples is at least $1 - \\exp\\left( -\\frac{(M^2 - M)\\rho}{2N} \\right)$.\nNow, suppose we have empirically found this minimum probability of collision to be $\\gamma$. Then it can be shown that under realistic assumptions on parameters, the following holds:\n$$ N \\leq \\frac{2M\\rho^2}{\\left(-3 + \\sqrt{9+\\frac{24}{M}\\log \\frac{1}{1-\\gamma}}\\right)-2M(1-\\rho)^2} $$\nThis gives an upper bound on the support size of the distribution learned by the generator.\nGenerative models are definitely very promising, especially with the recent interest in transfer learning with unsupervised pretraining. While I have tried to explain the recent insights into GANs as best as possible, it is not possible to explain every detail in the proof in an overview post. Even so, I hope I have been able to at least give a flavor of how veterans in the field approach theoretical guarantees.\n","date":1532974177,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532975373,"objectID":"2785fb25e39cc85c8783354497f41062","permalink":"https://desh2608.github.io/post/deep-learning-theory-4/","publishdate":"2018-07-30T23:39:37+05:30","relpermalink":"/post/deep-learning-theory-4/","section":"post","summary":"Till now, in this series based on the ICML 2018 tutorial on \u0026ldquo;Toward a Theory for Deep Learning\u0026rdquo; by Prof. Sanjeev Arora, we have limited our discussion to the theory of supervised discriminative neural models, i.e., those models which learn the conditional probability $P(y|x)$ from a set of given $(x_i,y_i)$ samples. In particular, we saw how deep networks find good solutions, why they generalize well despite being overparametrized, and what role depth plays in all of this.","tags":["deep learning","learning theory"],"title":"Theory of Deep Learning: Generative Models","type":"post"},{"authors":null,"categories":[],"content":" In the previous posts of this series, we have looked at how stochastic gradient descent is able to find a good solution despite the nonconvex objective, and why overparametrized neural networks generalize so well. In this post, we will look at the titular property of deep networks, namely depth, and what role they play in the learning ability of the model.\nAn ideal result in this regard would be if we can show that there exists a class of natural learning problems (recall the idea of a \u0026ldquo;natural\u0026rdquo; problem from the first post) which cannot be solved with depth $d$ neural networks, but are solvable with at least one model of depth $d+1$. However, such a result is elusive at present, since we have already established that there exists no mathematical formulation of a \u0026ldquo;natural\u0026rdquo; learning problem.\nHowever, there has been some advancement in establishing similar results in the case of less natural problems, and in this regard, the following papers are worth mentioning.\n The Power of Depth for Feedforward Neural Networks by Eldan and Shamir Benefit of Depth in Neural Networks by Telgarsky  I will now discuss both of these in some detail.\nRole of depth for \u0026ldquo;less natural\u0026rdquo; problems 1. Approximating radial functions At the outset, note that if we allow the neural network to be unbounded, i.e., have exponential width, even a 2-layer network can approximate any continuous function. As such, for our study, we only use \u0026ldquo;bounded\u0026rdquo; networks where the number of hidden layer units cannot be exponential in the dimension of input. With this understanding, we will look at the simplest case: we try to find a function (or a family of functions) that are expressible by a 3-layer network but cannot be expressed by any 2-layer network. Before we get into the details, we first look at what a 2-layer and 3-layer networks represent.\nA 2-layer network represents the following function:\n$$ f_2(\\mathbf{x}) = \\sum_{i=1}^w v_i \\sigma(\u0026lt; \\mathbf{w}_i,\\mathbf{x} \u0026gt;+b_i), $$\nand a 3-layer network represents the following:\n$$ f_3(\\mathbf{x}) = \\sum_{i=1}^w u_i \\sigma\\left( \\sum_{j=1}^w v_{i,j} \\sigma (\u0026lt; \\mathbf{w}_{i,j},\\mathbf{x} \u0026gt;+ b_{i,j}) + c_i \\right). $$\nHere, $w$ is the size of the hidden layer and $\\sigma$ is an activation function. The only constraint on $\\sigma$ is that it should be \u0026ldquo;universal\u0026rdquo;, i.e., a 2-layer network should be able to approximate any Lipschitz function that is non-constant on a bounded domain for some $w$ (which need not be bounded). This constraint is satisfied by all the standard activation functions such as sigmoid and ReLU.\nUnder this assumption, the main result in the paper is as follows:\n There exists a radial function $g$ depending only on the norm of the input, which is expressible by a 3-layer network of width polynomial in the input dimension, but not by any 2-layer neural network.\n More importantly, apart from the universal assumption, this result does not depend on any characteristic of $\\sigma$. Furthermore, there are no constraints on the size of the parameters $\\mathbf{w}$. The only constraint worth noting is that $g$ must be a radial function.\nTo prove this result, we need to show 2 things:\n $g$ can be approximated by a 3-layer neural network. $g$ cannot be approximated by any 2-layer network of bounded width.  Part 1: This is trivial to show, since any radial function can be approximated by a 3-layer network. To do this, we compute the Euclidean norm $\\lVert \\mathbf{x} \\rVert^2$ from the input $\\mathbf{x}$ in the first layer using a linear combination of neurons. This is possible because the squared norm is just the sum of squares of all the components, and each squared component can be approximated in a finite range, for example, using the step function.\nOnce the norm is computed, the second layer can be used to approximate the required radial function using RBF nodes. This completes the construction.\nPart 2: Let the input be taken from a probability distribution $\\mu$, which has a density function $\\phi^2(x)$, for some known function $\\phi$. Suppose we are trying to approximate a function $f$ using a function $g$. Then, the distance between the functions can be given as\n$$ \\begin{align} \\mathbb{E}_{\\mu}(f(x)-g(x))^2 \u0026amp;= \\int (f(x)-g(x))^2 \\phi^2(x) dx \\\\\\ \u0026amp;= \\int (f(x)\\phi (x) - g(x)\\phi (x))^2 dx \\\\\\ \u0026amp;= \\lVert f\\phi - g\\phi \\rVert_{L_2}^2 \\end{align} $$\nNow, we can replace $f\\phi$ and $g\\phi$ with their respective Fourier transforms since the Fourier transform is an isometric mapping (i.e., distance remains same before and after the mapping). Therefore, we get\n$$ \\mathbb{E}_{\\mu}(f(x)-g(x))^2 = \\lVert \\hat{f\\phi} - \\hat{g\\phi} \\rVert_{L_2}^2 $$\nWhile this replacement may seem arbitrary at first, it has a very clear motivation. We have done this because the Fourier transform of functions expressible by a 2-layer network has a very particular form, which we will use here. Specifically, consider the function\n$$ f(x) = \\sum_{i=1}^k f_i (\u0026lt; \\mathbf{v}_i,\\mathbf{x} \u0026gt;), $$\nwhich is expressible by any 2-layer network. The component function $f_i (\u0026lt; \\mathbf{v}_i,\\mathbf{x} \u0026gt;)$ is constant in any direction perpendicular to $\\mathbf{v}_i$ and so its Fourier transform is non-zero only in the direction of $\\mathbf{v}_i$, and so the whole distribution is supported on $\\bigcup_i \\text{span}(\\mathbf{v}_i)$. Now we just need to compute the support of $\\hat{\\phi}$, and then we can directly use the convolution-multiplication principle.\nSince we haven\u0026rsquo;t yet chosen a density function, we choose $\\phi$ to make the computation of support easier. Specifically, we choose $\\phi$ to be the inverse Fourier transform of $\\mathbb{1}\\{x\\in B\\}$, which is the indicator function of a unit Euclidean ball. Then, $\\hat{\\phi}$ becomes $\\mathbb{1}\\{x\\in B\\}$ itself, and its support is simply the ball $B$. Using these, we get\n$$ \\text{Supp}(\\hat{f\\phi}) \\subseteq T = \\bigcup_{i=1}^k (\\text{span}\\{ \\mathbf{v}_i \\} + B ), $$\nwhich is basically the union of $k$ tubes passing through origin. This is because $\\text{span}\\{\\mathbf{v}_i\\}$ is just a straight line, and $B$ is a ball. Sum here means the direct sum, i.e., for every element $a \\in A$ and $b \\in B$, form a set of $a+b$. So we just put the Euclidean ball on every point on the line, which gives us a cylinder passing through the origin.\nSince we are looking for a $g$ which cannot be approximated by the neural network, we try to make $\\lVert f\\phi - g\\phi \\rVert_{L_2}^2$ as large as possible. We have already seen what the support of $\\hat{f\\phi}$ looks like. Now, we want a $g$ such that\n $g$ should have most of its mass away from the origin in all the directions, and the Fourier transform $\\hat{g}$ should be outside $B$.  If $g$ is chosen as a radial function, the first criteria will be satisfied if we just put large mass away from the origin in one direction. To satisfy the second criteria, $g$ should have a high-frequency component. To see why, see the following figure which shows the sine curve and its Fourier transform.\nThe only thing that remains to be shown is that if $\\hat{g}$ contains a significant mass away from the origin, then so does $\\hat{g\\phi}$. But this proof is somewhat technical in nature and I avoid it here for sake of simplicity.\nThis completes our proof for the result given in the paper. While this result is an important step in quantifying the role of depth in a neural network, it is still limited in that it only holds for radial functions. This is what I meant earlier by \u0026ldquo;less natural\u0026rdquo; problems, since in most of the common learning problems, the $(x,y)$ pairs are not generated from a simple radial distribution, and are much more complex in nature.\n2. Exponential separation between shallow and deep nets In the proof of the previous result, the key idea was to have a high-frequency component in the function required to be approximated. This means that the function was highly oscillatory. In this paper as well, a similar oscillation argument is used to prove another important result.\n For every positive integer $k$, there exists neural networks with $\\theta(k^3)$ layers, $\\theta(1)$ nodes per layer, and $\\theta(1)$ distinct parameters, which cannot be approximated by networks with $\\mathcal{O}(k)$ layers and $o(2^k)$ nodes.\n This result is proven using three steps.\n Functions with few oscillations poorly approximate functions with many oscillations. Functions computed by networks with few layers must have few oscillations. Functions computed by networks with many layers can have many oscillations.  Approximation via oscillation counting We will first look at a metric to count oscillations of a function. For this, consider the following graph which shows functions $f$ and $g$ which are defined from $\\mathbb{R}$ to $[0,1]$.\nHere, the horizontal line denotes $y = \\frac{1}{2}$. The classifiers $\\tilde{f}$ and $\\tilde{g}$ obtained from $f$ and $g$ perform binary classification according to the rule $\\tilde{f}(x) = \\mathbb{1}[f(x)\\geq \\frac{1}{2}]$. Let $\\mathcal{I}_f$ denote the set of partitions of $\\mathbb{R}$ into intervals so that the classifier $\\tilde{f}$ is constant in each interval. Then, the crossing number is defined as\n$$ \\text{Cr}(f) = |\\mathcal{I}_f|. $$\nFrom our definition of $\\tilde{f}$, this clearly means that $\\text{Cr}(f)$ counts the number of times that $f$ crosses the line $y = \\frac{1}{2}$, and hence the name. In this way, we formalize the notion of counting the number of oscillations of a function.\nWith this definition, if $\\text{Cr}(f)$ is much larger than $\\text{Cr}(g)$, then most piecewise constant regions of $\\tilde{g}$ will exhibit many oscillations of $f$, and thus $g$ poorly approximates $f$.\nNow we will prove the following lemma, where the counting number $\\text{Cr}(f)$ is denoted by $s_f$ for sake of convenience.\n $$ \\frac{\\text{No. of regions of }\\mathcal{I}_f \\text{ where} ~ \\tilde{f}\\neq \\tilde{g}}{s_f} \\geq \\frac{1}{2} - \\frac{s_g}{s_f} $$\n Now, if $s_f \u0026gt;\u0026gt; s_g$, then the RHS approximately becomes $\\frac{1}{2}$, which implies that for more than half of all the regions of $f$, $\\tilde{g}$ classifies $x$ incorrectly, and so $g$ is a poor approximation of $f$.\nProof: We choose a region $J$ where $\\tilde{g}$ is constant but $\\tilde{f}$ alternates, such as the region where $g$ is red in the above figure. We denote by $X_J$ all the partitions of $\\mathcal{I}_f$ that are contained in $J$. Since $f$ oscillates within $g$, this means that $\\tilde{g}$ disagrees with $\\tilde{f}$ for half of all $X_J$, i.e., at least $\\frac{|X_J|-1}{2}$ in general.\nIn the LHS of the claim, we need to count all the regions of $\\mathcal{I}_f$ where the classifiers disagree for all points in the region. From above, we have a lower bound on the number of such regions within one $J$. So now we just take sum over all $J \\in \\mathcal{I}_g$ to get\n$$ \\frac{\\text{No. of regions of }\\mathcal{I}_f \\text{ where} ~ \\tilde{f}\\neq \\tilde{g}}{s_f} \\geq \\frac{1}{s_f}\\sum_{J \\in \\mathcal{I}_g} \\frac{|X_J|-1}{2}. $$\nNow we need to bound $s_f$. For this, see that the total number of oscillations of $f$ are at least its number of oscillations within a single partition of $\\mathcal{I}_g$ summed over all such partitions. I say \u0026ldquo;at least\u0026rdquo; because this will not include those partitions of $\\mathcal{I}_f$ whose interior intersects with the boundary of an interval in $\\mathcal{I}_g$. At most, there would be $s_g$ such partitions, and so\n$$ s_f \\leq s_g + \\sum_{J\\in \\mathcal{I}_g}|X_J|. $$\nThis means that $\\sum_{J\\in \\mathcal{I}_g}|X_J| \\geq s_f - s_g$. Using this bound in the previously obtained inequality, we get the desired result.\nFew layers, few oscillations Adding more nodes is similar to adding polynomials, while adding layers is like composition of polynomials. Adding polynomials yields a new polynomial with degree equal to the higher of the two and at most twice as many terms, but composing them (i.e. taking product) would yield a polynomial with higher degree and more than the product of terms. Clearly, composition would lead to more number of roots of the new polynomial. This suggests that adding layers should lead to a higher number of oscillations than adding nodes.\nLet $f$ be a function computed by the neural network $\\mathcal{N}((m_i,t_i,\\alpha_i,\\beta_i)_{i=1}^l)$, i.e. a network of $l$ layers where the $i$th layer has $m_i$ nodes, such that the activation function at each node is $(t,\\alpha)-poly$ (a piecewise function containing $t$ parts where each piece is a polynomial of degree at most $\\alpha$). Then, we claim that\n$$ \\text{Cr}(f) \\leq \\mathcal{O}\\left( \\left( \\frac{tm\\alpha}{l} \\right)^l \\beta^{l^2} \\right). $$\nProof: We will prove this in two parts. First, we bound the counting number of a $(t,\\alpha)-poly$ function, and then we will show that the function $f$ as computed by the above network is $(t,\\alpha)-poly$.\nFor the first part, see that each piece of the function $f$ is a polynomial of degree at most $\\alpha$, which means that each piece oscillates at most $1 + \\alpha$ times. Since there are $t$ such pieces\n$$ \\text{Cr}(f) \\leq t(1+\\alpha). $$\nNow it remains to show that the function $f$ computed by the network is indeed $(t,\\alpha)-poly$. To see this, consider the function computed by a single layer. Each node in the layer computes a $(t,\\alpha)-poly$ function, say $g_i$, and we apply a composition function, say $f$, on these $g_i$\u0026rsquo;s, which is a polynomial with degree at most $\\gamma$. The final function computed by this layer is\n$$ h(x) = f(g_1(x),\\ldots,g_k(x)). $$\nTo visualize such a composition, consider the following figure.\nHere, each horizontal line denotes one node\u0026rsquo;s partition function, i.e., $\\tilde{g_i}$. There are $k$ such lines with at most $t$ intervals each. The composition takes the union of all the partitions of all these lines. As such, the maximum number of intervals after composition will be equal to $kt$. Within each such interval, since we are taking a composition of a degree $\\gamma$ polynomial with one with degree at most $\\alpha$, the resulting polynomial has degree at most $\\alpha \\gamma$. Hence, $h$ is $(tk,\\alpha\\gamma)-poly$.\nSince there are $l$ layers and the total number of nodes in the network is $m$, it implies there are $\\frac{m}{l}$ nodes on average in each layer, and each node has at most $t$ intervals. So after every layer, the number of intervals gets multiplied by a factor of $\\frac{mt}{l}$. Finally, the total number of intervals will be of the order $\\left(\\frac{mt}{l}\\right)^l$.\nSimilarly, the degree of resulting function gets multiplied by $\\alpha$ after every layer, so the final degree is of the order $\\alpha^l$. Using the result shown in the first part, the resulting function will have a counting number bounded by $\\mathcal{O}\\left(\\frac{tm\\alpha}{l} \\right)^l$.\nThe $\\beta$ term comes due to technicalities associated with taking an activation function which is semi-algebraic rather than piecewise polynomial, but the proof technique remains the same.\nMany layers, many oscillations In the figure that I showed for explaining counting number, notice that oscillations usually (always?) mean repetitions of a triangle-like function (strictly increasing till some point and then strictly decreasing thereafter). Also, the usual functions computed by a single layer of most of the common neural networks are like these triangular functions.\nIn the last result, we used the composition of $(t,\\alpha)-poly$ functions across several layers to bound the counting number of a network. Similarly in this section, we will use the concept of a $(t,[a,b])$-triangle. It represents a function which is continuous in $[a,b]$ and consists of $t$ triangle-like pieces. Also, since this function oscillates $2t$ times, its counting number is $2t+1$.\nNow it remains to show that the composition of 2 such functions gives a similar function (which is a similar technique to what we used earlier). More formally, we will prove this claim.\n Claim: If $f$ is a $(s,[0,1])$-triangle and $g$ is a $(t,[0,1])$-triangle, then $f \\circ g$ is a $(2st,[0,1])$-triangle.\n Proof: First, we note that $f \\circ g$ is continuous in $[0,1]$ since a composition of continuous functions is continuous in the same domain.\nNow, consider any odd (i.e., strictly increasing) interval $g_j$ of $g$. Suppose $(a_1,\\ldots,a_{2s+1})$ are the interval boundaries of $f$. Since the range of $g_j$ is $[0,1]$, $g_j^{-1}(a_i)$ exists for all $i$ and is unique, since $g_j$ is strictly increasing. Let $a_i^{\\prime}=g_j^{-1}(a_i)$, i.e., $g_j(a_i^{\\prime})=a_i$. If $i$ is odd, the composition $f \\circ g_j(a_i^{\\prime}) = f(a_i)=0$, and $f \\circ g_j$ is strictly increasing in $[a_i^{\\prime},a_{i+1}^{\\prime}]$, since $g_j$ is strictly increasing everywhere and $f$ is strictly increasing in $[a_i,a_{i+1}]$. By a similar argument, if $i$ is even, $f \\circ g_j$ is strictly decreasing along $[a_i^{\\prime},a_{i+1}^{\\prime}]$. In this way, we get $2s$ triangular pieces for a single $g_j$, and so the overall composition $f \\circ g$ has $2st$ triangular pieces.\nHaving shown this, it is easy to see that if there are $l$ layers and each layer computes a $(t,[0,1])$-triangle, the final layer will output a $((2t)^l,[0,1])$-triangle. In this way, the counting number of the overall function becomes $(2t)^l + 1$.\nImplicit acceleration by overparametrization In the previous section, we have seen some results which show that depth plays a role in the expressive capacity of neural networks. Specifically, we saw that:\n Radial functions can be approximated by depth-3 networks but not with depth-2 networks. Functions expressible by $\\theta(k^3)$-depth networks of constant width cannot be approximated by $\\mathcal{O}(k)$-depth networks with polynomial width.  In this section, we will look at a new paper from Arora, Cohen, and Hazan that suggests that, sometimes, increasing depth can speed up optimization (which is rather counterintuitive given the consensus on expressiveness vs. optimization trade-off), i.e., depth plays some role in convergence. Furthermore, this acceleration is more than what could be obtained by commonly used techniques, and is theoretically shown to be a combination of momentum and adaptive regularization (which we will discuss later).\nTo isloate convergence from expressiveness, the authors focus solely on linear neural networks, where increasing depth has no impact on the expressiveness of the network. This is because in such networks, adding layers manifests itself only in the replacement of a matrix parameter by a product of matrices – an overparameterization.\nEquivalence to adaptive learning rate and momentum The first result that we prove is the following.\n Overparametrized gradient descent with small learning rate and near-zero initialization is equivalent to GD with adaptive learning rate and momentum terms.\n Proof: This can be seen by simple analysis of gradients for an $l_p$-regression with parameter $\\mathbf{w}\\in \\mathbb{R}^d$. The loss function can be given as\n$$ L(\\mathbf{w}) = \\mathbb{E}_{(\\mathbf{x},y)\\sim S}\\left[ \\frac{1}{p}(\\mathbf{x}^T\\mathbf{w} - y)^p \\right]. $$\nNow, if we add a scalar parameter, the new parameters are $\\mathbf{w}_1$ and $w_2 \\in \\mathbb{R}$, i.e., $\\mathbf{w} = w_2 \\mathbf{w}_1$, and we can write the new loss function as\n$$ L(\\mathbf{w}_1,w_2) = \\mathbb{E}_{(\\mathbf{x},y)\\sim S}\\left[ \\frac{1}{p}(\\mathbf{x}^T\\mathbf{w}_1 w_2 - y)^p \\right]. $$\nWe can now compute the gradients of the objective with respect to the parameters as\n$$ \\nabla_{\\mathbf{w}} = \\mathbb{E}_{(\\mathbf{x},y)\\sim S}\\left[ (\\mathbf{x}^T\\mathbf{w} - y)^{p-1}\\mathbf{x} \\right] $$\n$$ \\nabla_{\\mathbf{w}_1} = \\mathbb{E}_{(\\mathbf{x},y)\\sim S}\\left[ (\\mathbf{x}^T\\mathbf{w}_1 w_2 - y)^{p-1}w_2\\mathbf{x} \\right] = w_2 \\nabla_{\\mathbf{w}} $$\n$$ \\nabla_{w_2} = \\mathbb{E}_{(\\mathbf{x},y)\\sim S}\\left[ (\\mathbf{x}^T\\mathbf{w} - y)^{p-1}\\mathbf{w}_1^T \\mathbf{x} \\right] $$\nThe update rules for $\\mathbf{w}_1$ and $w_2$ can be given as\n$$ \\mathbf{w}_1^{(t+1)} = \\mathbf{w}_1^{(t)} - \\eta \\nabla_{\\mathbf{w}_1}^{(t)} \\quad \\text{and} \\quad w_2^{(t+1)} = w_2^{(t)} - \\eta \\nabla_{w_2}^{(t)}, $$\nand the updated parameter $\\mathbf{w}$ is\n$$ \\begin{align} \\mathbf{w}^{(t+1)} \u0026amp;= \\mathbf{w}_1^{(t+1)} w_2^{(t)} \\\\\\ \u0026amp;= \\left( \\mathbf{w}_1^{(t)} - \\eta \\nabla_{\\mathbf{w}_1}^{(t)} \\right) \\left( w_2^{(t)} - \\eta \\nabla_{w_2}^{(t)} \\right) \\\\\\ \u0026amp;= \\mathbf{w}_1^{(t)}w_2^{(t)} - \\eta w_2^{(t)}\\nabla_{\\mathbf{w}_1^{(t)}} - \\eta \\nabla_{w_2^{(t)}}\\mathbf{w}_1^{(t)} + \\mathcal{O}(\\eta^2) \\\\\\ \u0026amp;= \\mathbf{w}^{(t)} - \\eta \\left( w_2^{(t)} \\right)^2 \\nabla_{\\mathbf{w}^{(t)}} -\\eta \\left( w_2^{(t)} \\right)^{-1} \\nabla_{w_2^{(t)}} \\mathbf{w}^{(t)} + \\mathcal{O}(\\eta^2). \\end{align}$$\nWe can ignore $\\mathcal{O}(\\eta^2)$ since the learning rate is assumed to be low. Also, we take $\\rho^{(t)} = \\eta(w_2^{(t)})^2$ and $\\gamma^{(t)}=\\eta(w_2^{(t)})^{-1}\\nabla_{w_2^{(t)}}$, so the update becomes\n$$ \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\rho^{(t)}\\nabla_{\\mathbf{w}^{(t)}} - \\gamma^{(t)}\\mathbf{w}^{(t)}. $$\nSince $\\mathbf{w}$ is initialized near $0$, it is essentially a weighted combination of the past gradients at any given time, i.e., $\\gamma^{(t)}\\mathbf{w}^{(t)} = \\sum_{\\tau=1}^{t-1}\\mu^{(t,\\tau)}\\nabla_{\\mathbf{w}^{(\\tau)}}$.\nThis is similar to the momentum term in the popular momentum algorithm for optimization (see this earlier post for an overview), and the learning rate term $\\rho^{(t)}$ is time-varying and adaptive.\nUpdate rule for end-to-end matrix The next derivation is a little more involved, and I defer the reader to the actual paper for the detailed proof. I will give a brief outline here.\nSuppose we have a depth-$N$ linear network such that the weight matrices are given by $W_1,\\ldots,W_N$. Let $W_e$ denote the final end-to-end update matrix. The authors use differential techniques to compute an update rule for $W_e$. For this, the important assumption is that $\\eta^2 \\approx 0$. When step sizes are taken to be small, trajectories of discrete optimization algorithms converge to smooth curves modeled by continuous-time differential equations.\nAfter obtaining such a differential equation, integration over the $N$ layers gives the derivative of $W_e$, which is then transformed back to the discrete update rule given as\n$$ W_e^{(t+1)} = (1 - \\eta\\lambda N)W_e^{(t)} - \\eta \\sum_{i=1}^N \\left[ W_e^{(t)} (W_e^{(t)})^T \\right]^{\\frac{j-1}{N}} \\frac{\\partial L^1}{\\partial W}(W_e^{(t)}) \\cdot \\left[ (W_e^{(t)})^T W_e^{(t)} \\right]^{\\frac{N-j}{N}}. $$\nLet us break down this expression. The first part is similar to a weight-decay term for a 1-layer update. The second part also has the derivative w.r.t parameters, but it is multiplied by some preconditioning terms. On further inspection of these terms, it is found that their eigenvalues and eigenvectors depend on the singular value decomposition of $W_e$. Qualitatively, this means that these multipliers favor the gradient along those directions that correspond to singular values whose presence in $W_e$ is stronger. If we assume, as is usually the case in deep learning, that the initialization was near 0, this means that these multipliers act similar to acceleration and push the gradient along the direction already taken by the optimization.\nFor further reading, check out the author\u0026rsquo;s blog post about the paper.\nTo summarize, we looked at three recent papers which prove results on the role of depth in expressibility and optimization of neural networks. People often think that working on the mathematics of deep learning would require complex group theory formalisms and difficult techniques in high-dimensional probability, but as we saw in the proofs of some of these results (especially in Telgarsky\u0026rsquo;s paper), a lot can be achieved using simple counting logic and concentration inequalities.\n","date":1532799020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532975373,"objectID":"dc3b3d867b3ff0a12e7335b997da7e5e","permalink":"https://desh2608.github.io/post/deep-learning-theory-3/","publishdate":"2018-07-28T23:00:20+05:30","relpermalink":"/post/deep-learning-theory-3/","section":"post","summary":"In the previous posts of this series, we have looked at how stochastic gradient descent is able to find a good solution despite the nonconvex objective, and why overparametrized neural networks generalize so well. In this post, we will look at the titular property of deep networks, namely depth, and what role they play in the learning ability of the model.\nAn ideal result in this regard would be if we can show that there exists a class of natural learning problems (recall the idea of a \u0026ldquo;natural\u0026rdquo; problem from the first post) which cannot be solved with depth $d$ neural networks, but are solvable with at least one model of depth $d+1$.","tags":["deep learning","learning theory"],"title":"Theory of Deep Learning: Role of Depth","type":"post"},{"authors":null,"categories":[],"content":" In Part 1 of this series, based on the ICML 2018 tutorial on \u0026ldquo;Toward a Theory for Deep Learning\u0026rdquo; by Prof. Sanjeev Arora, we looked at several aspects of optimization of the nonconvex objective function that is a part of most deep learning models. In this article, we will turn our attention to another important aspect, namely generalization.\nA distinguishing feature of most modern deep learning architectures is that they generalize to test cases exceptionally well, even though the number of parameters is far greater than the number of training samples. VGG19, for instance, which has approximately 20 million weights to be tuned, gives $\\sim 93\\%$ classification accuracy on CIFAR-10, which has only 50000 training images. If you have studied statistical learning theory (see my previous blogs on the topic), this behavior is extremely counter-intuitive, and begs the question: why don\u0026rsquo;t deep neural networks overfit even with small number of training samples?\nBefore we try to understand the reason, let us look at a popular folklore experiment that is described in Livni et al \u0026lsquo;14 related to over-specification.\n For sufficiently over-specified networks, global optima are ubiquitous and in general computationally easy to find.\n To see this, we fix a depth-2 neural network (i.e. a network with 1 hidden layer) consisting of $n$ hidden nodes. We provide random inputs to the network and obtain their corresponding output. Now, take a randomly initialized neural network with the same architecture as the above, and train it using the input-output pairs obtained earlier. It is found that this is really difficult to achieve. However, if we take a large number of hidden nodes, the training becomes easier.\nAlthough this result has been known and verified empirically for some time, it remains to be proven theoretically. This is a striking example of the difficulty of proving generalization guarantees in deep learning.\nEffective capacity of learning The capacity of a learning model, in an abstract sense, means the complexity of training samples that it can fit. For instance, a quadratic regression has inherently more capacity than linear regression, but is also more prone to overfitting. Furthermore, the effective capacity can be thought of as analogous to the number of bits required to represent all possible states that the hypothesis class contains. For this reason, the capacity is approximately the log of the number of apriori functions in the hypothesis class.\nWe will now see a general result that is true for learning models including deep neural networks.\n Claim: Test loss - training loss $\\leq \\sqrt{\\frac{N}{m}}$, where $N$ is the effective capacity and $m$ is the number of training samples.\nProof: First let us fix our neural network $\\theta$ and its parameters. Suppose we take an i.i.d sample $S$ containing $m$ data points. Consider Hoeffding\u0026rsquo;s inequality: If $x_1,\\ldots,x_m$ are $m$ i.i.d samples of a random variable $X$ distributed by $P$, and $a\\leq x_i \\leq b$ for every $i$, then for a small postive non-zero value $\\epsilon$:\n$$ P\\left( \\mathbb{E}_{X \\sim P} - \\frac{1}{m}\\sum_{i=1}^m x_i \\right) \\leq 2\\exp \\left( \\frac{-2m\\epsilon^2}{(b-a)^2} \\right) $$\nWe can apply this inequality to our generalization probability, assuming that our errors are bounded between 0 and 1 (which is a reasonable assumption, as we can get that using a 0/1 loss function or by squashing any other loss between 0 and 1) and get for a single hypothesis $h$:\n$$ P(|R(h) - \\hat{R}(h)| \u0026gt; \\epsilon) \\leq 2\\exp (-2m\\epsilon^2), $$\nwhere $R(h)$ denotes generalization error and $\\hat{R}(h)$ denotes empirical error on the sample.\nHowever, this is not the true generalization bound. This is because we have first fixed out network and we are then choosing the sample i.i.d. However, in a real learning problem, we are given the sample $S$ and we have to learn the parameters to best fit this sample. Therefore, to obtain the actual generalization bound, we take the union bound over all possible neural net configurations $\\mathcal{W}$. Now, equating the RHS with the confidence $\\delta$, we get\n$$ \\begin{align} \u0026amp; 2\\mathcal{W}\\exp(-2m\\epsilon^2) \\leq \\delta \\\\\\ \\Rightarrow \u0026amp; -2m\\epsilon^2 \\leq \\log \\frac{\\delta}{2\\mathcal{W}} \\\\\\ \\Rightarrow \u0026amp; \\epsilon \\geq \\sqrt{\\frac{\\log \\frac{2\\mathcal{W}}{\\delta}}{2m}}, \\end{align} $$\nwhich completes the proof.\n In statistical learning theory, the most popular metrics for measuring the capacity of a model are Rademacher complexity and VC dimension, which I have explained in this post. I will quickly summarize them here.\nRademacher complexity: It is a measure of how well the model can fit a random assignment of labels. Its mathematical formulation is:\n$$ \\hat{\\mathcal{R}_S}(G) = \\mathbb{E}_{\\sigma}[\\text{sup}_{g\\in G}\\frac{1}{m}\\sigma_i g(z_i)] $$\nEssentially, it denotes an expectation of the best possible average correlation that the random labels have with any function present in the hypothesis class $G$. Therefore, a higher Rademacher complexity would imply that the function class $G$ is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class $G$ is, higher is the probability that it would have some $g$ which correlates well with random noise.\nThe generalization error $R(h)$ can be written in terms of R.C. as\n$$ R(h) \\leq \\hat{R}(h) + \\mathcal{R}_m(H) + \\sqrt{\\frac{\\log \\frac{1}{\\delta}}{2m}}, $$\nwhere $\\hat{R}(h)$ is the empirical error, $\\delta$ is the confidence, and $m$ is the number of training samples.\nVC dimension: It is the size of the largest set that can be fully shattered by $G$. By shattering, we mean that $G$ can classify the given set in all possible ways. As such, higher the VC-dimension, more is the capacity of the hypothesis class. We can bound the generalization error in terms of the VC-dimension of the hypothesis class as\n$$ R(h) \\leq \\hat{R}(h) + \\mathcal{O}\\left( \\sqrt{\\frac{\\log(m/d)}{m/d}} \\right) $$\nAlthough these metrics are well established in learning theory, they fail for deep neural networks since they are usually equally vacuous, i.e, the upper bound is greater than 1. This means that the bounds are so large that they are meaningless, since error can never exceed 1, and in practice the generalization error of the networks is many orders of magnitude less than these bounds.\nDeep networks have \u0026ldquo;excess capacity\u0026rdquo; As mentioned earlier, deep neural networks generalize surprisingly well despite having a huge number of parameters. They can be shown by the dotted red line (figure taken from tutorial slides) in the following popular figure which is often found in textbooks.\nOther learning models with a \u0026ldquo;high capacity\u0026rdquo; would follow the general trend and fail to generalize well, which may be evidence that somehow, the large number of parameters in deep networks is not necessarily translating to a high capacity. For a long time, it was believed that a combination of stochastic gradient descent and regularization eliminates the \u0026ldquo;excess capacity\u0026rdquo; of the neural network.\nBut this belief is wrong!\nIn their ICLR \u0026lsquo;17 paper (which I have previously discussed in this post), Zhang et. al., in a series of well-designed experiments, showed that deep networks do retain this excess capacity. From Prof. Arora\u0026rsquo;s blog post on the subject: \u0026ldquo;Their main experimental finding is that if you take a classic convnet architecture, say Alexnet, and train it on images with random labels, then you can still achieve very high accuracy on the training data. (Furthermore, usual regularization strategies, which are believed to promote better generalization, do not help much.) Needless to say, the trained net is subsequently unable to predict the (random) labels of still-unseen images, which means it doesn’t generalize.\u0026rdquo;\nAn interesting (and provable) guarantee that the paper contains is the following theorem: There exists a two-layer neural network with ReLU activations and $2n+d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions.\nIn a related paper published recently, it was shown that the \u0026ldquo;excess capacity\u0026rdquo; is not just limited to deep networks, since even linear models possess this feature. Furthermore, when it comes to fitting noise, there are some interesting similarities between Laplacian kernel machines and ReLU networks. But before we get to that, I will briefly define Laplacian and Gaussian kernels. (For an overview of several kernel functions, check out this article.)\n Kernel Methods Kernel methods map the data into higher-dimensional spaces, in the hope that in this higher-dimensional space the data could become more easily separated or better structured. However, when we talk about transforming data to a higher dimension, called a $z$-space, an actual transformation would involve paying computation costs. To avoid this, we need to look at what we actually want from the $z$-space.\nSupport Vector Machines (SVMs), which are among the most popular kernel-based methods for classification, involve solving for the following Lagrangian.\n$$ \\mathcal{L}(\\alpha) = \\sum_{n=1}^N \\alpha_n - \\frac{1}{2}\\sum_{n=1}^N \\sum_{m=1}^M y_n y_m \\alpha_n \\alpha_m z_n^T z_m $$\nunder the constraints $\\alpha_n \\geq 0 \\forall n$ and $\\sum_{n=1}^N \\alpha_n y_n = 0$. On solving this, we get the boundary as\n$$ g(x) = \\text{sgn}(w^T z + b) $$\nwhere $w = \\sum_{z_n \\in SV} \\alpha_n y_n z_n$.\nWe can see from this that the only value we need from the $z$-space is the inner product $z^T z^{\\prime}$. If we can show that obtaining this inner product is possible without actually going to the $z$-space, we are done.\nIt turns out that this is indeed possible, and there are several such functions, known as kernel functions, which can be written as the inner product in some space. The only constraint on the $z$-space is that it should exist. Interestingly, kernels such as the radial basis function (RBF) kernel exist in an $\\infty$-dimensional space. Furthermore, in order for the problem to be convex and have a unique solution, it is important to select a positive semi-definite kernel, i.e., whose kernel matrix contain only non-negative eigenvalues. Such a kernel is said to obey Mercer\u0026rsquo;s theorem.\n Now that we have some idea what kernels are, let us look at Laplacian and Gaussian kernels.\n Laplacian kernel: It is mathematically defined as  $$ K(x,y) = \\exp \\left( - \\frac{\\lVert x-y \\rVert}{\\sigma} \\right). $$\n Gaussian kernel: Its mathematical formulation is  $$ K(x,y) = \\exp \\left( - \\frac{\\lVert x-y \\rVert^2}{2\\sigma^2} \\right). $$\nBoth the Laplacian and Gaussian kernels are examples of the radial basis function kernels. The difference lies only in the parameter $\\sigma$. Since the Gaussian depends on the square of this parameter, it is more sensitive to changes in $\\sigma$ than the Laplacian.\nThe authors found in their empirical evaluations that Laplacian kernels were much more adept at fitting random labels than Gaussian kernels. This property may be attributed to the inherent non-smoothness of Laplacians as opposed to the Gaussians being smooth. This discontinuity in derivative is reminiscent of that for ReLU units, which, as we saw above, were found to fit random labels exceptionally well. As such, the conjecture is that the radial structure of the kernels, as opposed to the specifics of optimization, plays a key role in ensuring strong classification performance.\nAnother take-away from this paper is that they establish stronger bounds for classification performance of kernel methods. If understanding kernels can indeed lead to a better understanding of deep learning, then maybe these bounds will lead to tighter bounds for the effective capactity of deep neural networks.\nOther notions of generalizability We now look at 2 other concepts that seek to explain why deep neural networks generalize well: flat minima, and noise stability.\nFlat minima Hochreiter and Schmidhuber first conjectured that the flatness of the local minima found by the stochastic gradient descent may be an indicator of its generalization performance. Sharpness of a minimizer can be characterized by the magnitude of the eigenvalues of $\\nabla^2 f(x)$, but since the computation of this quantity is expensive, Keskar et. al. defined a new metric for sharpness that is easier to compute.\nGiven $x \\in \\mathbb{R}^n$, $\\epsilon \u0026gt; 0$, and $A \\in \\mathbb{R}^{n \\times p}$, the $(C_{\\epsilon},A)$-sharpness of $f$ at $x$ is defined as\n$$ \\phi_{x,f}(\\epsilon,A) = \\frac{(\\max_{y\\in C_{\\epsilon}} f(x+Ay))-f(x)}{1+f(x)}\\times 100 $$\nThe metric is based on exploring a small neighborhood of a solution and computing the largest value that $f$ can attain in that neighborhood. We use that value to measure the sensitivity of the training function at the given local minimizer.\nIntuitively, flat minima have lower description lengths (since less information is required to represent a flat surface), and consequently, fewer number of models are possible with this length. The effective capactiy thus becomes less, and so the hypothesis is able to generalize well.\nHowever, recent research suggests that flatness is sensitive to reparametrizations of the neural network: we can reparametrize a neural network without changing its outputs while making sharp minima look arbitrarily flat and vice versa. As a consequence the flatness alone cannot explain or predict good generalization.\nAs Prof. Arora pointed out in his talk, most of the existing theory that tries to explain generalization is only doing a \u0026ldquo;postmortem analysis\u0026rdquo;. This means that they look at some property $\\phi$ that is seemingly possessed by a few neural networks that generalize well, and they argue that the generalization is due to this property. The notion of \u0026ldquo;flat minima\u0026rdquo; is a prime example of this. However, correlation is not causation. Instead of such a qualitative check, the theoretical approach would be to use the property $\\phi$ to compute an upper bound on the number of possible neural networks that would generalize well with this property. This computation is very nontrivial and is therefore ignored.\nNoise stability While flat minima was an old concept, the notion of noise stability is a very recent formalization for the same, proposed in Prof. Arora\u0026rsquo;s ICML\u0026rsquo;18 paper. Essentially, it means that if we add some zero-mean Gaussian noise at an intermediate output of a neural network, the noise gets attenuated as the signal moves to higher layers. Therefore, the capacity of a network to fit random noise can be measured by adding a Gaussian noise at an intermediate layer and measuring the change in output at higher layers.\nThis is also biologically inspired, since neurologists believe that single neurons are extremely susceptible to errors. However, the fact that we still function well suggests that there must be some mechanism to attenuate these errors.\n Noise stability implies compressibility.\n First, what is meant by compression of a neural network? Given a network $C$ with $N$ parameters and some training loss, compression means obtaining a new network $C^{\\prime}$ containing $N^{\\prime}$ parameters ($N^{\\prime} \u0026lt; N$), such that the training loss effectively remains the same. From the generalization claim proved earlier, this compression would mean better generalization capability for the network $C^{\\prime}$.\nNow, let us consider a depth-2 network consisting only of linear transformations. This network can be represented by some matrix $M$, which transforms input $x$ to output $Mx$.\nIn the above figure, $\\eta$ is a zero-mean Gaussian noise that is added to the input. We say that the matrix $M$ is noise stable, i.e. $M(x+\\eta)\\approx Mx$. This means that $\\frac{|Mx|}{|x|} \u0026gt;\u0026gt; \\frac{|M\\eta|}{|\\eta|}$. Here, the value $\\frac{|Mx|}{|x|}$ is at most equal to the largest singular value of $M$, which we denote by $\\sigma_{\\max}(M)$. The RHS is approximately $\\frac{(\\sum_i (\\sigma_i (M))^2)^{\\frac{1}{2}}}{\\sqrt{n}}$ where $\\sigma_i(M)$ is the $i$th singular value of $M$ and $n$ is dimension of $Mx$. The reason is that gaussian noise divides itself evenly across all directions, with variance in each direction $1/n$. Thus,\n$$ (\\sigma_{max}(M))^2 \\gg \\frac{1}{h} \\sum_i (\\sigma_i(M)^2) $$\nThe ratio of the LHS to the RHS in the above inequality is known as the stable rank. Higher the stable rank, more uneven is the distribution of singular values in the matrix. This is easily seen since the highest singular value is much larger than the RMS of all the singular values, something similar to the following figure.\nThe actual signal $x$ is usually correlated with the eigenvectors corresponding to the larger singular values, and as such, the other directions can be ignored without any loss in performance. This is similar to feature selection by a principal component analysis approach.\nNonvacuous bounds for true capacity We have earlier seen that most of the classical metrics used for bounding the generalization error in learning systems prove to be vacuous in case of deep neural networks. The following blog posts by Prof. Arora discuss this issue in some detail and also introduce a new generalization bound based on the compressibility of neural networks explained in the previous section.\n Generalization theory and deep nets, an introduction Proving generalization of deep nets via compression  In this section, I will discuss two approaches for computing nonvacuous bounds for deep networks. The first is from Dziugaite and Roy, and the second is from Prof. Arora\u0026rsquo;s ICML\u0026rsquo;18 paper mentioned previously.\nAs discussed earlier, a common framework for addressing this problem would involve showing under certain assumptions that either SGD performs implicit regularization, or that it finds a solution with some known structure connected to regularization. Once this is found, a nonvacuous bound for the generalization error of such models would have to be determined.\n1. PAC-Bayes approach The first question is how to identify structure in the solutions found by SGD? For this, we again turn to the old notion of flat minima. If SGD finds a flat minima, it means that the solution is surrounded by a large volume of solutions that are nearly as good. If we then represent these nearby solutions by some distribution and pick an average classifier from this distribution, it would be very likely that its generalization error is very close to that of the true solution.\nThis concept is very similar to the PAC-Bayes theorem, which informally bounds the expected error of a classifier chosen from a distribution $Q$ in terms of its KL divergence from a priori fixed distribution $P$. But first, what is KL divergence?\n Kullback-Leibler divergence It is a metric that compares the similarity between two probability distributions. Mathematically, it is the expectation of the log difference between the probability of data in the original distribution $p$ and the approximating distribution $q$.\n$$ \\begin{align} KL(p||q) \u0026amp;= \\mathbb{E}(\\log p(x) - \\log q(x)) \\\\\\ \u0026amp;= \\sum_{i=1}^N p(x_i)(\\log p(x_i) - \\log q(x_i)) \\end{align}$$\nIn information theory, the most important notion is that of entropy, which represents the minimum number of bits required to encode some information, and is mathematically represented as\n$$ H = -\\sum_{i=1}^N p(x_i)\\log p(x_i). $$\nAs such, the KL divergence can be seen to compute how many bits of information will be lost in approximating a distribution $p$ with another distribution $q$.\n The PAC-Bayes bound is given as\n$$ KL(\\hat{e}(Q,S_m)||e(Q)) \\leq \\frac{KL(Q||P)+\\log \\frac{m}{\\delta}}{m-1}, $$\nwhere $\\hat{e}(Q,S_m)$ is the empirical loss of $Q$ w.r.t some i.i.d sample $S_m$, and $e(Q)$ is the expected loss. If we now find a $Q$ that minimizes this value, we are likely to find a minima that generalizes well and has a nonvacuous bound. This is exactly what is proposed in the paper.\nOn a binary variant of MNIST, the computed PAC-Bayes bounds on the test error are in the range 16-22%. While this is a loose bound (actual bounds are around 3%), it is still surprising to find a non-trivial numerical bound for a model with such a large capacity on so few training examples. The authors comment that these are, in all likelihood, \u0026ldquo;the first explicit and nonvacuous numerical bounds computed for trained neural networks in the deep learning regime\u0026rdquo;.\n2. Compressibility approach Although the PAC-Bayes bound is nonvacuous, it is still looser than actual sample complexity bounds computed empirically. Instead, Arora et al. introduce a new compression framework to address this problem. Earlier while discussing noise stability, we have already seen that if we can compress a classifier $f$ without decreasing the empirical loss, it becomes much more generalizable according to the fundamental theorem proved earlier.\nWe say that $f$ is $(\\gamma,S)$-compressible using helper string $s$ if there exists some other classifier $g_{A,s}$ on a class of parameters $A$ such that the classification loss of $f$ on every $x \\in S$ differs from that of $g_{A,s}$ by at most $\\gamma$. Here, $s$ is fixed before looking at the training sample, and is often just for randomization.\nThen, the main theorem in the paper is as follows: If $f$ is $(\\gamma,S)$-compressible using helper string $s$, then with high probability,\n$$ L_0 (g_A) \\leq \\hat{L}_{\\gamma}(f) + \\mathcal{O}\\left( \\sqrt{\\frac{q \\log r}{m}} \\right), $$\nwhere $A$ is a set of $q$ parameters each having at most $r$ discrete values, $L_0 (g_A)$ is the generalization loss of compressed classifier, and $\\hat{L}_{\\gamma}(f)$ is the empirical estimate of the marginal loss of original classifier. Note that the bound is for the compressed classifier, but the same is also true for earlier works (like the PAC-Bayes approach). The proof is very elementary and uses just simple concentration inequalities.\n Proof: First, using Hoeffding\u0026rsquo;s inequality, we can write\n$$ P(L_0 (g_A) - \\hat{L}_0 (g_A) \\geq \\epsilon) \\leq 2\\exp(-2m\\epsilon^2). $$\nTaking $\\epsilon = \\sqrt{\\frac{q \\log r}{m}}$, we get, with probability at least $1 - \\exp(-2q\\log r)$,\n$$ L_0 (g_A) \\leq \\hat{L}_0 (g_A) + \\mathcal{O}\\left( \\sqrt{\\frac{q \\log r}{m}} \\right). $$\nNext, by definition of $(\\gamma,S)$-compressibility, we can write\n$$ \\lvert f(x)[y] - g_A(x)[y] \\rvert \\leq \\gamma. $$\nThis means that as long as the original function has margin at least $\\gamma$, the new function classifies the example correctly. Therefore,\n$$ \\hat{L}_0 (g_A) \\leq \\hat{L}_{\\gamma}(f). $$\nCombining this with the earlier inequality, we immediately get the result.\n In addition to providing a tighter generalization bound for fully connected networks, the paper also proposes some theory for convolutional nets, which have been notoroiusly difficult to theorize. For details, readers are suggested to refer to the paper.\n","date":1532679311,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535404102,"objectID":"db486aae92453761b91f02ac69f0ea2c","permalink":"https://desh2608.github.io/post/deep-learning-theory-2/","publishdate":"2018-07-27T13:45:11+05:30","relpermalink":"/post/deep-learning-theory-2/","section":"post","summary":"In Part 1 of this series, based on the ICML 2018 tutorial on \u0026ldquo;Toward a Theory for Deep Learning\u0026rdquo; by Prof. Sanjeev Arora, we looked at several aspects of optimization of the nonconvex objective function that is a part of most deep learning models. In this article, we will turn our attention to another important aspect, namely generalization.\nA distinguishing feature of most modern deep learning architectures is that they generalize to test cases exceptionally well, even though the number of parameters is far greater than the number of training samples.","tags":["deep learning","learning theory"],"title":"Theory of Deep Learning: Generalization","type":"post"},{"authors":null,"categories":[],"content":" I only just got around to watching the ICML 2018 tutorial on \u0026ldquo;Toward a Theory for Deep Learning\u0026rdquo; by Prof. Sanjeev Arora. In this and the next few posts, I will discuss the subject in some detail, including the referenced papers and blogs. Very conveniently, the talk itself was divided into 5 parts, and I will structure this series accordingly.\nAt the outset, we should understand that a number of important concepts in deep learning are already shaped by optimization theory. Backpropagation, for instance, is basically just a linear time dynamic programming algorithm to compute gradient. Recent methods for gradient descent, such as momentum, Adagrad, etc. (see this post for a quick overview) are obtained from convex optimization techniques. However, over the last decade, the deep learning community has come up with several models based on intuition mostly, that do not have any theoretical support yet.\n The goal, then, is to find theorems that support these intuitions, leading to new insights and concepts.\n In this first part of the series, we will try to understand why (and how) deep learning almost always finds decent solutions to problems that are highly nonconvex.\nPossible goals for optimization Any neural network essentially tries to minimize a loss function. However, in almost all cases, this loss function is highly nonconvex (and sometimes NP-hard), which means that no provably polytime algorithm exists for its optimization. Even so, deep networks are quite adept at finding an approximately good solution.\nWhenever the gradient $\\nabla$ is non-zero, there exists a descent direction. As such, a possible goal for the network may be any of the following (in increasing order of difficulty):\n Finding a critical point, i.e. $\\nabla = 0$. Finding a local optimum, i.e. $\\nabla = 0$ and $\\nabla^2$ is positive semi-definite. Finding a global optimum.  Furthermore, this descent may be from several possible initializations, namely all points, random points, or specially-chosen points. Now, if there are $d$ parameters (weights) to be optimized, we say that the problem is in $\\mathbb{R}^d$ space. It is usually visualized by the following sea-urchin figure (or a $d$-urchin figure, according to Prof. Arora).\nIn $\\mathbb{R}^d$ space, there exit exp($d$) directions which can be explored to find the optimal solution, which makes the naive approach infeasible. Also, we cannot use non black box approaches to prune the number of explorations, since there is no clean mathematical formulation for the problem.\nBut what does this mean? This means that problems in deep learning are usually of the kind where, given pixels of an image, you have to label it as a cat or a dog. Such an $(x_i,y_i)$ has no mathematical meaning. This means that we do not understand the inherent landscape of the problem we are trying to solve, and so no special pruning can be done.\nThis, combined with the nonconvex nature of the loss function, also means that it becomes infeasible to find a global optimum for the optimization problem. As such, we have to settle for goals 1 and 2, i.e. a critical point or a local optimum.\nFinding critical points The update function for a parameter $\\theta$ is given as\n$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla f(\\theta_t) $$\nIf the second derivative $\\nabla^2$ is high, $\\nabla f(\\theta_t)$ will vary a lot, and we may miss the actual critical point. To prevent this, it is advisable to take small steps.\nBut how do we quantify small? In other words, how do we determine a good learning rate for the optimization problem? For this, we again look at $\\nabla^2$, which will determine the smoothness of the function. Suppose there exists a $\\beta$ such that the Hessian $-\\beta I \\leq \\nabla^2 f(\\theta) \\leq \\beta I$, where $I$ is the identity matrix. Essentially, a higher $\\beta$ means that $\\nabla^2$ varies more, and so the learning rate should be lower. From this understanding, we can prove the following claim.\n Claim (Nesterov 1998): If we choose $\\eta = \\frac{1}{2\\beta}$, we can achieve $|\\nabla f|\u0026lt;\\epsilon$ in number of steps proportional to $\\frac{\\beta}{\\epsilon^2}$.\n Proof: See the proof of Lemma 2.8 here (see Definition 2.7). So a single update reduces the function value by at least $\\frac{\\epsilon^2}{2\\beta}$. Therefore, it would take $\\mathcal{O}(\\frac{\\beta}{\\epsilon^2})$ steps to arrive at a critical point.\nEvading saddle points While we have a theoretical upper limit for the time taken for convergence at a critical point, this is still problematic since it may be a saddle point, i.e., the function value is minimum in $d-1$ directions but maximum in one direction. Such a surface literally looks like a saddle as follows.\nAn important question, then, is how to evade saddle points while looking for critical points. This question is explored in a series of papers and corresponding blog posts on Prof. Arora\u0026rsquo;s blog.\n Polynomial time guarantee for GD to escape saddle points (based on this paper) Random initialization for asymptotically avoiding saddle points (based on this paper) Perturbing gradient descent (based on this paper)  Here I will try to summarize these discussions in several bullet points.\n Most learning problems have exponentially many saddle points. Learning problems usually involve searching for $k$ components, for example clustering, $k$-node hidden layer in a neural network, etc. Suppose $(x_1,x_2,\\ldots,x_k)$ is an optimal solution. Then, $(x_2,x_1,\\ldots,x_k)$ is also an optimal solution, but the mean of these is not an optimal solution. This suffices to show that the learning problem is nonconvex, since for a convex function, the average of optimal solutions is also optimal. Furthermore, we can keep swapping the $k$ components to obtain exponential optimal solutions. Saddle points lie on the paths joining these isolated solutions, and hence, are exponential in number themselves.\n Hessians can be used to evade saddle points. Consider the second order Taylor expansion given below. If there exists a direction where $\\frac{1}{2}(y-x)^T \\nabla^2 f(x)(y-x)$ is significantly less than 0, then using this update rule can avoid saddle points. Such saddle points are called \u0026ldquo;strict,\u0026rdquo; and for these, methods such as trust region algorithms and cubic regularization can find the local optimum.\n  $$ f(y) = f(x) + \u0026lt;\\nabla f(x), y-x\u0026gt; + \\frac{1}{2}(y-x)^T \\nabla^2 f(x)(y-x) $$\n Noisy gradient descent converges to local optimum in polynomial number of steps. Although the Hessian method provides a theoretical way to escape saddle points, the computation of $\\nabla^2$ is still expensive. Suppose we put a ball on a saddle point. Then, giving it only a slight push will move it away from the saddle. This intuition leads to the notion of \u0026ldquo;noisy\u0026rdquo; GD, i.e., $y = x - \\eta \\nabla f(x) + \\epsilon$, where $\\epsilon$ is a zero-mean error, which is often cheaper to compute than the true gradient. The authors in also prove the theorem in the paper, but it is very non-trivial.\n It is hard to converge to a saddle point. Furthermore, a random initialization of GD will asymptotically converge to a local minimum, rather than other stationary points. In (2), Ben Recht emphasized that \u0026ldquo;even simple algorithms like gradient descent with constant step sizes can’t converge to saddle points unless you try really hard.\u0026rdquo; To prove this, they use the Stable Manifold Theorem, taking $x^{\\ast}$ to be an arbitrary saddle point and showing that this measure was always zero.\n   The Stable Manifold theorem is concerned with fixed point operations of the form $x^{(k+1)}=\\psi(x^{(k)})$. It quantifies that the set of points that locally converge to a fixed point $x^{\\ast}$ of such an iteration have measure zero whenever the Jacobian of $\\psi$ at $x^{\\ast}$ has eigenvalues bigger than 1.\n In fact, it has been shown long back that additive Gaussian noise is sufficient to prevent convergence to saddles, without even assuming the \u0026ldquo;strictness\u0026rdquo; criteria of (1).\nNow that it is clear that GD can avoid saddle points almost certainly, it remains to be seen whether it is efficient in doing so. The paper (1), although it did show a poly-time convergence for the noisy GD, was still inefficient because its polynomial dependency on the dimension $n$ and the smallest eigenvalue of the Hessian are impractical. The paper (3) further improves this aspect of the problem.\n A perturbed form of GD, under an additional Hessian-Lipschitz condition, converges to a second-order stationary point in almost the same time required for GD to converge to a first-order stationary point. Furthermore, the dimensional dependence is only polynomial in $\\log(d)$.\n Finally, recent work definitely shows that PGD is much better than GD with random initialization, since the latter can be slowed down by saddle points, taking exponential time to escape. This is because if there are a sequence of closely-spaced saddle points, GD gets closer to the later ones, and takes $e^i$ iterations to escape the $i^{th}$ saddle point. PGD, on the other hand, escapes each saddle point in a small number of steps regardless of history.\n  Summary: Although most learning problems have exponentially many saddle points, they are hard to converge to, and even random initializations can escape them. They take a long time for this escape though, which is why using perturbations is more efficient, and actually as efficient as GD for first-order stationary points. Therefore, using information from Hessians is not necessary to escape saddle points efficiently.\nSecond-order methods for local optimum Although we have established that Hessians are unnecessary for finding the local optimum, it would still be enlightening to look at some approaches for the same.\nAgarwal et. al \u0026lsquo;17 proposed LiSSA, or Linear (time) Stochastic Second-order Algorithm. The basic update rule is\n$$ x_{t+1} = x_t - \\eta [\\nabla^2 f(x)]^{-1}\\nabla f(x), $$\ni.e. the gradient is scaled by the inverse of the Hessian, which intuitively makes sense as discussed earlier. Although backpropagation can compute the Hessian itself in linear time, we require the inverse. In this paper, the LiSSA algorithm uses the idea that $(\\nabla^2)^{-1} = \\sum_{i=1}^{\\infty}(I - \\nabla^2)^i$, but with finite truncation.\nCarmon et al. \u0026lsquo;17 further improved upon the $\\mathcal{O}(\\frac{1}{\\epsilon^2})$ guarantee provided by gradient descent for $\\epsilon$-first-order convergence, without any need for Hessian computation. They use two competing techniques for this purpose. The first has already been discussed above:\n If the problem is locally non-convex, the Hessian must have a negative eigenvalue. In this case, under the assumption that the Hessian is Lipschitz continuous, moving in the direction of the corresponding eigenvector must make progress on the objective.\n The second technique is more novel. They show that if the Hessian\u0026rsquo;s smallest eigenvalue is at least $-\\gamma$, we can apply proximal point techniques and accelerated gradient descent to a carefully constructed regularized problem to obtain a faster running time.\nWhile their approach is asymptotically faster than first-order methods, it is still empirically slower. Furthermore, it doesn\u0026rsquo;t seem to find better quality neural networks in practice.\nUnderstanding the landscape: Matrix completion Very early on in this post, we established that in deep learning problems, the landscape is unknown, i.e. the problem does not have a meaningful mathematical formulation. In this vein, we now look at a paper that develops a new framework to capture the landscape. In particular, we will approach this problem in the context of matrix completion. (Interestingly, this paper is again from Rong Ge, who first showed polytime convergence to local minimum for noisy GD.)\nBut first, what is matrix completion. Matrix completion is a learning problem wherein the objective is to recover a low-rank matrix from partially observed entries. The mathematical formulation of the problem is:\n$$ \\min_{X} \\text(rank)(X) \\quad \\text{subject to} \\quad X_{ij} = M_{ij} ~~ \\forall i,j \\in E $$\nwhere $E$ is the set of observed entries. Most approaches to solve this problem represent it in the form of the following nonconvex objective.\n$$ f(X) = \\frac{1}{2}\\sum_{i,j\\in E}[M_{i,j}-(XX^T)_{i,j}]^2 +R(X) $$\nHere, $R(X)$ is a regularization term which ensures that no single row of $X$ becomes too large, otherwise most observed entries will be 0.\nGe showed in an earlier paper that in case of matrix completion (others have shown the same result for other problems like tensor decomposition and dictionary learning), all local minima are also global minima.\n For matrix completion, all local minima are also global minima.\n In the present paper, the authors proposed the new insight that for the case of the matrix completion objective as defined above, the function $f$ is quadratic in $X$, which means that its Hessian w.r.t $X$ is constant. Furthermore, any saddle point has at least one strictly negative eigenvalue in its Hessian. Together, these ensure that simple local search algorithms can find the desired low rank matrix from an arbitrary starting point in polynomial time with high probability.\nThese advances, while mathematically involved, show that characterizing the various stationary points of the learning objective can be helpful in providing theoretical guarantees for learning algorithms. While I have avoided proof details for the several important theorems here, I will try to understand and explain them lucidly in some later post.\n","date":1532583918,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535404102,"objectID":"fc1dfdb79be24008249c0460d24b2d0c","permalink":"https://desh2608.github.io/post/deep-learning-theory-1/","publishdate":"2018-07-26T11:15:18+05:30","relpermalink":"/post/deep-learning-theory-1/","section":"post","summary":"I only just got around to watching the ICML 2018 tutorial on \u0026ldquo;Toward a Theory for Deep Learning\u0026rdquo; by Prof. Sanjeev Arora. In this and the next few posts, I will discuss the subject in some detail, including the referenced papers and blogs. Very conveniently, the talk itself was divided into 5 parts, and I will structure this series accordingly.\nAt the outset, we should understand that a number of important concepts in deep learning are already shaped by optimization theory.","tags":["deep learning","learning theory"],"title":"Theory of Deep Learning: Optimization","type":"post"},{"authors":null,"categories":[],"content":" Transfer learning is undoubtedly the new (well, relatively anyway) hot thing in deep learning right now. In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks. In NLP, though, transfer learning was mostly limited to the use of pretrained word embeddings (which, to be fair, improved baselines significantly). Recently, researchers are moving towards transferring entire models from one task to another, and that is the subject of this post.\nSebastian Ruder (whose biweekly newsletter inspires a lot of my deep learning reading) and Jeremy Howard were perhaps the first to make transfer learning in NLP exciting through their ULMFiT method which surpassed all text classification state-of-the-art. This Monday, OpenAI extended their idea and outperformed SOTAs on several NLP tasks. At NAACL 2018, the Best Paper award was given to the paper introducing ELMo, a new word embedding technique very similar to the idea behind ULMFiT, from researchers at AllenAI and Luke Zettlemoyer’s group at UWash (Seattle).\nIn this article, I will discuss all of these new work and how they are interrelated. Let’s start with Ruder and Howard’s trend-setting architecture.\nUniversal Language Model Fine-Tuning for Text Classification Most datasets for text classification (or any other supervised NLP tasks) are rather small. This makes it very difficult to train deep neural networks, as they would tend to overfit on these small training data and not generalize well in practice.\nIn computer vision, for a couple of years now, the trend is to pre-train any model on the huge ImageNet corpus. This is much better than a random initialization because the model learns general image features and that learning can then be used in any vision task (say captioning, or detection).\nTaking inspiration from this idea, Howard and Ruder propose a bi-LSTM model that is trained on a general language modeling (LM) task and then fine tuned on text classification. This would, in principle, perform well because the model would be able to use its knowledge of the semantics of language acquired from the generative pre-training. Ideally, this transfer can be done from any source task $S$ to a target task $T$. The authors use LM as the source task because:\n it is able to capture long-term dependencies in language it effectively incorporates hierarchical relations it can help the model learn sentiments large data corpus is easily available for LM  Formally, \u0026ldquo;LM introduces a hypothesis space $H$ that should be useful for many other NLP tasks.\u0026rdquo;\nFor the architecture, they use the then SOTA AWD-LSTM (which is, I suppose, a multi-layer bi-LSTM network without attention, but I would urge you to read the details in the paper from Salesforce Research). The model was trained on the WikiText-103 corpus.\nOnce the generic LM is trained, it can be used as is for multiple classification tasks, with some fine-tuning. For this fine tuning and subsequent classification, the authors propose 3 implementation tricks.\nDiscriminative fine tuning: Different learning rates are used for different layers during the fine-tuning phase of LM (on the target task). This is done because the layers capture different types of information.\nSlanted triangular learning rates (STLR): Learning rates are first increased linearly, and then decreased gradually after a cut, i.e., there is a \u0026ldquo;short increase\u0026rdquo; and a \u0026ldquo;long decay\u0026rdquo;. This is similar to the aggressive cosine annealing learning strategy that is popular now.\nGradual unfreezing: During the classification training, the LM model is gradually unfreezed starting from the last layer. If all the layers are trained from the beginning, the learning from the LM would be forgotten quickly, and so gradual unfreezing is important to make use of the transfer learning.\nOn the 6 text classification tasks that they evaluated, there was a relative improvement of 18–24% on the majority of tasks. Further, the following was observed:\n Only 100 labeled samples in classification were sufficient to match the performance of a model trained on 50–100x samples from scratch. Pretraining is more useful on small and medium sized data. LM quality affects final classification performance.  The analysis in the paper is very thorough, and I would recommend going through it for details, and also to learn how to design experiments for strong empirical results. They suggest some possible future directions as follows:\n The LM pretraining and fine-tuning can be improved. The LM can be augmented with other tasks in a multi-task learning setting. The pretrained model can be evaluated on tasks other than classification. Further analysis can be done to determine what information is captured during pretraining and changed during fine-tuning.  1 and 3 should be noted, in particular, as that makes up the novelty in OpenAI’s new paper discussed below.\nImproving Language Modeling by Generative Pre-training This paper was published on ArXiv this Monday (11 June), and my Twitter feed has been inundated with talk about it since then. Jeremy Howard himself tweeted favorably about it, saying that this was exactly the kind of work he was hoping for in his \u0026ldquo;future directions\u0026rdquo;.\nThis is exactly where we were hoping our ULMFit work would head - really great work from @OpenAI! 😊\nIf you\u0026#39;re doing NLP and haven\u0026#39;t tried language model transfer learning yet, then jump in now, because it\u0026#39;s a Really Big Deal. https://t.co/0Dj8ChCxvu\n\u0026mdash; Jeremy Howard (@jeremyphoward) June 11, 2018 \nWhat Alec Radford (the first author) does here is\n use a Transformer network (explained below in detail) instead of the AWD-LSTM; and evaluate the LM on a variety of NLP tasks, ranging from textual entailment to question-answering.  If you are already aware of the ULMFiT architecture, you only need to know 2 things to understand this paper: (a) how the Transformer works, and (b) how an LM-trained model can be used to evaluate the different NLP tasks.\nThe Transformer This blog provides an extensive description of the model, originally proposed in this highly popular paper from last year. Here I will go over the salient features. For details, you can go through the linked blog post or the paper itself.\nThe problem with RNN-based seq2seq models is that since they are sequential models, they cannot be parallelized. One possible solution that was proposed to remedy this involved the use of fully convolutional networks with positional embeddings, but it required O(nlogn) time to relate 2 words at some distance in the sentence. The Transformer solves this problem by completely doing away with convolutions or recurrence, and relying entirely upon self-attention.\nIn a simple scalar dot-product attention, weight is computed by taking the dot product of the query (Q) and key (K). The weighted sum of all values V is then the required output. In contrast, in a multihead attention, the input vector itself is divided into chunks and then the scalar dot-product attention is applied on each chunk in parallel. Finally, we compute the average of all the chunk outputs.\nThe final step consists of a position-wise FFN, which itself is a combination of 2 linear transformations and a ReLU for each position. The following GIF explains this process very effectively.\nTransformer\nTask-specific input transformations The second novelty in the OpenAI paper is how they use the pretrained LM model on several NLP tasks.\n Textual entailment: The text (t) and the hypothesis (h) are cocatenated with a $ in between. This makes it naturally suitable for evaluation on an LM model. Text similarity: Since the order is not important here, the texts are concatenated in both orders and then processed independently and added element-wise. Question-answering and commonsense reasoning: The text, query, and answer option are concatenated with some differentiation symbol in between and each such sample is processed. They are then normalized via softmax to produce output distribution over possible answers.  The authors trained the Transformer LM on the Book Corpus dataset, and improved SOTA on 9 of the 12 tasks. While the results are indeed amazing, the analysis is not as extensive as that performed by Howard and Ruder, probably because the training required a month on 8 GPUs. This was even pointed out by Yoav Goldberg.\nStrong empirical results, but I wish there was more focus on a proper comparison / controlled experiments. Is the improvement due to LSTM -\u0026gt; Transformer or due to Wiki-1B (single sents) -\u0026gt; BooksCorpus (longer context)? https://t.co/L3WrJW3z12\n\u0026mdash; (((λ()(λ() \u0026#39;yoav)))) (@yoavgo) June 12, 2018 \nDeep Contextualized Word Representations The motivation for this paper, which won the Best Paper award at NAACL’18, is that word embeddings should incorporate both word-level characteristics as well as contextual semantics.\nThe solution is very simple: instead of taking just the final layer of a deep bi-LSTM language model as the word representation, obtain the vectors of each of the internal functional states of every layer, and combine them in a weighted fashion to get the final embeddings.\nThe intuition is that the higher level states of the bi-LSTM capture context, while the lower level captures syntax well. This is also shown empirically by comparing the performance of 1st layer and 2nd layer embeddings. While the 1st layer performs better on POS tagging, the 2nd layer achieves better accuracy for a word-sense disambiguation task.\nFor the initial representation, the authors chose to initialize with the embeddings obtained from a character CNN, so as to have character level morphological information incorporated in the embeddings. Finally for an $L$-layer bi-LSTM, $2L+1$ such vectors need to be combined to get the final representation, after performing some layer normalization.\nIn the empirical evalutation, the use of ELMo resulted in up to 25% relative increase in performance across several NLP tasks. Moreover, it improves sample efficiency considerably.\n(Interestingly, a Google search revealed that this paper was first submitted at ICLR’18 but later withdrawn. I wonder why.)\nAs Jeremy Howard says, transfer learning is indeed the Next Big Thing in NLP, and these trend-setting papers demonstrate why. I am sure we will see a lot of development in this area in the days to come.\n","date":1529050338,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531407517,"objectID":"007a60d0796fd3556841f8f3b60df535","permalink":"https://desh2608.github.io/post/transfer-learning-nlp/","publishdate":"2018-06-15T13:42:18+05:30","relpermalink":"/post/transfer-learning-nlp/","section":"post","summary":"Transfer learning is undoubtedly the new (well, relatively anyway) hot thing in deep learning right now. In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks. In NLP, though, transfer learning was mostly limited to the use of pretrained word embeddings (which, to be fair, improved baselines significantly).","tags":["natural language processing","transfer learning"],"title":"Transfer Learning in NLP","type":"post"},{"authors":null,"categories":[],"content":" Spoken Dialog Systems (SDS) have become very popular recently, especially for goal completion tasks on mobile devices. Also, with the increasing use of IoT devices and their associated assistants like Alexa, Google Home, etc., systems that can converse with users in natural language are set to be the primary mode of human-computer interaction in the coming years.\nEarly SDSs used to be extremely modular, with components such as automatic speech recognition, natural language understanding, dialog management, response generation, and speech synthesis, each trained separately and then combined. For the last few years, especially after neural models gained popularity in language models and machine translation, researchers are slowly but surely moving towards more end-to-end approaches. Very recently, most of the dialog systems proposed in conferences all seem to be built around reinforcement learning frameworks.\nAlthough they are critical in industry, SDSs have traditionally been notoriously difficult to evaluate. This is because any evaluation strategy associated with them needs to have at least the following features.\n It should provide an estimate of how well the goal is met. It should allow for comparative judgement of different systems. It should (ideally) identify factors that can be improved. It should discover trade-offs or correlations between factors.  For a long time, the most prominent evaluation scheme for SDSs was PARADISE (Walker’97), developed at AT\u0026amp;T labs.\nPARADISE: PARAdigm for DIalog System Evalutation The Paradise scheme comprises of 2 objectives:\n What does the agent accomplish? i.e., task completion How is it accomplished? i.e., agent behavior  For quantifying these objectives, the task is represented in the form of an Attribute Value Matrix (AVM), which consists of the information that must be exchanged between the agent and the user during the dialogue, represented as a set of ordered pairs of attributes and their possible values. Essentially, this is a confusion matrix between attributes in the actual dialogue and attributes in the expected dialogue.\nOnce the AVM is available, the task completion success is computed by a metric $\\kappa$ defined as\n$$ \\kappa = \\frac{P(A)-P(E)}{1-P(E)} $$\nwhere $P(A)$ is the proportion of times that the actual dialogue agrees with the scenario keys, and $P(E)$ is the expected proportion of times for the same. If $M$ is the matrix, then\n$$ P(E) = \\sum_{i=1}^n \\left( \\frac{t_i}{T} \\right)^2 $$\nwhere $t_i$ is the sum of frequencies in column $i$, and $T$ is the sum of frequencies in $M$. $P(A)$ is given as\n$$ P(A) = \\sum_{i=1}^n \\frac{M(i,i)}{T} $$\nSince $\\kappa$ includes $P(E)$, it inherently includes the task complexity as well, thereby making it a better metric for task completion than, say, transaction success, concept accuracy, or percent agreement.\nFor measuring the second objective, i.e., agent behavior, all the AVM attributes are tagged with respective costs. Some examples of cost attributes are: number of user initiatives, mean word per turn, mean response time, number of missing/inappropriate responses, number of errors, etc. Thereafter, the final performance is defined as\n$$ P = (\\alpha \\mathcal{N}(\\kappa)) - \\sum_{i=1}^n (w_i \\mathcal{N}(c_i)) $$\nHere, $\\mathcal{N}$ is some Z-score normalization factor, such as simple normalization based on mean and standard deviation.\nAlthough PARADISE was an important evaluation scheme for evaluating older statistical SDS models, I haven’t seen it used in any of the recent papers on the subject. A major factor for this is probably the choice of the regression coefficients (costs and coefficient for Kappa), which would greatly affect the performance statistic.\nSchemes in Recent Papers For the last couple years, most papers on SDSs propose end-to-end neural architectures. As such, they prefer an evaluation scheme based on a corpus of dialogues divided into training, validation, and development sets.\nData collection Amazon Mechanical Turk (MT), which is a crowdsourcing website, is the most popular method for collecting data. Peng’17, Li’17, and Wen’16 all use Amazon MT to source their training dialogue sets. Furthermore, the protocol used for this is usually the Wizard-of-Oz scheme.\nThe Wizard-of-Oz protocol: In this scheme, a user converses with an agent, which he believes is autonomous. However, there is actually a human in the loop (called a \u0026ldquo;wizard\u0026rdquo;) which controls some key features of the agent which require tuning. The protocol is implemented as follows.\n A metric (e.g., task completion rate) is selected to serve as the objective function. Some particular features, called the “repair strategy,” are varied to best match the desired performance claim for the metric. All other input, output are kept constant through the interface. The process is repeated using different wizards.  In Wen’16, the authors further expediated this protocol by parallelizing it on Amazon MT, such that there are multiple users and wizards working simultaneously on single-turn dialogues.\nTask completion usually involved slot filling as an intermediate objective. As such, several \u0026ldquo;informable slots\u0026rdquo; (e.g., food, price range, area, for a restaurant search system) and \u0026ldquo;requestable slots\u0026rdquo;(e.g., address, phone, postal code) are identified. Users are provided with the keys for the informable slots and wizards are provided with keys for the requestable slots.\nIn some cases, like Liu’18 and Peng’17, user simulators are also used to create such a corpus of dialogues. An implementation of such a system can be found here. For Liu’18, the authors further used a public dataset (DSTC) for corpus-based training.\nAutomatic evaluation Automatic evaluation metrices may be unsupervised or supervised in nature. Liu’17 summarizes several unsupervised schemes for evaluation, and I list them here.\nWord overlap based metrics\nThese include BLEU, ROUGE, and METEOR, and are inspired from machine translation tasks. I have previously described all of these metrics in another blog post.\nEmbedding based metrics\nThese are based on the word embeddings (skip-gram, GloVe, etc.) of the actual response and the expected response. Some of them are:\n Greedy matching Embedding average Vector extrema  Please refer to the linked paper for details of these metrics. They are conceptually very simple so I won’t describe them here.\nA problem with both of these techniques is that they may only be suitable for task completion dialog systems, where there are only a few expected responses. Any open-world dialog will necessarily beat the metric, e.g.\n User: Do you want to watch a movie today?\nGold-standard: Yeah, let’s catch the new Bond film.\nActual: No, I am busy with something.\n For this reason, several other metrics are employed, such as task success rate, average reward (+1 for each slot filled correctly), average number of turns, entity matching rate, prediction accuracy, etc.\nHuman evaluation Most importantly though, all researchers agree that human evaluation can never be replaced by automatic evaluation metrics. Usually, several human users are asked to test a trained system with goal-oriented dialogues, and at the end of the dialogue, they are asked to rate it on several criteria such as:\n Task completion success Comprehension Naturalness  To avoid extreme scores (due to bias etc.), inter-user agreement is calculated using the Kappa value, and only those users with $\\kappa \u0026gt; 0.2$ are retained in the final measure. Their scores are then matched against scores from the automatic evaluations by computing correlation coefficients such as Spearman’s or Pearson’s (like in Liu’16).\nKey Takeaways From this entire literature survey, I have extracted the following key points to note if you are working to build a task-oriented dialog system and want to evaluate it:\n Choose one specific domain, e.g., restaurant search. Use either DSTC (or an equivalent large corpus of dialogues), or use Amazon MT to create one for your task. Train your model on the dataset created above. Use a word overlap based and a few task completion based metrics for automatic evaluation statistics. Compare with at least a few popular neural baselines. RL frameworks with LSTMs are in vogue these days, I suppose. Definitely do human evaluation for your method.  ","date":1528272724,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531407517,"objectID":"8fb2e705859eee600de8a6f6b5c49f94","permalink":"https://desh2608.github.io/post/evaluation-methods-dialog-systems/","publishdate":"2018-06-06T13:42:04+05:30","relpermalink":"/post/evaluation-methods-dialog-systems/","section":"post","summary":"Spoken Dialog Systems (SDS) have become very popular recently, especially for goal completion tasks on mobile devices. Also, with the increasing use of IoT devices and their associated assistants like Alexa, Google Home, etc., systems that can converse with users in natural language are set to be the primary mode of human-computer interaction in the coming years.\nEarly SDSs used to be extremely modular, with components such as automatic speech recognition, natural language understanding, dialog management, response generation, and speech synthesis, each trained separately and then combined.","tags":["dialog system"],"title":"Evaluation Methods for Dialog Systems","type":"post"},{"authors":null,"categories":[],"content":" This article is in a different flavor from the other posts in this publication. This is because I have been reading Roman Vershynin’s \u0026ldquo;High Dimensional Probability\u0026rdquo; for the last few days, and between that and visa formalities, I didn’t get a chance to check out new papers. I do plan to write an article on new methods for object detection (such as RCNN, Faster RCNN, and YOLO) sometime next month.\nI have only been a researcher for a couple of years now but during this period I have gained valuable insights on how to structure a research project. When I started out with research back in 2016, I was too eager to obtain results, a mistake that most beginner researchers make. In my eagerness, I used to cut corners with my code structure and take several liberties, especially because there was no review process. I made several mistakes during the project (on relation classification: link to Github repo), some of which I list here:\n Lack of planning: I did not have a research plan to begin with, which showed in my code. It is true that in applied machine learning, much of the progress is dictated by experimental results, but a broad outline still helps. At the beginning, I just read papers, cloned their repositories, and ran them on my GPU. Sometimes this used to eat up a lot of precious time since some repositories had several dependencies. Haphazard code format: This stemmed from the first issue. Since I had not planned in advance, I would work with every dataset differently, depending upon how it was available. Some of the code would be in IPython notebooks, while some would be Shell scripts. I would use plain Tensorflow for some training and a Keras wrapper for others. Not caring about reproducibility: This is perhaps the biggest crime an ML researcher can commit. Although my code is legitimate and publicly available, I highly doubt that anyone could reproduce it (not easily, in any case). This is because at that time, all I cared about was getting a publication (which I did, in the end). I did not have a good README file, nor instructions on how to reproduce the results.  Based on these and several other mistakes, I have come up with some guidelines on how to write good code for a research project. I ascribe much of my learning to working on this project (which is only in its beginning phase) for the last few months. Here are the 8 commandments, along with examples in Python.\n1. Define and validate data types at the outset Define data structures which will hold your input and output data. Since Python allows using data structures without declaring them implicitly, this can be done by having validation functions which are invoked whenever the data structure is used. This would ensure that the structure is consistent throughout the project. An example of a data structure validation for an “object” type (which is a dict with just one key) is below.\ndef validate_object(x): \u0026quot;\u0026quot;\u0026quot;This function validates an object x that is supposed to represent an object inside an image, and throws an exception on failure. Specifically it is checking that: x['polygon'] is a list of \u0026gt;= 3 integer (x,y) pairs representing the corners of the polygon in clockwise or anticlockwise order. \u0026quot;\u0026quot;\u0026quot; if type(x) != dict: raise ValueError('dict type input required.') if 'polygon' not in x: raise ValueError('polygon object required.') if not isinstance(x['polygon'], (list,)): raise ValueError('list type polygon object required.') points_list = x['polygon'] if len(points_list) \u0026lt; 3: raise ValueError('More than two points required.') for x, y in points_list: if type(x) != int or type(y) != int: raise ValueError('integer (x,y) pairs required.') return  2. Write data loader scripts for all your datasets Now that wehave common data structures to use with our model(s), we need to convert all our datasets to that format. There are 2 ways to achieve this:\n Preprocess the dataset to the required structure and save in a serialized file (e.g. Pickle in Python). Have a data loader class to read the dataset from source at the time of running and return in the desired format.  When should you use the second method? When the dataset itself is large, or we need to have several additional elements in the structure, such as mask data (for an image), or associate word vectors (for text data).\nAdditionally, if you have a decent processor and parallelizable script, the compute time in method 2 should be low enough such that the total runtime in 1 becomes larger due to greater I/O time.\n3. Put common methods in a shared library Since all the datasets are in a common structure, several transformation methods may be applicable to many of them. So it would make sense to have these methods in a global shared library and link to this library inside each of the local dataset directories. This achieves 2 things:\n Reduces clutter and reduplication in the directory. Allows for ease in making modifications to the shared functions.  4. Write unit tests for utility functions Instead of writing a test file and modifying it for testing the utility functions, it would be better to use the unittest package in Python, or analogous packages in other languages. For example, in an object detection project, there may be utilities to visualize the object with a mask, or to compress the image. The unit test file may then look like this.\nimport unittest class ImageUtilsTest(unittest.TestCase): \u0026quot;\u0026quot;\u0026quot;Testing image utilities: visualization and compression \u0026quot;\u0026quot;\u0026quot; def setUp(self): \u0026quot;\u0026quot;\u0026quot;This method sets up objects for all the test cases. \u0026quot;\u0026quot;\u0026quot; \u0026lt;code for loading data\u0026gt; def test_visualize_object(self): \u0026quot;\u0026quot;\u0026quot;Given a dictionary object as follows x['img']: numpy array of shape (height,width,colors) x['mask']: numpy array of shape (height,width), with every element categorizing it into one of the object ids The method generates an image overlaying a translucent mask on the image. \u0026quot;\u0026quot;\u0026quot; visualize_mask(self.test_object) def test_compress_object(self): \u0026quot;\u0026quot;\u0026quot;Given a dictionary object x, the method compresses the object and prints the original and compressed sizes. It also asserts that the original size should be greater than the compressed size. \u0026quot;\u0026quot;\u0026quot; y = compress_image_with_mask(self.test_object,self.c) x_mem = sys.getsizeof(self.test_object) y_mem = sys.getsizeof(y) self.assertTrue(y_mem \u0026lt;= x_mem) if __name__ == '__main__': unittest.main()  5. Prepare installation and run scripts This is key for reproducibility. It is very arrogant to assume that readers would clone your repository, install several dependencies one by one, then download the datasets, preprocess the data using some script in your repo, and only then be able to start training. All of these steps can and should be automated using simple Bash shell scripts, so that the users can just run an install.sh or a run.sh file with certain parameters to get things done.\nIf you have built a small library providing some functionality, say a text classification library, it would be best if you package it and make it available for download via a manager such as pip so that the package can be used directly in other projects.\nIn any case, installation and run instructions should be documented elaborately in a README file.\n6. Put parameter tuning options as command line arguments In continuation with #5, the user should never be expected to open your training script to tune hyperparameters, or provide path to data directories, or other similar stuff. Python has the argparse library which facilitates parsing command line arguments, and it is insanely simple to use. Bash has the parser available by default and the arguments can be accessed using the numbered variables $0, $1, and so on. Similar functionalities are available for almost every programming language.\nimport argparse parser = argparse.ArgumentParser(description='Process some integers.') parser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator') parser.add_argument('--sum', dest='accumulate', action='store_const', const=sum, default=max, help='sum the integers (default: find the max)') args = parser.parse_args() print(args.accumulate(args.integers))  7. Have a defined naming convention for saved models and result files Your saved model and result file names should indicate the important hyperparameters that were used in that instance of training. This simplifies testing with several available models later during analysis.\n8. Get your code reviewed before merging I can’t stress this enough. Regardless of how sincere you have been in your coding, your commits would still be flawed in some way. Having a reviewer always helps, even if it is to point out some pep8 naming convention.\nIn my undergrad thesis project, I was the sole contributor, and so there used to be several weeks in which I didn’t push any code, arguing that it was all there in my local system anyway. I would think of Git as an additional time-consuming formality, instead of the immensely useful tool it is. Don’t make that mistake!\nI hope these guidelines are useful to some researcher who is just starting out on her first project. Yes, it would take some time to get used to following all these rules, but trust me, your research would only be the better for it in the long run!\n","date":1527063110,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531407517,"objectID":"4c8069625e975cd214663fe29d2d351c","permalink":"https://desh2608.github.io/post/8-commandments-for-coding-research/","publishdate":"2018-05-23T13:41:50+05:30","relpermalink":"/post/8-commandments-for-coding-research/","section":"post","summary":"This article is in a different flavor from the other posts in this publication. This is because I have been reading Roman Vershynin’s \u0026ldquo;High Dimensional Probability\u0026rdquo; for the last few days, and between that and visa formalities, I didn’t get a chance to check out new papers. I do plan to write an article on new methods for object detection (such as RCNN, Faster RCNN, and YOLO) sometime next month.","tags":["research"],"title":"The 8 Commandments for Coding Your Research","type":"post"},{"authors":null,"categories":[],"content":" Until now, all of my blog posts have been about deep learning methods or their application to NLP. Since the last couple of weeks, however, I have started learning about Automatic Speech Recognition (ASR)1. Therefore, I will also include speech-related articles in this publication now.\nThe ASR logic is very simple (it’s just Bayes rule, like most other things in machine learning). Essentially, given a speech waveform, the objective is to transcribe it, i.e., identify a text which aligns with the waveform. Suppose $Y$ represents the feature vectors obtained from the waveform (Note: this “feature extraction” itself is an involved procedure, and I will describe it in detail in another post), and $\\mathbf{w}$ denotes an arbitrary string of words. Then, we have the following.\n$$ \\hat{\\mathbf{w}} = \\text{arg}\\max_{\\mathbf{w}} { P(\\mathbf{w}|Y)} = \\text{arg} \\max_{\\mathbf{w}} {P(Y|\\mathbf{w})P(\\mathbf{w}) } $$\nThe two likelihoods in the term are trained separately. The first component, known as acoustic modeling, is trained using a parallel corpus of utterances and speech waveforms. The second component, called language modeling, is trained in an unsupervised fashion from a large corpus of text.\nAlthough the ASR training appears simple from this abstract level, the implementation is arguably more complex, and is usually done using Weighted Finite State Transducers (WFSTs). In this post, I’ll describe WFSTs, some of their basic algorithms, and give a brief introduction to how they are used for speech recognition.\nWeighted Finite State Transducers (WFSTs) If you have taken any Theory of Computation course before, you’d probably already be aware what an automata is. Essentially, a finite automaton accepts a language (which is a set of strings). They are represented by directed graphs as shown below.\nEach such automaton has a start state, one or more final states, and labeled edges connecting the states. A string is accepted if it ends in a final state after traversing through some path in the graph. For instance in the above DFA (deterministic finite automata), a, ac, and ae are allowed.\nSo an acceptor maps any input string to a binary class {0,1} depending on whether or not the string is accepted. A transducer, on the other hand, has 2 labels on each edge — an input label, and an output label. Furthermore, a weighted finite state transducer has weights corresponding to each edge and every final state.\nTherefore, a WFST is a mapping from a pair of strings to a weight sum. The pair is formed from the input/output labels along any path of the WFST. For pairs which are not possible in the graph, the corresponding weight is infinite.\nIn practice, there are libraries available in every language to implement WFSTs. For C++, OpenFST is a popular library, which is also used in the Kaldi speech recognition toolkit.\nIn principle, it is possible to implement speech recognition algorithms without using WFSTs. However, these data structures have several proven results2 and algorithms which can directly be used in ASRs without having to worry about correctness and complexity. These advantages have made WFSTs almost omniscient in speech recognition. I’ll now summarize some algorithms on WFSTs.\nSome basic algorithms on WFSTs Composition Composition, as the name suggests, refers to the process of combining 2 WFSTs to form a single WFST. If we have transducers for pronunciation and word-level grammar, such an algorithm would enable us to form a phone-to-word level system easily.\nComposition is done using 3 rules:\n Initial state in the new WFST are formed by combining the initial states of the old WFSTs into pairs. Similarly, final states are combined into pairs. For every pair of edges such that the o-label of the first WFST is the i-label of the second, we add an edge from the source pair to the destination pair. The edge weight is summed using the sum rules.  An example of composition is shown below.\nAt this point, it may be important to define what \u0026ldquo;sum\u0026rdquo; means for edge weights. Formally, the \u0026ldquo;languages\u0026rdquo; accepted by WFSTs are generalized through the notion of semirings. Basically, it is a set of elements with 2 operators, namely $\\oplus$ and $\\otimes$. Depending on the type of semiring, these operators can take on different definitions. For example, in a tropical semiring, $\\oplus$ denotes $\\min$, and $\\otimes$ denotes sum. Furthermore, in any WFST, weights are $\\otimes$-multiplied along paths (Note: here “multiplied” would mean summed for a tropical semiring) and $\\oplus$-summed over paths with identical symbol sequence.\nSee here for OpenFST implementation of composition.\nDeterminization A deterministic automaton is one in which there is only one transition for each label in every state. By such a formulation, a deterministic WFST removes all redundancy and greatly reduces the complexity of the underlying grammar. But, are all WFSTs determinizable?\nThe Twins Property: Let us consider an automaton A. Two states p and q in A are said to be siblings if both can be reached by string x and both have cycles with label y. Essentially, siblings are twins if the total weight for the paths until the states, as well as that including the cycle, are equal for both.\n A WFST is determinizable if all its siblings are twins.\n This is an example of what I said earlier regarding WFSTs being an efficient implementation of the algorithms used in ASR. There are several methods to determinize a WFST. One such algorithm is shown below.\nIn simpler steps, this algorithm does the following:\n At each state, for every outgoing label, if there are multiple outgoing edges for that label, replace them with a single edge with weight as the $\\otimes$-sum of all edge weights containing that label.  Since this is a local algorithm, it can be efficiently implemented in-memory. To see how to perform determinization in OpenFST, see here.\nMinimization Although minimization is not as essential as determinization, it is still a nice optimization technique. It refers to minimizing the number of states and transitions in a deterministic WFST.\nMinimization is carried out in 2 steps:\n Weight pushing: All weights are pushed towards the start state. See the following example.   After this is done, we combine those states which have identical paths to any final state. For example in the above WFST, states 1 and 2 have become identical after weight pushing, so they are combined into one state.  In OpenFST, the implementation details for minimization can be found here.\nThe following3 shows the complete pipeline for a WFST reduction.\nWFSTs in speech recognition Several WFSTs are composed in sequence for use in speech recognition. These are:\n Grammar (G): This is the language model trained on large text corpus. Lexicon (L): This encodes information about the likelihood of phones without context. Context-dependent phonetics (C ): This is similar to n-gram language modeling, except that it is for phones. HMM structure (H): This is the model for the waveform.  In general, the composed transducer HoCoLoG represents the entire pipeline of speech recognition. Each of the components can individually be improved, so that the entire ASR system gets improved.\nThis was just a brief introduction to WFSTs which are an important component in ASR systems. In further posts on speech, I hope to discuss things such as feature extraction, popular GMM-HMM models, and latest deep learning advances. I am also reading papers mentioned here to get a good overview of how ASR has progressed over the years.\n Gales, Mark, and Steve Young. \u0026ldquo;The application of hidden Markov models in speech recognition.\u0026rdquo; Foundations and Trends in Signal Processing 1.3 (2008): 195–304. ^ Mohri, Mehryar, Fernando Pereira, and Michael Riley. \u0026ldquo;Weighted finite-state transducers in speech recognition.\u0026rdquo; Computer Speech \u0026amp; Language 16.1 (2002): 69–88. ^ Lecture slides from Prof. Hui Jiang (York University) ^   ","date":1524471091,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531376430,"objectID":"34480372faf33524345b6e2d58849abf","permalink":"https://desh2608.github.io/post/intro-speech-recognition-wfst/","publishdate":"2018-04-23T13:41:31+05:30","relpermalink":"/post/intro-speech-recognition-wfst/","section":"post","summary":"Until now, all of my blog posts have been about deep learning methods or their application to NLP. Since the last couple of weeks, however, I have started learning about Automatic Speech Recognition (ASR)1. Therefore, I will also include speech-related articles in this publication now.\nThe ASR logic is very simple (it’s just Bayes rule, like most other things in machine learning). Essentially, given a speech waveform, the objective is to transcribe it, i.","tags":["machine learning","speech recognition"],"title":"An Introduction to Speech Recognition using WFSTs","type":"post"},{"authors":null,"categories":[],"content":" In several of my previous posts, I have discussed methods for obtaining word embeddings, such as SVD, word2vec, or GloVe. In this post, I will abstract a level higher and talk about 4 different methods that have been proposed to get embeddings for sentences.\nBut first, some of you may ask why do we even need a different method for obtaining sentence vectors. Since sentences are essentially made up of words, it may be reasonable to argue that simply taking the sum or the average of the constituent word vectors should give a decent sentence representation. This is akin to a bag-of-words representation, and hence suffers from the same limitations, i.e.\n It ignores the order of words in the sentence. It ignores the sentence semantics completely.  Other word vector based approaches are also similarly constrained. For instance, a weighted average technique again loses word order within the sentence. To remedy this issue, Socher et al. combined the words in the order given by the parse tree of the sentence. While this technique may be suitable for complete sentences, it does not work for phrases or paragraphs.\nIn an earlier post, I discussed several ways in which sentence representations are obtained as an intermediate step during text classification. Several approaches are used for this purpose, such as character to sentence level feature encoding, parse trees, regional (two-view) embeddings, and so on. However, the limitation with such an \u0026ldquo;intermediate\u0026rdquo; representation is that the vectors obtained are not generic in that they are closely tied to the classification objective. As such, vectors obtained through training on one objective may not be extrapolated for other tasks.\nIn light of this discussion, I will now describe 4 recent methods that have been proposed to obtain general sentence vectors. Note that each of these belongs to either of 2 categories: (i) inter-sentence, wherein the vector of one sentence depends on its surrounding sentences, and (ii) intra-sentence, where a sentence vector only depends on that particular sentence in isolation.\nParagraph Vectors In this ICML’14 paper1 from Mikolov (who also invented word2vec), the authors propose the following solution: a sentence vector can be learned simply by assigning an index to each sentence, and then treating the index like any other word. This is shown in the following figure.\nEssentially, every paragraph (or sentence) is mapped to a unique vector, and the combined paragraph and word vectors are used to predict the next word. Through such a training, the paragraph vectors may start storing missing information, thus acting like a memory for the paragraph. For this reason, this method is called the Distributed Memory model (PV-DM).\nTo obtain the embeddings for an unknown sentence, an inference step needs to be performed. A new column of randomly initialized values is added to the sentence embedding matrix. The inference step is performed keeping all the other parameters fixed to obtain the required vector.\nThe PV-DM model requires a large amount of storage space since the paragraph vectors are concatenated with all the vectors in the context window at every training step. To solve this, the authors propose another model, called the Distributed BOW (PV-DBOW), which predicts random words in the context window. The downside is that this model does not use word order, and hence performs worse than PV-DM.\nSkip-thoughts While PV was an intra-sentence model, skip-thoughts2 is inter-sentence. The method uses continuity of text to predict the next sentence from the given sentence. This also solves the problem of the inference step that is present in the PV model. If you have read about the skip-gram algorithm in word2vec, skip-thoughts is essentially the same technique abstracted to the sentence level.\nIn the paper, the authors propose an encoder-decoder framework for training, with an RNN used for both encoding and decoding. In addition to a sentence embedding matrix, this method also generates vectors for the words in the corpus vocabulary. Finally, the objective function to be maximized is as follows.\n$$ \\sum_t \\log P(w_{i+1}^t|w_{i+1}^{\u0026lt; t},\\mathbf{h}_i) + \\sum_t \\log P(w_{i-1}^t|w_{i-1}^{\u0026lt; t},\\mathbf{h}_i) $$\nHere, the indices $i+1$ and $i-1$ represent the next sentence and the previous sentence, respectively. Overall, the function represents the sum of log probabilities of correctly predicting the next sentence and the previous sentence, given the current sentence.\nSince word vectors are also precited at training time, a problem may arise at the time of inference if the new sentence contains an OOV word. To solve this, the authors present a simple solution for vocabulary expansion. We assume that any word, even if it is OOV, will definitely come from some vector space (say w2v), such that we have its vector representation in that space. As such, every known word has 2 representations, one in the RNN space and another in the w2v space. We can then identify a linear transformation matrix that transforms w2v space vectors into RNN space vectors, and this matrix may be used to obtain the RNN vectors for OOV words.\nFastSent This model, proposed by Kyunghun Cho3, is also an inter-sentence technique, and is conceptually very similar to skip-thoughts. The only difference is that it uses a BOW representation of the sentence to predict the surrounding sentences, which makes it computationally much more efficient than skip-thoughts. The training hypothesis remains the same, i.e., rich sentence semantics can be inferred from the content of adjacent sentences. Since the details of the method are same as skip-thoughts, I will not repeat them here to avoid redundancy.\nSequential Denoising Autoencoders (SDAE) This technique was also proposed in the same paper3 as FastSent. However, it is essentially an intra-sentence method wherein the objective is to regenerate a sentence from a noisy version.\nIn essence, in an SDAE, a high-dimensional input data is corrupted according to some noise function and the model is trained to recover the original data from the corrputed version.\nIn the paper, the noise function $N$ uses 2 parameters as follows.\n For each word $w$ in the sentence $S$, $N$ deletes it according to some probability $p_0$. For each non-overlapping bigram in $S$, $N$ swaps the bigram tokens with probability $p_x$.  These are inspired from the “word dropout” and “debagging” approaches, respectively, which have earlier been studied in some detail.\nIn the last paper3, the authors have performed detailed empirical evaluations of several sentence vector methods, including all of the above. From this analysis, the following observations can be drawn,\n Task-dependency: Although the methods intend to produce general sentence representations which work well across different tasks, it is found that some methods are more suitable from some tasks due to the inherent algorithm. For instance, skip-thoughts perform well on textual entailment tasks, whereas SDAEs perform much better on paraphrase detection. Inter vs. intra: The inter-sentence models generate similar vectors in that their nearest neighbors are those sentences which have shared concepts. In contrast, for the intra-sentence models, these are sentences which have more overlapping words. Dependency on word order: Although the widely held view is that word order is critical for sentence vectors, the average score for models which are sensitive to word order was found to be almost equal to those which are not. It was even lower for RNN models in unsupervised objectives, which is indeed surprising. One explanation for this may be that the sentences in the dataset, or the evaluation techniques, are not robust enough so as to sufficiently challenge simple word frequency based techniques.   Le, Quoc, and Tomas Mikolov. “Distributed representations of sentences and documents.” International Conference on Machine Learning. 2014. ^ Kiros, Ryan, et al. “Skip-thought vectors.” Advances in neural information processing systems. 2015. ^ Hill, Felix, Kyunghyun Cho, and Anna Korhonen. “Learning distributed representations of sentences from unlabelled data.” arXiv preprint arXiv:1602.03483 (2016). ^   ","date":1523520674,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531376430,"objectID":"969f9440d1dadfb9667a65198c146665","permalink":"https://desh2608.github.io/post/how-to-obtain-sentence-vectors/","publishdate":"2018-04-12T13:41:14+05:30","relpermalink":"/post/how-to-obtain-sentence-vectors/","section":"post","summary":"In several of my previous posts, I have discussed methods for obtaining word embeddings, such as SVD, word2vec, or GloVe. In this post, I will abstract a level higher and talk about 4 different methods that have been proposed to get embeddings for sentences.\nBut first, some of you may ask why do we even need a different method for obtaining sentence vectors. Since sentences are essentially made up of words, it may be reasonable to argue that simply taking the sum or the average of the constituent word vectors should give a decent sentence representation.","tags":["representation learning"],"title":"How to Obtain Sentence Vectors","type":"post"},{"authors":null,"categories":[],"content":" Word vectors have become the building blocks for all natural language processing systems. I have earlier written an overview of popular algorithms for learning word embeddings here. One limitation with all these methods (namely SVD, skip-gram, and GloVe) is that they are all “batch” techniques. In this post, I will discuss two recent papers (which are very similar but were developed independently) which aim to provide an online approximation for the skip-gram algorithm.\nBut first, what do we mean by a “batch” algorithm?\nSimply put, in a batch algorithm, the entire data set needs to be available before we begin the processing. In contrast, an “online” algorithm can process inputs on-the-fly, i.e., in a streaming fashion. Needless to say, such algorithms are also preferable when the available resources are not sufficient to process the entire dataset at once.\nNow that we have some idea about batch algorithms, I’ll explain why the existing methods for word representation learning are of this kind. First, in the case of the standard SVD and Stanford’s GloVe, the entire cooccurence matrix needs to be computed, and only then can the processing be started. If some additional data arrives later, the matrix would have to be recomputed, and training would have to be restarted (if at least one of the updates depends on a changed matrix element). Second, in the case of Mikolov’s word2vec (skip-gram and CBOW), negative sampling is often used to make the computation more efficient. This sampling depends on the unigram probability distribution of the vocabulary words in the corpus. As such, before learning can happen, we need to compute the vocabulary as well as the unigram distribution.\nRecently, two very similar methods (developed independently) have been proposed to make the skip-gram with negative sampling (SGNS) algorithm learn in a streaming fashion. I’ll quickly review the SGNS algorithm first so that there is some context when we discuss the papers.\nBatch SGNS algorithm SGNS is a window-based method with the following training objective: Given the target word, predict all the context words in the window.\nSuppose we have a context window where $w$ is the target word and $c$ is one of the context words. Then, skip-gram’s objective is to compute $P(c|w)$, which is given as\n$$ p(c|w;\\theta) = \\frac{\\exp(v_c \\cdot v_w)}{\\sum_{c^{\\prime}\\in C}\\exp(v_{c^{\\prime}}\\cdot v_w)} $$\nBasically, it is just a softmax probability distribution over all the word-context pairs in the corpus, directed by the cosine similarity. However, the denominator term here is very expensive to compute since there may be a very large number of possible context words. To solve this problem, negative sampling is used.\nGoldberg and Levy have explained the derivation for the objective function in SGNS very clearly in their note. I will try to provide a little intuition here.\nFor the word $w$, we are trying to predict the context word $c$. Since we are using softmax, this is essentially like a multi-class classification problem, where we are trying to classify the next word into one of $N$ classes (where $N$ is the number of words in the dictionary). Since $N$ may be quite large, this is a very difficult problem.\nWhat SGNS does is that it converts this multi-classification problem into binary classification. The new objective is to predict, for any given word-context pair $(w,c)$, whether the pair is in the window or not. For this, we try to increase the probability of a \u0026ldquo;positive\u0026rdquo; pair $(w,c)$, while at the same time reducing the probability of $k$ randomly chosen \u0026ldquo;negative samples\u0026rdquo; $(w,s)$ where $s$ is a word not found in $w$’s context. This leads to the following objective function which we try to maximize in SGNS:\n$$ J = \\log \\sigma(c\\cdot w) + \\sum_{i=1}^k \\mathbb{E}_{w_i \\sim p(w)}[\\log \\sigma (-w_i \\cdot w)] $$\nIn other words, we push the target vector in the direction of the positive context vector, and pull it away from $k$ randomly chosen (w.r.t. the unigram probability distribution) negative vectors. Here \u0026ldquo;negative\u0026rdquo; means that these vectors are not actually present in the target’s context.\nWhat do we need to make SGNS online? As is evident from the above discussion, since SGNS is a window-based approach, the training itself is very much in an online paradigm. However, the constraints are in creating a vocabulary and a unigram distribution for negative sampling, which makes SGNS a two-pass method. Further, if additional data is seen later, the distribution and vocabulary would change, and the model would have to be retrained.\nEssentially, we need online alternatives for 2 aspects of the algorithms:\n Dynamic vocabulary building Adaptive unigram distribution  With this background, I will now discuss the two proposed methods for online SGNS.\nSpace-Saving word2vec In this paper1 from researchers at Johns Hopkins, the following solutions were proposed for the two problems mentioned above.\n Space-saving algorithm for dynamic vocabulary building. Reservoir sampling for adaptive unigram distribution.  Space-saving algorithm: It is a popular method to estimate the top-$k$ most frequent items in a streaming data.\n We declare a structure V containing $k$ pairs of word and their counts, and initialize it to empty pairs. As word $w$ arrives, if $w \\in V$, we increment its count. Otherwise, if $V$ has space, we append the pair $(w,1)$ to $V$. If not, the word with the lowest count is replaced by $w$.  At any instant, the words in the structure V denote the dynamic vocabulary of the corpus.\nReservoir sampling: Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of $k$ items from a list S containing $n$ items, where $n$ is either a very large or unknown number. (Wikipedia)\n Similar to the SS algorithm, we declare a structure (called the reservoir) of $k$ empty elements (not pairs this time). In addition, we initialize a counter $c$ to 0. The first $k$ elements in the stream are filled into the reservoir. $c$ is incremented at every occurence. For the remaining items, we draw $j$ from $1,\\ldots,c$ randomly. If $j \u0026lt; k$, the $j^{\\text{th}}$ element of the reservoir is replaced with the new element.  At any instant, the samples present in the reservoir provide an approximate distribution of items in the entire data stream.\nWhile the algorithm itslelf is conceptually simple, the authors have mentioned several implementation choices which are important for training SGNS online. I list them here with some observations:\n When a word is ejected from a bin in the dynamic vocabulary, its embeddings are re-initialized. As such, every bin has its own learning rate which is reset when the word in the bin is changed. During sentence subsampling, all words not in $W$ are retained. Those in $W$ are retained with a probability which is inversely proportional to the square root of its count in the dictionary. Probably the most important deviation from the SGNS algorithm is that the reservoir sampling essentially generates an empirical distribution from which to sample negative context words. In contrast, in the original SGNS algorithm, a smoothed empirical distribution is used. The authors have themselves allowed that “ smoothing the negative sampling distribution was (sic) shown to increase word embedding quality consistently.”  Incremental SGNS This EMNLP’17 paper2 from researchers at Yahoo Japan proposes the following alternative solutions for the aforementioned problems.\n Misra-Gries algorithm for dynamic vocabulary building. A modified reservoir sampling algorithm for adaptive unigram table.  Misra-Gries algorithm: This was developed long before the space-saving algorithm (1982) and was the go-to technique for top-$k$ most frequent itemset estimation in streaming data, before the space-saving algorithm was developed. The method is very similar to SS except for one difference:\n When word $w$ is not in $V$ and there is no space to append, every element in $V$ is decremented until some element becomes 0, at which point it is replaced by the new word.  Modified reservoir sampling: Here is the pseudocode from the paper.\nThis algorithm differs from the conventional Reservoir Sampling in two important ways:\n The counts used here are smoothed (see line 4 to 6). This has been shown to be important for word vector quality, as discussed above. If the reservoir does not have enough space, we iterate over all existing words and replace them with some probability (which is proportional to the smoothed count of $w$). Contrast this with the earlier technique, where a $j$ was randomly sampled and word at that index was replaced. (Disclaimer: I am not sure how exactly this modification helps in learning. If I am allowed to venture a guess, I would say that it is a “soft” equivalent of the hard replacement in the original algorithm. This probably helps in the theoretical analysis of the algorithm.)  In addition, the authors have also provided theoretical justification for their algorithm and proved the following theorem: The loss in case of incremental SGNS converges in probability to that of batch SGNS.\nIn summary, SGNS is probably the easiest batch word embedding algorithm to “streamify” because of its inherent window-based nature. The constraints of vocabulary and counts are addressed with approximation algorithms. I can think of several possible directions in which this work can be continued.\nFirst, there are several algorithms for estimating the top-$k$ most frequent items in a data stream. These are divided into count-based and sketch-based methods. The SS algorithm is probably the most efficient count-based technique, but it may be useful to look at other methods to see if they provide some edge. (Although I’m pretty sure the JHU researchers would have been thorough in their selection of the algorithm.)\nSecond, GloVe and SVD are yet to be addressed. In case of GloVe in particular, the problem would be to construct the co-occurence matrix in a online fashion. There should be some related work in statistics which can be leveraged for this, but I haven’t conducted much literature survey in this direction.\n May, Chandler, Kevin Duh, Benjamin Van Durme, and Ashwin Lall. \u0026ldquo;Streaming word embeddings with the space-saving algorithm.\u0026rdquo; arXiv preprint arXiv:1704.07463 (2017). ^ Kaji, Nobuhiro, and Hayato Kobayashi. \u0026ldquo;Incremental skip-gram model with negative sampling.\u0026rdquo; arXiv preprint arXiv:1704.03956 (2017).* ^   ","date":1521015057,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531376430,"objectID":"8ba152566cc1ce8b8216821c71908995","permalink":"https://desh2608.github.io/post/online-learning-word-embeddings/","publishdate":"2018-03-14T13:40:57+05:30","relpermalink":"/post/online-learning-word-embeddings/","section":"post","summary":"Word vectors have become the building blocks for all natural language processing systems. I have earlier written an overview of popular algorithms for learning word embeddings here. One limitation with all these methods (namely SVD, skip-gram, and GloVe) is that they are all “batch” techniques. In this post, I will discuss two recent papers (which are very similar but were developed independently) which aim to provide an online approximation for the skip-gram algorithm.","tags":["online learning","representation learning"],"title":"Online Learning of Word Embeddings","type":"post"},{"authors":null,"categories":[],"content":" Sparse vectors have become popular recently for 2 reasons:\n Sparse matrices require much less storage since they can be stored using various space-saving methods. Sparse vectors are much more interpretable than dense vectors. For instance, the non-zero non-negative components of a sparse word vector may be taken to denote the weights for certain features. In contrast, there is no interpretation for a value like $-0.1347$.  Sparsity is often induced through the use of L1 (or Lasso) regularization. There are 2 formulations of the Lasso: (i) convex constraint, and (ii) soft regularization.\nConvex constraint\nAs the name suggests, a convex constraint is added to the minimization problem so that the parameters do not exceed a certain value.\n$$ \\min_{\\beta \\in \\mathbb{R}^p}\\lVert y - X\\beta \\rVert_2^2 \\quad \\text{s.t.} \\quad \\lVert \\beta \\rVert_1 \\leq t $$\nThe smaller the value of the tuning parameter $t$, fewer is the number of non-zero components in the solution.\nSoft regularization\nThis is just the Lagrange form the the convex constraint, and is used because it is easier to optimize. Note that it is equivalent to the convex constraint formulation for an appropriately chosen $g$.\n$$ \\min_{\\beta \\in \\mathbb{R}^p}\\lVert y - X\\beta \\rVert_2^2 + g\\lVert \\beta \\rVert_1 $$\nThere is a great theoretical explanation of sparsity with Lasso regularization by Ryan Tibshirani and Larry Wasserman which you can find here. I will instead be focusing on some methods that have been introduced recently for inducing sparsity while learning online i.e., when the samples are obtained one at a time. In addition to such a scenario, online learning also comes into the picture when the data set is simply too large to be loaded in memory at once, and there are not sufficient resources for performing batch learning in a parallel fashion.\nIn this post, I will summarize 3 such methods:\n Stochastic Truncated Gradient1 Forward Backward Splitting2 Regularized Dual Averaging3  But first, why a simple soft Lasso regularization won’t work? With the soft regularization method, we are essentially summing up 2 floating point values. As such, it is highly improbable that the sum will be zero, since very few pairs of floats add up to zero.\nStochastic Truncated Gradient (STG) STG combines ideas from 2 simple techniques:\n Coefficient rounding: In this method, the coefficients are rounded to 0 if they are less than a value $\\theta$. This is denoted in the figure above (left graph). The rounding is done after every $k$ steps. The problem with this approach is that if $k$ is small, the coefficients do not get an opportunity to reach a value above $\\theta$ before they are pulled back to $0$. On the other hand, if $k$ is large, the intermediate steps in the algorithm need to store a large number of non-zero coefficients, which does not solve the storage issue. Sub-gradient method: In this method, L1-regularization is performed by shifting the update in the opposite direction depending on the sign of the coefficient. The update equation is  $$ f(w_i) = w_i - \\eta\\nabla_1 L(w_i,z_i) - \\eta g \\text{sgn}(w_i) $$\nSTG combines rounding from (1) and gravity from (2) so that (i) sparsity is achieved (unlike the sub-gradient method), and (ii) the rounding off is not too aggressive (unlike the direct rounding approach). The parameter update is then given by the function $T_1$ (shown in the right graph above).\n$$ T_1(v_j,\\alpha,\\theta) = \\begin{cases} \\max(0,v_j-\\alpha) \\quad \u0026amp;\\text{if}~ v_j \\in [0,\\theta] \\\\\\ \\min(0,v_j+\\alpha) \\quad \u0026amp;\\text{if}~ v_j \\in [-\\theta,0] \\\\\\ 0 \\quad \u0026amp;\\text{otherwise} \\end{cases} $$\nThe update rule is given using $T_1$ as\n$$ f(w_i) = T_1 (w_i - \\nabla_1 L_1 (w_i,z_i,\\eta g_i,\\theta)) $$\nHere, $g$ may be called the gravity parameter, and $\\theta$ is called the truncation parameter. In general, the larger these parameters are, the more sparsity is incurred. This can be understood easily from the definition of the truncation function.\nFurthermore, note that on setting $\\theta = \\infty$ in the truncation function yields a special case of the Sub-gradient method wherein max and min operations are performed after applying gravity pull.\nIn the remainder of the paper, the authors prove a strong regret bound for the STG method, and also provide an efficient implementation for the same. Furthermore, they show the asymptotic solution of one instance of the algorithm is essentially equivalent to the Lasso regression, thus justifying the algorithm’s ability to produce sparse weight vectors when the number of features is intractably large.\nForward Backward Splitting (FOBOS) Note: The method was named Forward Looking Subgradient (FOLOS) in the first draft and later renamed since it was essentially the same as an earlier proposed technique, the Forward Backward Splitting. The authors abbreviated it to FOBOS instead of FOBAS to avoid confusing readers of the first draft.\nFirst, a little background. Consider an objective function of the form $f(w) + r(w)$. In the case of a number of machine learning algorithms, the function $f$ denotes the empirical sum of some loss function (such as mean squared error), and the function $r$ is a regularizer (such as Lasso). If we use a simple gradient descent technique to minimize this objective function, the iterates would be of the form\n$$ w_{t+1} = w_t - \\eta_t g_t^f - \\eta_t g_t^r $$\nwhere the $g$’s are vectors from the subgradient sets of the corresponding functions. From the paper:\n A common problem in subgradient methods is that if $r$ or $f$ is non-differentiable, the iterates of the subgradient method are very rarely at the points of non-differentiability. In the case of the Lasso regularization function, however, these points are often the true minima of the function.\n In other words, the subgradient approach will result in neither a true minima nor a sparse solution if $r$ is the L1 regularizer.\nFOBOS, as the name suggests, splits every iteration into 2 steps — a forward step and a backward step, instead of minimizing both $f$ and $r$ simultaneously. The motivation for the method is that for L1 regularization functions, true minima is usually attained at the points of non-differentiability. For example, in the 2-D space, the function resembles a Diamond shape and the minima is obtained at one of the corner points. Each iteration of FOBOS consists of the following 2 steps:\n$$ w_{t+\\frac{1}{2}} = w_t - \\eta_t g_t^f \\\\\\ w_{t+1} = \\text{argmin}_w { \\frac{1}{2}(w_t - w_{t+\\frac{1}{2}})^2 + \\eta_{t+\\frac{1}{2}}r(w) } $$\nThe first step is a simple unconstrained subgradient step with respect to the function $f$. In the second step, we try to achieve 2 objectives:\n Stay close to the interim update vector. This is achieved by the first term. Attain a low complexity value as expressed by $r$. (Second term)  So the first step is a forward step, where we update the coefficient in the direction of the subgradient, while the second is a backward step where we pull the update back a little so as to obtain sparsity by moving in the direction of the non-differentiable points of $r$.\nUsing the first equation in the second, taking derivative w.r.t $w$, and equating the derivative to $0$, we obtain the update scheme as\n$$ w_{t+1} = w_t - \\eta_t g_t^f + \\eta_{t+\\frac{1}{2}} g_{t+1}^r $$\n(Note: The equation above looks suspiciously similar to the Nesterov Accelerated Gradient (NAG) method for optimization. The authors have even cited Nesterov’s paper in related work. It might be interesting to investigate this further.)\nThis update scheme has 2 major advantages, according to the author.\n First, from an algorithmic standpoint, it enables sparse solutions at virtually no additional computational cost. Second, the forward-looking gradient allows us to build on existing analyses and show that the resulting framework enjoys the formal convergence properties of many existing gradient-based and online convex programming algorithms.\n In the paper, the authors also prove convergence of the method and show that on setting the intermediate learning rate properly, low regret bounds can be proved for both online as well as batch settings.\nRegularized Dual Averaging (RDA) Both of the above discussed techniques have one limitation — they perform updates depending only on the subgradients at a particular time step. In contrast, the RDA method “exploits the full regularization structure at each iteration.” Also, since the authors derive closed-form solutions for several popular optimization objectives, it follows that the computational complexity of such an approach is not worse than the methods which perform updates only based on current subgradients (both being $\\mathcal{O}(n)$).\nRDA comprises of 3 steps in every iteration.\nIn the first step, the subgradient is computed for that particular time step. This is the same as every other subgradient-based online optimization method.\nThe second step consists of computing a running average of all past subgradients. This is done using the online approach as\n$$ \\bar{g}_t = \\frac{t-1}{t}\\bar{g}_{t-1} + \\frac{1}{t}g_t $$\nIn the third step, the update is computed as\n$$ w_{t+1} = \\text{argmin}_w { \u0026lt;\\bar{g}_t,w\u0026gt; + \\psi(w) + \\frac{\\beta}{t}h(w) } $$\nLet us try to understand this update scheme. First, the function $h(w)$ is a strongly convex function such that the update vector which minimizes it also minimizes the regularizer. In the case of Lasso regularization, $h(w)$ is chosen as follows.\n$$ h(w) = \\frac{1}{2}\\lVert w \\rVert_2^2 + \\rho \\lVert w \\rVert_1 $$\nwhere $\\rho$ is a parameter called the sparsity enhancing parameter. $\\beta$ is a predetermined non-negative and non-decreasing sequence.\nNow to solve the equation, we can just take the derivative of the argument of argmin and equate it to $0$. On solving this equation, we get an update of the form\n$$ w_{t+1} = \\frac{t}{\\beta_t}(\\bar{g}_t + \\rho) $$\nSo the scheme ensures that the update is in the same convex space as the regularized dual average. Sparsity can further be controlled by tuning the value of the parameter $\\rho$. The scaling factor can be regulated using the non-decreasing sequence selected at the beginning of the algorithm. For the case when it is equal to the time step $t$, the new coefficient is simply the sum of the dual average and the sparsity parameter.\nThe above is just my attempt at understanding the update scheme for RDA. I would be happy to discuss it further if you find something wrong with this explanation.\nNow the method itself would become extremely infeasible if this differentiation would have to be performed for every iteration. However, for most commonly used regularizers and loss functions, the update rule can be represented with a closed-form solution. For this reason, the overall algorithm has the same complexity as earlier algorithms which use only the current step subgradient for performing updates.\n Langford, John, Lihong Li, and Tong Zhang. “Sparse online learning via truncated gradient.” Journal of Machine Learning Research 10.Mar (2009): 777–801. ^ Duchi, John, and Yoram Singer. “Efficient online and batch learning using forward backward splitting.” Journal of Machine Learning Research 10.Dec (2009): 2899–2934. ^ Xiao, Lin. “Dual averaging methods for regularized stochastic learning and online optimization.” Journal of Machine Learning Research 11.Oct (2010): 2543–2596. ^   ","date":1519459842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531309945,"objectID":"2042718c8e79aff32eae1950bdd0e48e","permalink":"https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/","publishdate":"2018-02-24T13:40:42+05:30","relpermalink":"/post/sparse-online-learning-lasso-regularization/","section":"post","summary":"Sparse vectors have become popular recently for 2 reasons:\n Sparse matrices require much less storage since they can be stored using various space-saving methods. Sparse vectors are much more interpretable than dense vectors. For instance, the non-zero non-negative components of a sparse word vector may be taken to denote the weights for certain features. In contrast, there is no interpretation for a value like $-0.1347$.  Sparsity is often induced through the use of L1 (or Lasso) regularization.","tags":["machine learning","online learning"],"title":"Sparsity in Online Learning with Lasso Regularization","type":"post"},{"authors":null,"categories":[],"content":" I just finished reading Sebastian Ruder’s amazing article providing an overview of the most popular algorithms used for optimizing gradient descent. Here I’ll make very short notes on them primarily for purposes of recall.\nMomentum The update vector consists of another term which has the previous update vector (weighted by $\\gamma$). This helps it to move faster downhill — like a ball.\n$$ v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta}J(\\theta) $$\nNesterov accelerated gradient (NAG) In Momentum optimizer, the ball may go past the minima due to too much momentum, so we want to have a look-ahead term. In NAG, we take gradient of future position instead of current position.\n$$ v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta}J(\\theta - \\gamma v_{t-1}) $$\nAdagrad Instead of a common learning rate for all parameters, we want to have separate learning rate for each. So Adagrad keeps sum of squares of parameter-wise gradients and modifies individual learning rates using this. As a result, parameters occuring more often have smaller gradients.\n$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t +\\epsilon}} \\odot g_t $$\nRMSProp In Adagrad, since we keep adding all gradients, gradients become vanishingly small after some time. So in RMSProp, the idea is to add them in a decaying fashion as\n$$ \\mathbb{E}[g^2]_t = \\gamma \\mathbb{E}[g^2]_{t-1} + (1-\\gamma)g_t^2 $$\nNow replace $G_t$ in the denominator of Adagrad equation by this new term. Due to this, the gradients are no more vanishing.\nAdam (Adaptive Moment Estimation) Adam combines RMSProp with Momentum. So, in addition to using the decaying average of past squared gradients for parameter-specific learning rate, it uses a decaying average of past gradients in place of the current gradient (similar to Momentum).\n$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t}+\\epsilon}}\\hat{m}_t $$\nThe $\\hat{}$ terms are actually bias-corrected averages to ensure that the values are not biased towards 0.\nNadam Nadam combines RMSProp with NAG (since NAG is usually better for slope adaptation than Momentum. The derivation is simple and can be found in Ruder’s paper.\nIn summary, SGD suffers from 2 problems: (i) being hesitant at steep slopes, and (ii) having same learning rate for all parameters. So the improved algorithms are categorized as:\n Momentum, NAG: address issue (i). Usually NAG \u0026gt; Momentum. Adagrad, RMSProp: address issue (ii). RMSProp \u0026gt; Adagrad. Adam, Nadam: address both issues, by combining above methods.  Note: I have skipped a discussion on AdaDelta in this post since it is very similar to RMSProp and the latter is more popular.\n","date":1518077425,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531309945,"objectID":"8df59239ea77d110fee748836cc881c6","permalink":"https://desh2608.github.io/post/short-note-sgd-algorithms/","publishdate":"2018-02-08T13:40:25+05:30","relpermalink":"/post/short-note-sgd-algorithms/","section":"post","summary":"I just finished reading Sebastian Ruder’s amazing article providing an overview of the most popular algorithms used for optimizing gradient descent. Here I’ll make very short notes on them primarily for purposes of recall.\nMomentum The update vector consists of another term which has the previous update vector (weighted by $\\gamma$). This helps it to move faster downhill — like a ball.\n$$ v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta}J(\\theta) $$","tags":["machine learning","optimization"],"title":"A Short Note on Stochastic Gradient Descent Algorithms","type":"post"},{"authors":null,"categories":[],"content":" There was a SemEval 2018 Shared Task on “irony detection in tweets” that ended recently. As a fun personal project, I thought of giving it a shot, just to implement some new ideas. In this post, I will describe my approach for the problem along with some code.\nProblem description The task itself was divided into two subtasks:\n Task A: Binary classification. Given a tweet, detect whether it has irony or not. Task B: Multi-label classification. Given a tweet and a set of labels: i) verbal irony realized through a polarity contrast, ii) verbal irony without such a polarity contrast (i.e., other verbal irony), iii) descriptions of situational irony, iv) non-irony, find the correct irony type.  While the task appears to be a simple text classification job, there are several nuances that make it challenging. Irony is often context-dependent or derived from world knowledge. In sentiment analysis, the semantics of the sentences are sufficient to judge whether the sentence has been spoken in a positive or negative manner. However, irony, by definition, almost always exists when the literal meaning of the sentence is dramatically different from what has been implied. Sample this:\n Just great when you’re (sic) mobile bill arrives by text.\n From a sentiment analysis perspective, the presence of the phrase “just great” would adjudge this sentence strongly positive. However, from our world knowledge, we know the nuances of the interplay between a “mobile bill” and “text.” As a human, then, we can judge that the sentence is spoken in irony.\nThe problem is: how can we have an automated system understand this?\nCircular correlation between text and hashtags The first idea of a solution came from how the dataset was generated in the first place. To mine tweets containing irony, those tweets were selected which contained the hashtag #not. The idea was that a lot of people explicitly declare their intent at irony through hashtags. For instance, consider the following tweet:\n Physical therapy at 8 am is just what I want to be doing with my Friday #iwanttosleep\n In this example, let us breakdown the tweet into 2 components:\n Text: Physical therapy at 8 am is just what I want to be doing with my Friday. Hashtag: I want to sleep  It is obvious from the semantics of the 2 components that they imply very different things. As such, it may help to model the interaction between the “text” and “hashtag” components of the tweet and then use the resulting embedding for classification. In this regard, we are essentially treating the problem as that of relation classification, where the entities are the 2 components and we need to identify whether there exists a relation between them (task A), and if yes, of which type (task B).\nThe problem, now, is reduced to the issue of how to model the two components and their interaction. This is where deep learning comes into the picture.\nModeling embeddings and interaction The embeddings to represent the components are obtained simply by passing their pretrained word vectors through a bidirectional LSTM layer. This is fairly simple for the text component.\nHowever, in the hashtag component, a single hashtag almost always consists of multiple words concatenated into a single string. Therefore, we first perform word segmentation on the hashtag and use the resulting segments to obtain the embedding.\nimport wordsegment as ws ws.load() hashtag = “ “.join(ws.segment(temp)) ## Here, 'temp' is the original hashtag  Once the embeddings for the two components have been obtained, we use the circular cross-correlation technique (which I have earlier described in this blog post to model their interaction. Essentially, the operator is defined as\n$$ [a\\cdot b]_k = \\sum_{i=1}^{d-1}a_i b_{(k+i)\\text{mod}d}. $$\nIn Tensorflow, this is implemented as follows:\nimport tensorflow as tf def holographic_merge(inp): [a, b] = inp a_fft = tf.fft(tf.complex(a, 0.0)) b_fft = tf.fft(tf.complex(b, 0.0)) ifft = tf.ifft(tf.conj(a_fft) * b_fft) return tf.cast(tf.real(ifft), 'float32')  The output of this merge is then passed to an XGBoost classifier (whose implementation was used out-of-the-box from the corresponding Python package).\nThis model resulted in a validation accuracy of ~62%, compared to ~59% for a simple LSTM model. Time to analyze where it was failing!\nWorld knowledge for irony detection The problem with this idea was that although it performed well for samples similar to the example given above, such samples constituted only about 20% of the dataset. For a majority of the tweets containing irony, there was no hashtag, and as such, modeling interactions was useless.\nIn such cases, we have to solely rely upon the text component to detect hashtag, for e.g.\n The fun part about 4 am drives in the winter, is no one has cleaned the snow yet\n If an automated system has to understand that the above sentence contains irony, it needs to know that there is nothing fun about driving on a road covered in snow. This knowledge cannot be gained from learning on a few thousand tweets. We now turn to transfer learning!\nMIT researchers recently built an unsupervised system called DeepMoji for emoji prediction in tweets. According to the website, \u0026ldquo;DeepMoji has learned to understand emotions and sarcasm based on millions of emojis. We hypothesize that if we use this pretrained model to extract features from the text component, it may then be used to predict whether the text contains irony. In a way, we are transfering world knowledge to our model (assuming that the million tweets on which DeepMoji was trained is our world!).\nAs expected, concatenating the DeepMoji features with the holographic embeddings resulted in a validation accuracy of $\\sim69\\%$, i.e., a jump of almost 7%. This reinforces our hypothesis that world knowledge is indeed an important ingredient in any kind of irony detection.\nSummary In essence, we identified 2 aspects that were essential to identify irony in tweets:\n Semantic interaction between text and hashtags, modeled using holographic embeddings World knowledge about irony in text, obtained through transfer learning from DeepMoji  The code for the project is available here.\nDisclaimer: In the final test phase, the results were disappointing (~50% for task A) especially given the high performance on validation set. This could likely have been due to some implementation error on the test set, and we are waiting for the gold labels to be released to analyze our mistake.\n","date":1517991006,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531309945,"objectID":"727603af7dba0bdadcefd01c68debe56","permalink":"https://desh2608.github.io/post/irony-detection-in-tweets/","publishdate":"2018-02-07T13:40:06+05:30","relpermalink":"/post/irony-detection-in-tweets/","section":"post","summary":"There was a SemEval 2018 Shared Task on “irony detection in tweets” that ended recently. As a fun personal project, I thought of giving it a shot, just to implement some new ideas. In this post, I will describe my approach for the problem along with some code.\nProblem description The task itself was divided into two subtasks:\n Task A: Binary classification. Given a tweet, detect whether it has irony or not.","tags":["natural language processing","representation learning"],"title":"Irony Detection in Tweets","type":"post"},{"authors":["Shakaiba Majheed","Aditya Gupta","**Desh Raj**","Frank Chung-Hoon Rhee"],"categories":null,"content":"","date":1516008755,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"6c170107210020e7aa633a544f800721","permalink":"https://desh2608.github.io/publication/infosc-17-art/","publishdate":"2018-01-15T15:02:35+05:30","relpermalink":"/publication/infosc-17-art/","section":"publication","summary":"","tags":["fuzzy","machine learning"],"title":"Uncertain Fuzzy Self-organization based Clustering: Interval Type-2 Approach to Adaptive Resonance Theory","type":"publication"},{"authors":null,"categories":[],"content":" In the first part of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used — Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.\nRademacher complexity Given a family of functions, one of the ways to measure its complexity is to see how well it can fit a random assignment of labels. A more complex hypothesis set would be able to fit a random noise better, and vice versa. For this purpose, we define $m$ random variables $\\sigma_i$, called Rademacher variables. We then define the empirical Rademacher complexity as\n$$ \\hat{\\mathcal{R}_S}(G) = \\mathbb{E}_{\\sigma}[\\text{sup}_{g\\in G}\\frac{1}{m}\\sigma_i g(z_i)] $$\nHere the summation term is essentially the inner product of the vector of noise (Rademacher variables) and the labels with some $g \\in G$. Intuitively, this term can be taken to represent the correlation between the actual assignment and the random assignment. On taking the supremum over all $g \\in G$, we are computing how well the function class $G$ correlates with random noise on $S$. The expectation of this term over all random noise distributions measures the average correlation.\nTherefore, a higher Rademacher complexity would imply that the function class $G$ is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class $G$ is, higher is the probability that it would have some $g$ which correlates well with random noise.\nHowever, this is just the empirical R.C. since we are computing the mean on the given sample set. The actual R.C. is obtained by taking the expectation of this value by sampling $S$ from a distribution $D$ consisting of sample sets of size $m$. Having thus defined the R.C., we can obtain an upper bound on the expected value of an error function $g$ taken from a family of functions $G$.\n$$ \\mathbb{E}[g(z)] \\leq \\frac{1}{m} \\sum_{i=1}^m g(z_i) + 2\\mathcal{R}_m(G) + \\sqrt{\\frac{\\log \\frac{1}{\\delta}}{2m}} $$\nNote that if we take the first term on RHS to LHS, the LHS becomes the maximum difference between the empirical and general loss (function value if function is binary-valued). We have access to the empirical values, but not the expectation. So we take 2 sample sets A and B which differ at only 1 point, so that we can use the McDiarmid’s inequality.\n The McDiarmid’s inequality bounds the probability that the actual mean and expected mean of a function differ by more than a fixed quantity, given that the function does not deviate by a large amount on perturbing a single element.\n The actual proof then becomes simply manipulating the expectation and supremum using Jensen’s inequality (function of an expectation is at most expectation of the function, if the function itself is convex). I do not go into the details of the proof here since it is readily available.\nTill now, we have only computed the bounds on the expectation of the set of loss functions $G$. We actually need to compute bounds on the general loss on the hypothesis class $H$, which assigns binary values to given samples. For this, we use the following lemma which is simple to prove.\n$$ \\hat{\\mathcal{R}_S} (G) = \\frac{1}{2}\\hat{\\mathcal{R}_{S_X}}(G) $$\nFrom this and the earlier result, we easily arrive at an upper bound on the generalization error of the hypothesis class in terms of its Rademacher complexity.\n$$ R(h) \\leq \\hat{R}(h) + \\mathcal{R}_m(H) + \\sqrt{\\frac{\\log \\frac{1}{\\delta}}{2m}} $$\nHere, computing the empirical loss is simple, but computing the R.C. for some hypothesis sets may be hard (since it is equivalent to an empirical risk minimization problem). Therefore, we need some complexity measures which are easier to compute.\nGrowth function The growth function of a hypothesis class $H$ for sample size $m$ denotes the number of distinct ways that $H$ can classify the sample. A more complex hypothesis class would be able to have a larger number of possible combinations for any sample size $m$. However, unlike R.C., this measure is purely combinatorial, and independent of the underlying distributions in $H$.\nThe Rademacher complexity and the growth function are related by Massart’s lemma as\n$$ \\mathcal{R}_m(G) \\leq \\sqrt{\\frac{2\\log \\prod_G (m) }{m}} $$\n The Massart’s lemma bounds the expected correlation of a given vector taken from a set with a vector of random noise, in terms of the size of the set, dimensionality of the set, and the maximum L2-norm of the set.\n As soon as we see “expected correlation,” we should think of the Rademacher complexity. To introduce the growth function, we use the term for the size of the set, since it essentially denotes the size of set containing all possible assignments for a sample.\nUsing this relation in the earlier obtained upper bound, we can bound the generalization error in terms of the growth function.\nAlthough it is a combinatorial quantity, the growth function still depends on the sample size $m$, and thus would require repeated calculations for all values $m\u0026gt;1$. Instead, we turn to the third and most popular complexity measure for hypothesis sets.\nVC-dimension The VC-dimension of a hypothesis class is the size of the largest set that can be fully shattered by it. By shattering, we mean that $H$ can classify the given set in all possible ways. Formally,\n$$ VCdim(H) = \\max{ m:\\prod_H (m) = 2^m } $$\nIt is important to understand 2 things:\n If $VCdim(H) = d$, then there exists a set of size $d$ that can be fully shattered. This does not mean that all sets of size $d$ or less are fully shattered by $H$. Also, in this case, no set of size greater than $d$ can ever be shattered by $H$.  To relate VC-dimension with the growth function, we use the Sauer’s lemma:\n$$ \\prod_H(m) \\leq \\sum_{i=0}^m {m\\choose i} $$\nHere, the LHS, which is the growth function, represents the number of possible behaviors that $H$ can have on a set of size $m$. The RHS is the number of small subsets that are completely shattered by $H$. For a detailed proof, I highly recommend this lecture (Actually, I would highly recommend the entire course).\nUsing some manipulations on the combinatorial, we arrive at\n$$ \\prod_H(m) \\leq \\left( \\frac{em}{d} \\right)^d = \\mathcal{O}(m^d) $$\nNow we can use this relation with the earlier results to bound the generalization error in terms of the VC-dimension of the hypothesis class.\n$$ R(h) \\leq \\hat{R}(h) + \\mathcal{O}\\left( \\sqrt{\\frac{\\log(m/d)}{m/d}} \\right) $$\nwhere $m$ is the sample size and $d$ is the VC-dimension.\nHere is a quick recap:\n Rademacher complexity — ability to fit random labels (using correlation) Growth function — number of distinct behaviors on $m$ VC-dimension — largest set size that can be fully shattered  This blog post is loosely based on notes made from Chapter 3 “Rademacher complexity and VC-Dimension” of Foundations of Machine Learning.\n","date":1516003785,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532679065,"objectID":"5f9710a0b53223887aae704ec2c5e2ce","permalink":"https://desh2608.github.io/post/intro-learning-theory-2/","publishdate":"2018-01-15T13:39:45+05:30","relpermalink":"/post/intro-learning-theory-2/","section":"post","summary":"In the first part of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used — Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.","tags":["learning theory"],"title":"Introduction to Learning Theory - Part 2","type":"post"},{"authors":null,"categories":[],"content":" One of the most significant take-aways from NIPS 2017 was the \u0026ldquo;alchemy\u0026rdquo; debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.\nOne of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set. For this, we require the hypothesis class $H$ to approximate the concept class $C$ which determines the labels for the distribution $D$. Since both $C$ and $D$ are unknown, we try to model $H$ based on the known sample set $S$ and its labels.\nGeneralization error: The generalization error of a hypothesis $h$ is the expectation of the error on a sample $x$ picked from the distribution $D$.\nEmpirical error: This is the mean of the error of hypothesis $h$ on the sample $S$ of size $m$.\nHaving defined the generalization error and empirical error thus, we can state the objective of learning as follows.\n The objective of learning is to have the empirical error approximate the generalization error with high probability.\n This kind of a learning framework is known as PAC-learning (Probably Approximately Correct). Formally, a concept class $C$ is PAC-learnable if there is some algorithm A for which the generalization error on a sample $S$ derived from the distribution $D$ is very low (less than $\\epsilon$) with high probability (greater than $1- \\delta$). In other words, we can say that for a PAC-learnable class, the accuracy is high with good confidence.\nGuarantees for finite hypothesis sets The PAC-learning framework provides strong guarantees for finite hypothesis sets (i.e., where the size of $H$ is finite). Again, this falls in two categories — the consistent case, and the inconsistent case. A hypothesis class is said to be consistent if it admits no error on the training sample, i.e., the training accuracy is 100%.\nConsistent hypothesis Let us consider a finite hypothesis set $H$. We want the generalization error to be less than some $\\epsilon$, so we will take a consistent hypothesis $h \\in H$, and bound the probability that its error is more than $\\epsilon$, i.e., we are calculating the probability that there exists some $h \\in H$, such that $h$ is consistent and its generalization error is more than $\\epsilon$. This is simply the union of all $h \\in H$ such that it follows the said constraints. By the union bound, this probability will be less than the sum of the individual probabilities i.e.,\n$$ \\sum_{h\\in H}Pr[\\hat{R}(h)=0 \\wedge R(h) \u0026gt; \\epsilon] $$\nFrom the definition of conditional probability, we can write\n$$ Pr(A \\cap B) = Pr(A|B)Pr(B) \\leq Pr(A|B) $$\nwhich bounds the required probability $P$ as\n$$ P \\leq \\sum_{h\\in H} Pr[\\hat{R}(h) =0| R(h) \u0026gt; \\epsilon] $$\nThe condition says that the expectation of error of $h$ on any sample is at least $\\epsilon$, so it would correctly classify a sample with probability at most $1-\\epsilon$. Hence, to correctly classify $m$ training samples with $|H|$ hypotheses, the total probability is given as\n$$ P \\leq |H|(1-\\epsilon)^m \\leq |H|\\exp(-m\\epsilon) $$\nOn setting the RHS of the inequality to $\\delta$, we obtain the generalization bound of the finite, consistent hypothesis class as\n$$ R(h_S) \\leq \\frac{1}{m}\\left( \\log |H| + \\log \\frac{1}{\\delta} \\right) $$\nAs expected, the generalization error decreases with a larger training set. However, to arrive at a consistent algorithm, we may have to increase the size of the hypothesis class, which results in an increase in generalization error.\nInconsistent hypothesis In practical scenarios, it is very restrictive to always require a consistent hypothesis class to bound the generalization error. In this section, we look at a more general case where empirical error is non-zero. For this derivation, we use the Hoeffding’s inequality which provides an upper bound on the probability that the mean of independent variables in an interval $[0,1]$ deviates from its expected value by more than a certain amount.\n$$ P(\\bar{X} - \\mathbb{E}\\bar{X} \\geq t) \\leq \\exp(-2nt^2) $$\nIf we take the errors as the random variable, their mean is the empirical error and the expectation is the generalization error. We can then get an upper bound for the generalization error of a single hypothesis $h$ as\n$$ R(h) \\leq \\hat{R}(h) + \\sqrt{\\frac{\\log \\frac{2}{\\delta}}{2m}} $$\nHowever, this is still not the general case since the hypothesis $h$ returned by the learning algorithm is not fixed. Similar to the consistent case, we will try to obtain an upper bound on the generalization error for an inconsistent (but finite) hypothesis, i.e., we need to compute the probability that there exists some hypothesis $h \\in H$ such that the generalization error of $h$ differs from its empirical error by a value greater than $\\epsilon$. Again, using the union bound, we get\n$$ P \\leq \\sum_{h \\in H}Pr[|\\hat{R}(h)-R(h)|\u0026gt;\\epsilon] $$\nUsing the Hoeffdieng’s inequality, this becomes\n$$ P \\leq 2|H|\\exp(-2m\\epsilon^2) $$\nNow equating the RHS with $\\delta$, we can arrive at the result\n$$ R(h) \\leq \\hat{R}(h) + \\sqrt{\\frac{\\log |H| + \\log \\frac{2}{\\delta}}{2m}} $$\nHere it is interesting to note that for a fixed $|H|$, to attain the same guarantee as in the consistent case, a quadratically larger labeled sample is required. Let us now analyze the role of the size of hypothesis class. If we have a smaller $H$, the second term is reduced but the empirical error may increase, and vice versa. However, for the same empirical error, it is always better to go with the smaller hypothesis class, i.e., the famous Occam’s Razor principle.\nIn this article, we looked at some generalization bounds in case of a finite hypothesis, using the PAC learning framework. In the next part, I will discuss some measures for infinite hypotheses, namely the Rademacher complexity, growth function, and the VC dimension.\nThis blog post is loosely based on notes made from Chapter 2 \u0026ldquo;The PAC Learning Framework\u0026rdquo; of Foundations of Machine Learning.\n","date":1516003783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531145612,"objectID":"ed0afd503e4d225a62a60e2261c28b46","permalink":"https://desh2608.github.io/post/intro-learning-theory-1/","publishdate":"2018-01-15T13:39:43+05:30","relpermalink":"/post/intro-learning-theory-1/","section":"post","summary":"One of the most significant take-aways from NIPS 2017 was the \u0026ldquo;alchemy\u0026rdquo; debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.\nOne of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set.","tags":["learning theory"],"title":"Introduction to Learning Theory - Part 1","type":"post"},{"authors":null,"categories":[],"content":" Translation is one of those tasks in language where the arrival of deep learning systems, and in particular sequence-to-sequence, has been something like a boon. In less than 4 years since the first paper on Neural Machine Translation, software giants such as Google and Microsoft have already announced that their translation systems have almost completely shifted from statistical to neural. Gone are the days when researchers mulled over complex word and phrase alignment techniques, and yet fell short on several language combinations. With the latest framework, all you need are a million parallel sentences, and your system can then translate between this pair sufficiently well.\nA million parallel sentences — that’s a little constraining, though! It is often difficult and sometimes even impossible to obtain a bilingual parallel corpus for many pairs of languages. In such cases, using a pivot language for triangulation has been found to be helpful. However, even in such supervised systems, the performance is still constrained by the size of the training corpus.\nMonolingual data, on the other hand, is available in abundance, and a number of semi-supervised systems do use these, but mostly for the language modeling part of translation. For example, a naive system may perform word-by-word substitution and use a language model trained on the target language to obtain the most probable word order.\nRecently, there have been 2 very similar papers (both currently under review at ICLR ’18) which propose to perform completely unsupervised machine translation. In this article, I will discuss both of these papers. A similar blog is available here, but I didn’t know of its existence until I was already halfway through this post.\nUnsupervised Neural Machine Translation This paper1 is from Prof. Kyunghyu Cho (NYU), and the authors have used the traditional seq2seq model with a twist. The encoder is shared across all languages, but each language has its own decoder. The intuition is that a shared encoder will transform a sentence to a shared space representation, from where the language-specific decoder will be able to decode it to its own language.\nBoth the encoder and decoder are 2 layer bidirectional RNNs with GRU units. Furthermore, the embeddings used in the feature layer are fixed, and are obtained from pre-trained cross-lingual dictionary. This ensures that the shared space representation obtained using the encoder is language-independent.\nThe paper uses 2 interesting techniques for the unsupervised training.\nDenoising: The autoencoder (or seq2seq) is used to reconstruct a sentence in a language, since we only have a monolingual corpus on which to train the system. Due to such a setting, an optimal system would essentially learn to copy the input to the output, and the system would reduce to a word-by-word substitution system. To prevent this, “denoising” is used, which introduces random noise in the input sentence so that copying cannot give the best output. This is dones by making $\\frac{N}{2}$ random swaps for any sequence of $N$ tokens. There are 2 advantages to this technique:\n Since copying is out of the picture, the system needs to learn the internal structure of language to perform well. By swapping words randomly, we also account for word order divergence across languages. For instance, Los Angeles International Airport in English becomes Aéroport international de Los Angeles in French.  Backtranslation: Even with denoising added, the system is still monolingual. To integrate some element of cross-lingual training, the authors use the method of backtranslation. Given a sentence $x$ in language L1, the shared encoder is used to get the latent representation, and the decoder for the other language L2 is used to obtain a noisy translation $y$. This translation $y$ is then used to predict the original sentence $x$ using the encoder and decoder for L1. This technique creates a pseudo-parallel corpus so that the system can learn cross-lingual translation.\nDenoising forces the system to capture broad word-level equivalences, while backtranslation helps it to learn more subtle relations between the language pairs. Furthermore, using pretrained cross-lingual embeddings ensures that the shared latent space representations for sentences in both the languages are near each other when the sentences have the same sense (or meaning).\nUnsupervised Machine Translation using Monolingual Corpora Only A very similar paper2 from researchers at Facebook employs almost the same techniques, but differs slightly in the encoding mechanism. I personally enjoyed reading this paper more than the first one, although they haven’t gone into details of the components they use in their model. The explanation of the loss function for end-to-end training is very lucid, and the overall structuring itself is appealing to a novice researcher like myself.\nAnyway, the model used in this paper consists of a single encoder and a single decoder (bidirectional LSTM with attention in the decoder, similar to the NMT model used in Google Translate) which is shared by both the languages. For the unsupervised training, 3 techniques are employed.\n Denoising: Similar to the above paper, the autoencoder is denoised so that it does not learn a word-by-word substitution. The noise model in this case consists of: (i) dropping every word with some random probability, and (ii) shuffling the sentence by applying a random permutation. Cross-domain training: This is the same as the “backtranslation” technique used in the above paper. However, the authors have explicitly mentioned that to obtain the translation $x$ from the sentence $y$, the model of the previous iteration is used. This requires that the model be initialized with a naive translation strategy, which in this case, is simple word-by-word substitution. Adversarial training: In the above paper, due to the use of cross-lingual fixed embeddings in the shared encoder, the latent space representations were arguably similar for similar sentences in different languages. This method does not use cross-lingual embeddings, and hence, the representations will be similar only “as long as the two monolingual corpora exhibit strong structure in feature space.” (Full disclosure: This statement is written as a hand-waving argument without a justification, and one of the reviewers has even pointed this out.) In order to overcome this constraint, the authors employ a discriminator whose task is to predict the language of the encoded sentence. In turn, the encoder has an added term in its loss function which ensures that the representation of similar sentences in different languages are nearby in the latent space.  Since the training is done iteratively and BLEU scores are computed at every step, we can simply select the hyperparameters corresponding to the best performing iteration. Empirically, the authors found that this selection has good correlation with test-time performance of the system. Furthermore, this unsupervised model was found to perform as good as a comparable supervised model trained on 100,000 parallel sentences, which is definitely an encouraging achievement for further research in unsupervised NMT.\n Artetxe, Mikel, et al. “Unsupervised Neural Machine Translation.” arXiv preprint arXiv:1710.11041 (2017). ^ Lample, Guillaume, Ludovic Denoyer, and Marc’Aurelio Ranzato. “Unsupervised Machine Translation Using Monolingual Corpora Only.” arXiv preprint arXiv:1711.00043(2017). ^   ","date":1513238970,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531145612,"objectID":"4184a535b682db80827c427189ec1d5f","permalink":"https://desh2608.github.io/post/unsupervised-approaches-for-nmt/","publishdate":"2017-12-14T13:39:30+05:30","relpermalink":"/post/unsupervised-approaches-for-nmt/","section":"post","summary":"Translation is one of those tasks in language where the arrival of deep learning systems, and in particular sequence-to-sequence, has been something like a boon. In less than 4 years since the first paper on Neural Machine Translation, software giants such as Google and Microsoft have already announced that their translation systems have almost completely shifted from statistical to neural. Gone are the days when researchers mulled over complex word and phrase alignment techniques, and yet fell short on several language combinations.","tags":["deep learning","natural language processing","machine translation"],"title":"Unsupervised Approaches for NMT","type":"post"},{"authors":null,"categories":[],"content":" Representation learning, as the name suggests, seeks to learn representations for structures such as images, videos, words, sentencences, graphs, etc., which may then be used for several objectives. Arguably the most important representations used nowadays are word embeddings, usually learnt using the distributional semantics methods such as skip-gram or GloVe. I have previously written about these methods here.\nTwo assumptions are inherent while using these methods to learn word vectors:\n That words are best visualized as points in the $n$-dimensional space. That the Euclidean distance or the Euclidean dot product are the best measures of similarity between words (or other structures for which the embeddings have been learnt).  Over the last couple years, researchers have sought to challenge both of these assumptions by proposing several new non-Euclidean representations for words and graphs. Especially in the case of learning relational embeddings, the model should be able to learn all combinations of properties, namely reflexivity/irreflexivity, symmetry/anti-symmetry, and transitivity. Euclidean dot products are limited in that they cannot handle anti-symmetry, since dot products are commutative.\nIn this post, I will discuss 4 non-Euclidean embeddings: Gaussian, Holographic, Complex, and Poincare.\nWord representations via Gaussian embeddings The key idea in this ICLR ’15 paper1 is to map words to a density instead of a point. Density here is represented by a “potential function,” such as a Gaussian. The authors provide a nice recap of energy functions as a tool for learning word representations.\nEssentially, any representation learning involves an energy function $E(x,y)$ which scores pairs of inputs and outputs. A loss function is then uses this energy function to quantify the difference between actual output and predicted output. In the case of skip-gram models, the energy function used is a dot product, and the loss function is a logistic regression. In this paper, the authors propose 2 kinds of energy functions (for symmetric and asymmetric similarity), and the loss function used is max margin as follows.\n$$ L_m(w,c_p,c_n) = \\max(0,m-E(w,c_p)+E(w,c_n)) $$\nFor a Gaussian distribution to model any word, a baseline approach may involve using the distribution around the word to compute and mean and variance. If a word $w$ occurs $N$ times in the corpus, the covariance of the distribution around $w$ is given as\n$$ \\sum_w = \\frac{1}{NW}\\sum_i^N \\sum_j^W (c(w)_{ij})(c(w)_{ij}-w)^T $$\nwhere W is the window size, and $w$ is the assumed mean. However, the distributions learned using this empirical approach do not possess some desired properties such as unsupervised entailment represented as inclusion between ellipsoids. To solve this, 2 energy functions are proposed.\nMethod 1: Symmetric similarity\nThis method just computes the inner product between the two distributions. It has been shown that the inner product of two normal distributions is again a normal distribution. Furthermore, we take the log of this value for two reasons. First, since we are dealing with ranking loss, taking the logarithm converts absolute values into relative values, which is easier to interpret. Second, it is numerically easier to deal with.\nFurthermore, the energy function is shown to be of the form log det A + const. We can interpret the constant term as a regularizer that prevents us from decreasing the distance by only increasing joint variance. This combination pushes the means together while encouraging them to have more concentrated, sharply peaked distributions in order to have high energy.\nMethod 2: Asymmetric similarity\nThis method computes the energy function as the negative of the KL-divergence between the 2 distributions (negative because the KL-divergence returns a distance value and hence needs to be minimized to increase similarity). A low KL divergence from $x$ to $y$ indicates that we can encode $y$ easily as $x$, implying that $y$ entails (logically follows from) $x$.\nThe authors have further computed the gradients for each of the two energy functions, and they are easily expressible in terms of existing means and covariances.\nPoincare embeddings for hierarchical representations This paper2 proposes embeddings in hyperbolic spaces, such as the Poincare sphere. Before we get into the method itself, I think it would be best to give a brief overview of hyperbolic geometry itself.\nHyperbolic geometry\nIn his book Elements, Euclid provided a rigourous framework for axioms, theorems and postulates for all geometrical knowledge at the time. He stated 5 axioms which were to be assumed true. The first 4 were quite self-evident, and were:\n Any two points can be connected by a line. Any line segment can be extended indefinitely. Given a line segment, a circle can be drawn with center at one of the endpoints and radius equal to the length of the segment. Any two right angles are congruent.  However, the fifth axiom, also known as Playfair’s axiom, is much less obvious.\nPlayfair’s axiom: Given a line L and a point P, there exists at most one line through P that is parallel to L.\nEuclid himself wasn’t very fond of this axiom and his first 28 postulates depended only on the first 4 axioms, which are the “core” of Euclidean geometry. Even 2000 years after his death, mathematicians tried to derive the fifth axiom from the first 4. While using “proof by contradiction” for this purpose, they assumed the negation of the fifth axiom (Given a line L and a point P not on L, there are at least two distinct lines that can be drawn through P that are parallel to L) and tried to arrive at a contradiction. However, while the derived results were strange and very different from those in Euclidean geometry, they were consistent within themselves. This was a turning point in mathematics as such a bifurcation in geometry had never been expected before. The geometry that arose from these explorations is known as hyperbolic geometry.\nWith this knowledge, let us now look at how embeddings may be computed in this new model.\nThe Poincare sphere model of hyperbolic space is particularly suitable for representing hierarchies. Consider a knowledge base which can be visualized as a tree. For any branching factor b, the number of leaf nodes increases exponentially as the number of levels increases. If we try to replicate this construction in a Euclidean disk(sphere), it would not be possible since the area(volume) of a disk(sphere) increases only quadratically(cubically) with increase in radius. This requires that we increase the number of dimensions exponentially.\nHowever, the Poincare sphere embeds such hierarchies easily: nodes that are exactly $l$ levels below the root are placed on a sphere in hyperbolic space with radius $r \\propto l$ and nodes that are less than $l$ levels below the root are located within this sphere. This type of construction is possible as hyperbolic disc area and circle length grow exponentially with their radius. In the paper, the authors used a sphere instead of disk since more degrees of freedom implies better representation of latent hierarchies.\nDistances in the hyperbolic space are given as\n$$ d(u,v) = arcosh\\left( 1 + 2\\frac{\\lVert u-v \\rVert^2}{(1-\\lVert u \\rVert)^2(1-\\lVert v \\rVert)^2} \\right) $$\nHere, hierarchy is represented using the norm of the embedding, while similarity is mirrored in the norm of vector difference. Furthermore, the function is differentiable, which is good for gradient descent.\nFor optimization, the update term is the learning rate times the Riemannian gradient of the parameter. The Riemannian gradient itself is computed by taking the product of the Poincare ball matrix inverse (which is trivial to compute) with the Euclidean gradient (which depends on the gradients of the distance function). The loss function used in the paper is a softmax with negative sampling.\nHolographic embeddings for knowledge graphs This and the next method seek to learn embeddings for relations within knowledge graphs, and the motivation for both is to have embeddings that allow asymmetric relations to be sufficiently represented. To achieve said objective, this AAAI ’16 paper3 employs circular correlations, while the next paper from ICML ’164 uses complex embeddings.\nBefore describing the method, I will first describe the task. Given a set $E$ of entities and a set $P$ of relation types, the objective is to learn a characteristic function for each relation type that determines whether that relation exists between any two elements in $E$. The entities are referred to as the subject and the object.\nThe general approach is to approximate the characteristic function using a function that takes as input the relation vector, and the vectors corresponding to the subject and the object. Using a loss function such as log likelihood minimization with negative sampling, we can tune the parameters that describe the entity vectors and the relation type vector. This is similar to our earlier discussion on energy function optimization.\nThe catch here is that the characteristic function is supposed to output a scalar score (the probability of the relation), but the inputs to it are vectors. To convert the input to a scalar, the entity vectors are combined using a composition operator o(more on this later), and its dot product is taken with the relation type vector.\nSo the problem boils down to the choice of a good compositional operator. In the past, three different approaches have been taken for this problem.\n Tensor product: Take the outer product of the entity vectors. However, the resulting vector contains the square of the initial number of parameters, which may cause problems such as overfitting down the line. Concatenation, projection, and non-linearity: The projection matrix is learned during training. However, due to the absence of interaction between features, the representation learnt is not rich enough, even though non-linearity is added. Non-compositional methods: In these approaches, the score is computed as the distance of the difference vector with the relation vector (e.g., TransE).  Essentially, we want an operator which has cross-feature interactions without having the number of parameters explode. To this end, the authors propose the circular correlation operator, which is given as\n$$ [a\\cdot b]_k = \\sum_{i=1}^{d-1}a_i b_{(k+i)\\text{mod}d}. $$\nThe output contains as many parameters as the input vectors, while also capturing the interaction between the features. The function measures the covariance between embeddings at different dimension shifts, and the asymmetry stems from this circular correlation.\nAt this point, you may be wondering why a simple convolutional operator would not suffice. The answer is that convolution is a commutative function, while correlation is not. Again, the key lies in symmetry (or the lack of it)!\nComplex embeddings for link prediction In the objective of predicting relations described earlier, we can think of the characteristic function as a function which takes as input a latent matrix X of scores and outputs the corresponding probability. This latent matrix is an $E \\times E$ matrix since it contains the scores for every possible pair of entities. However, since the number of entitites may be very large, the problem we want to solve is that of matrix factorization.\nThis is similar to the singular value decomposition method for learning word vectors that I discussed in an earlier blog post. If we assume that an entity has only one unique representation, regardless of whether it occurs as subject or object, the matrix X can be factorized as\n$$ X = EWE^{-1} $$\nSince the entity vectors are complex in nature ($u$ = Re($u$) + $i$Im($u$)), the matrix factorization of $X$ may be either real or complex. But since the characteristic function returns a real output, we define $X$ as the Real part of the factorization. Now, our original objective is to learn $P(Y=1)$ for every $s-o$ pair, and we are trying to approximate this using the latent matrix $X$. In the case of binary relations (yes/no), $Y$ is essentially a sign matrix, and hence it is safe to assume that its “sign-rank” is low.\nBut what is a “sign-rank”? It refers to the smallest rank of a real matrix having the same sign pattern as $Y$. The authors showed in an earlier paper that if the sign rank of $Y$ is low, the rank of Re($EWE^T$) is at most twice that of $Y$. While this is a good upper bound, the actual rank is often much lower than the rank of $Y$.\nIn the case of multi-relational data, each relation has a representation $w$ associated with it. The characteristic function then takes as input the relation type along with the subject and object, and computes the score based on a novel scoring function. This function has the following property: if $w$ is real, the characteristic function is symmetric, and if $w$ is imaginary, then it is anti-symmetric.\nWhile Euclidean embeddings are popular, they are in no way sufficient to represent all the complexities and hierarchies in language. These methods suggest that looking at non-Euclidean spaces for representation learning may be the way to go.\n Vilnis, Luke, and Andrew McCallum. “Word representations via gaussian embedding.” arXiv preprint arXiv:1412.6623(2014). ^ Nickel, Maximilian, and Douwe Kiela. “Poincare Embeddings for Learning Hierarchical Representations.” arXiv preprint arXiv:1705.08039 (2017). ^ Nickel, Maximilian, Lorenzo Rosasco, and Tomaso A. Poggio. “Holographic Embeddings of Knowledge Graphs.” AAAI. 2016. ^ Trouillon, Théo, et al. “Complex embeddings for simple link prediction.” International Conference on Machine Learning. 2016. ^   ","date":1512547755,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531145612,"objectID":"1b7f8fd8fab7a2cffd989d14f8d867a1","permalink":"https://desh2608.github.io/post/beyond-euclidean-embeddings/","publishdate":"2017-12-06T13:39:15+05:30","relpermalink":"/post/beyond-euclidean-embeddings/","section":"post","summary":"Representation learning, as the name suggests, seeks to learn representations for structures such as images, videos, words, sentencences, graphs, etc., which may then be used for several objectives. Arguably the most important representations used nowadays are word embeddings, usually learnt using the distributional semantics methods such as skip-gram or GloVe. I have previously written about these methods here.\nTwo assumptions are inherent while using these methods to learn word vectors:","tags":["representation learning"],"title":"Beyond Euclidean Embeddings","type":"post"},{"authors":null,"categories":[],"content":" When I was browsing through research groups for my grad school applications, I came across some interesting applications of new deep learning methods in a multimodal setting. ‘Multimodal,’ as the name suggests, refers to any system involving two or more modes of input or output. For example, an image captioning system provides images as input and expects a textual output. Similarly, speech-to-text, descriptive art, video summarization, etc., are all examples of multimodal objectives. In this article, I will discuss 3 recent papers from Mohit Bansal (who joined UNC last year), based on album summarization, video captioning, and image captioning (with a twist).\nCreating a story from an album Given an album containing several images (which may or may not be similar), the task of Visual Storytelling is to generate a natural language story describing the album. In this EMNLP ’17 paper1, the task is decomposed into 3 steps:\n Album encoder: Encode the individual photos in the album to form photo vectors Photo selector: Select a small number of representative photos. Story generator: Compose a coherent story from the selected photo vectors.  For each of these three components, the paper uses a hierarchically-attentive RNN. The first component is similar to an embedding layer in a text classification setting, wherein a lookup table assigns some pretrained vectors to each word and then an RNN is applied to add sentence-level information to each word vector. In a similar fashion in this paper, the initial embeddings for each image are obtained using a pretrained ResNet101 layer, and then a bidirectional RNN with GRU cells is used to add information pertaining to the entire album in every image embedding.\nIn the Photo Selector stage, the selection is treated as a latent variable since we only have end-to-end ground truth labels. As such, we use soft attention to output $t$ probability distributions over all the images in the album, where $t$ is the number of summary images required, i.e., each image has $t$ probabilities associated with it. For this purpose, a GRU takes the previous $p$ and the previous hidden state h as input and outputs the next hidden state. We use a multilayer perceptron with sigmoid activation to fuse the hidden state with the photo vector and obtain the soft attention for the particular image.\n$$ h_t = GRU_{select}(p_{t-1},h_{t-1}) \\\\\\ p(y_{a_i}(t)=1) = \\sigma(MLP([h_t,v_i])) $$\nFinally, we can obtain $t$ weighted album representations by taking the weighted sum of the photo vectors with the corresponding probability distributions. Each of these vectors is then used to decode a single sentence. For this purpose, a GRU takes the joint input of the album vector at step $t$, the previous word embedding, and the previous hidden state, and outputs the next hidden state. We repeat this for $t$ steps, thus obtaining the required album summary.\nHow do we define loss in such a setting? First, since we already know the correct summary sentences, we can define a generation loss which is simply the sum of negative log likelihoods of the correct words. However, in addition to the words being similar, the story should be temporally coherent, i.e., the sentences themselves should be in a specific order. For this purpose, we apply a max-margin ranking loss as:\n$$ h_t = GRU_{select}(p_{t-1},h_{t-1}) \\\\\\ p(y_{a_i}(t)=1) = \\sigma(MLP([h_t,v_i])) $$\nThe total loss is just a linear combination of these two losses. This provides a framework for end-to-end training for the system.\nCaptioning videos using multi-task learning It seems multitask learning was under the spotlight in ACL ’17. Two semantic parsing papers I discussed in yesterday’s blog were both based on this paradigm, and so is this one.\nAt this point, I would like to clarify the difference between transfer learning and multitask learning by quoting directly from this answer on ResearchGate:\n Multi-task learning can be seen as one type of transfer learning, where the information to transfer is some inner representation/substructure of the models under consideration, or the relevant features for a prediction, and where all the target tasks use the same data samples, but predict different target features for these (e.g. Part Of Speech tagging and Named Entity Recognition for natural language processing tasks).\nTransfer Learning, on the other hand, would be the very general problem setting, where the “what” to transfer (representation, model substructures, data samples, parameter priors, …), the concurrency of learning (one or multiple target tasks using one or multiple source tasks, or learning several tasks jointly), the differences in domain (same data or different samples, samples from same or different/related distribution, same or partially different input features) and prediction problem (same target feature or different target features/tasks, same conditional or different/related conditional) are characteristics identifying the subclass of transfer learning problem, and maybe the approach taken to address this problem.\n A more formal definition can be found here. Essentially in multitask learning, all the tasks are learnt simultaneously, whereas in transfer learning, the knowledge from one task is used in another. Now that the terminology is clear, let us look at the tasks and the model used.\nThe objective in this paper is video captioning, and the co-learnt tasks are video prediction and language entailment generation. It is arguably difficult to obtain large amounts of annotated data for a video prediction task, and hence learning from other tasks is especially relevant in this context.\nVideo prediction refers to the task of predicting the next frame in a video given a sequence of frames. Recognizing textual entailment (RTE), means identifying the logical relationship between two sentences, i.e., whether a premise and hypothesis follow entailment, contradiction, or independence. Knowledge transfer from a video prediction setting helps the model learn the temporal flow of information in a video, while learning from an RTE setting helps it in logically infering a caption from the video. This is the rationale behind using these tasks for the multi-task learning framework.\nThe overall architecture of the system is given below.\nFor each subsystem, the paper uses a simple attention-based bidirectional LSTM for the encoding and decoding purposes. This is a fine example of how a simple sequence-to-sequence block can be leveraged in different settings to perform interesting tasks.\nPuns in image captions Humor is difficult to capture or create in general. Heterographic homophones (words with different spelling but similar sound) are often used by cartoonists to add subtext to illustrations.\nIn this paper2, the authors have proposed 2 different methods to generate “punny” captions for images, namely a Generation model, and a Retrieval model.\nThe Generation model works as follows:\n The first step is tagging. We identify the top 5 objects in the given image using an Inception-ResNet-v2 model trained on ImageNet. We also get the words from a simple caption generated for the image using a Show-and-Tell architecture. The objects and the words together are considered as tags for pun generation. We then generate a vocabulary of puns by mining the web and selecting all pairs of words with an edit distance of 0 based on articulatory features. From this pun vocabulary, we filter those puns where at least one of the homophones is related to the image in question. During the caption generation, at specific time steps, the model is forced to produce a phonological counterpart of a pun word associated with the image. The decoder generates next words based on all previously generated words. To solve the issue of non-grammatical sentences due to puns later in the sentence, two models are trained to decode the image in both forward and reverse directions.  The Retrieval model, on the other hand, tries to find relevant captions from a prebuilt corpus of captions. This is an entirely deterministic model which requires two conditions to be satisfied:\n The caption must contain the counterpart of the pun word present in the image so that incongruity is attained. The caption must be contextually relevant to the image, i.e., it must contain at least one of the \u0026ldquo;tagged\u0026rdquo; words that we found earlier.  Finally, the captions obtained from both models are pooled together and ranked by taking their log-probability score with respect to the original caption generated from the simple image captioning model. Non-maximal suppression is applied to remove captions which are similar to a higher-ranked caption, and the top 3 such obtained are retained.\nFrom these examples of multimodal systems, we see that simple sequence-to-sequence models work satsifactorily if used in conjuction with intelligent frameworks such as multitask learning or transfer learning, as is the trend in recent days. A cool thing is that reading about the various transfer learning approaches for this and the previous post has helped me come up with a new solution for a project that I have been working on. More on that later!\nConference on Empirical Methods in Natural Language Processing*. 2017.\n Yu, Licheng, Mohit Bansal, and Tamara Berg. “Hierarchically-Attentive RNN for Album Summarization and Storytelling.” *Proceedings of the 2017 ^ Chandrasekaran, Arjun, Devi Parikh, and Mohit Bansal. “Punny Captions: Witty Wordplay in Image Descriptions.” arXiv preprint arXiv:1704.08224 (2017). ^   ","date":1510214938,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531124725,"objectID":"89d2cbb344eedfb1d7d3044237e6e01d","permalink":"https://desh2608.github.io/post/deep-learning-multimodal-systems/","publishdate":"2017-11-09T13:38:58+05:30","relpermalink":"/post/deep-learning-multimodal-systems/","section":"post","summary":"When I was browsing through research groups for my grad school applications, I came across some interesting applications of new deep learning methods in a multimodal setting. ‘Multimodal,’ as the name suggests, refers to any system involving two or more modes of input or output. For example, an image captioning system provides images as input and expects a textual output. Similarly, speech-to-text, descriptive art, video summarization, etc., are all examples of multimodal objectives.","tags":["deep learning","multimodal"],"title":"Deep Learning for Multimodal Systems","type":"post"},{"authors":null,"categories":[],"content":" In Part 1 of this two-part series, I discussed some supervised approaches for the objective. In this part, we will look at some unsupervised or semi-supervised approaches, namely a Bayesian model, and transfer learning.\nAn unsupervised Bayesian model This paper was published in ACL 20111, back when statistical methods were still being used for NLP tasks. But with the recent forays into generative models, I feel it has again become relevant to understand how such methods worked. The task of frame semantic parsing can be broken down into 3 independent steps:\n Decompose the sentence into lexical items. Divide these items into clusters and assign a label to each cluster. Predict argument-predicate relations between clusters.  Frames essentially refer to a semantic representation of predicates (such as verbs), and their arguments are represented as clusters. For sake of convenience, we refer both of these structures as semantic classes. For example, in the sentences:\n [India] defeated [England]. [The Indian team] secured a victory over the [English cricket team].  Here, ‘defeated’ and ‘secured a victory’ both belong to the frame WINNING, while ‘India’ and ‘Indian team’ are grouped into the cluster labeled WINNER.\nThe authors proposed a generative algorithm which makes use of statistical processes to model semantic parsing. We can summarize the model as follows, for a particular sentence:\n The distribution of semantic classes is given by a hierarchical Pitman-Yor process, i.e.,  $$ \\theta_{root} = PY(\\alpha_{root},\\beta_{root},\\gamma). $$\n We start with obtaining the semantic class for the root of the tree from the probability distribution which is a sample drawn from the above Pitman-Yor process. Once the root is obtained, we call the function GenSemClass on this root. Since the current root only has a semantic class, we obtain its syntactic realization from a distribution over all possible syntactic realizations, which is given as a Dirichlet Process with the arguments as the base word and a prior.  $$ \\phi_c = DP(w^{\u0026copy;},H^{\u0026copy;}) $$\n Essentially, the base word $w$ is obtained from a geometric distribution, and the subsequent words are obtained by computing the conditional probability of dependency relation $r$ given $w$, and the next word $p$ given $r$. For each argument type $t$, if the probability of having at least 1 argument of type $t$ is non-zero, we generate an argument of that type using function GenArgument, until that probability becomes 0. The GenArgument function again computes the base argument from the distribution of syntactic realizations, and then obtains the next semantic class again from the hierarchical PY process. We then recursively call the GenSemClass function on this new class.  This is the essence of the algorithm. Basically we get a semantic frame from the PY process, and then generate the corresponding syntax from a Dirichlet process. This is done recursively, hence the need for a hierarchical PY process. For the details of the stochastic processes, you can look at their Wikipedia pages. For the root level parameters, a stick-breaking construction is used, but I am yet to look into the details of this method. However, I suppose this is similar to the broken-stick technique used to estimate the number of eigenvalues to retain in a principal component analysis.\nTransfer learning There were two recent papers in ACL 20172,3 which used some kind of multi-task or transfer learning approach in a neural framework for semantic parsing.\nThe first of these papers from Markus Dreyer at Amazon uses the popular sequence-to-sequence model developed for machine translation at Google. The sentence is first encoded into an intermediate vector representation using and encoder, and then decoded into an embedding representation for the parse tree. Popular encoders and decoders are stacked bidirectional LSTM layers, usually with some attention mechanism.\nOnce the parse tree embedding has been obtained, the task remains to generate the actual parse tree. For this, the authors have described a COPY-WRITE mechanism. While reading the output embedding at each step, the model has 2 options:\n COPY: This copies 1 symbol from the input to the output. WRITE: This selects one symbol from the vocabulary of all possible outputs.  A final softmax layer generates a probability distribution over both of these choices, such that the probability of choosing WRITE at any step is proportional to an exponential over the output vector at that step, and that for choosing COPY is proportional to an exponential over a non-linear function of the intermediate representation and the output vector (i.e., the encoded and decoded vectors). The authors further describe 3 ways to extend this method in a multi-task setting:\n One-to-many: In this, the encoder is shared but each task has its own decoder and attention parameters. One-to-one: The entire sequence is shared, with an added token at the beginning to identify the task. One-to-shareMany: This also has a shared encoder and decoder, but the final layer is independent for each task. In this way, a large number of parameters can be shared among tasks while still keeping them sufficiently distinct. Empirically, this model was found to perform best among the three.  The second paper is from Noah Smith’s group at Washington. As with the previous paper, I will first describe the basic model and then explain how it is extended in a multi-task setting.\nGiven a sentence $x$, and a set of all possible semantic graphs for that sentence $Y(x)$, we want to compute\n$$ \\hat{y} = \\text{arg}\\min_{y \\in Y(x)} S(x,y),~~~~ \\text{where } S(x,y) = \\sum_{p\\in y}s(p),$$\ni.e., the scoring function $S$ is a sum of local scores, each of which is itself a parametrized function of some local feature. In this paper, these features are taken to be the following 3 constructs (first order logic):\n Predicate Unlabeled arc Labeled arc  The model is given in the following diagram taken from the paper.\nFor the 2 input words, we first obtain vectors using a bi-LSTM layer, and these are then fed into multilayer perceptrons (MLPs) corresponding to each of the three local feature constructs mentioned above. Each first-order structure is itself associated with a vector (shown in red). The scoring function $s(p)$ is simply the dot product of the MLPs output and the first-order vector.\nThe cost function is a max-margin objective with a regularization parameter and a sum over individual losses given as\n$$ L(x_i,y_i,\\theta) = \\max_{y\\in Y(x_i)} S(x_i,y) + c(y,y_i) - S(x_i,y_i). $$\nHere, $y_i$ is the gold label output and $y$ is the obtained output, while $c$ is the weighted Hamming distance between the two outputs.\nOnce this basic architecture is in place, the authors describe 2 method to extend it with transfer learning. The tasks here are 3 different formalisms in semantic dependency parsing (Delph-in MRS, Predicate-Argument Structure, and Prague Semantic Dependencies), so that each of these require a different variation of the output form. In the first method, the representation is shared among all tasks but the scoring is done separately. This further has variants wherein we can either have a single common bi-LSTM for all tasks, or a concatenation of independent and common layers.\nThe second method describes a joint technique to perform representation and inference learning across all the tasks simultaneously. The description is mathematically involved but intuitively simple, since we are just expressing the inner product in the scoring function in a higher dimension. You can look at the original paper for details and notation.\nWith this, we come to the end of this series on semantic parsing. Since a lot of models are common between different objectives, these methods are highly relevant across any NLP task, especially with a shift from supervised to unsupervised techniques. While writing this article, I have been thinking of ways of adapting the generative model from the Bayesian paper to a neural architecture, and I might read up more about this in the coming weeks. Till then, keep “learning”!\n Titov, Ivan, and Alexandre Klementiev. “A Bayesian model for unsupervised semantic parsing.” Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011. ^ Fan, Xing, et al. “Transfer Learning for Neural Semantic Parsing.” arXiv preprint arXiv:1706.04326 (2017). ^ Peng, Hao, Sam Thomson, and Noah A. Smith. “Deep Multitask Learning for Semantic Dependency Parsing.” arXiv preprint arXiv:1704.06855 (2017). ^   ","date":1510128519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531124725,"objectID":"b5755536323e1b0b54e7c499bab8e6b3","permalink":"https://desh2608.github.io/post/trends-in-semantic-parsing-2/","publishdate":"2017-11-08T13:38:39+05:30","relpermalink":"/post/trends-in-semantic-parsing-2/","section":"post","summary":"In Part 1 of this two-part series, I discussed some supervised approaches for the objective. In this part, we will look at some unsupervised or semi-supervised approaches, namely a Bayesian model, and transfer learning.\nAn unsupervised Bayesian model This paper was published in ACL 20111, back when statistical methods were still being used for NLP tasks. But with the recent forays into generative models, I feel it has again become relevant to understand how such methods worked.","tags":["natural language processing","semantic parsing"],"title":"Trends in Semantic Parsing - Part 2","type":"post"},{"authors":null,"categories":[],"content":" The International Conference on Learning Representations (ICLR) has evolved into the deep learning conference over the last few years, and with its open review system, it is not difficult to understand why. I was recently going through some of the papers accepted at this year’s ICLR, especially the 3 that were awarded the Best Paper award. In this article, I will try to summarize these 3 papers in simple words, and hopefully get an idea about what’s hot in deep learning.\nThe 3 best papers are:\n Understanding deep learning requires rethinking generalization Making neural programming architectures generalize via recursion Semi-supervised knowledge knowledge transfer for deep learning from private training data  Statisticians always like saying that deep learning is a black box and eveything that happens is a result of hyperparameter tuning. You cannot say why you have obtained a good result, let alone providing a guarantee for a result. Well, not anymore. The 3 best papers were all about providing proveable guarantees, and it looks like the deep learning community is all set to move past its “black box” days.\nUnderstanding deep learning requires rethinking generalization This paper from Google Brain is very readable and discusses some very common occurences. The authors make 2 very straightforward observations: (i) Deep neural networks easily fit random labels, and (ii) Explicit regularization is neither necessary nor sufficient for controlling generalization error.\nEssentially, they evaluate well-known deep architectures on several popular datasets, with some randomness added, such as random labels, or Gaussian noise-added labels, or shifted input features, in an attempt to show that even though training error is still close to negligible, generalization error increases alarmingly even in the presence of explicit regularizers such as weight decay, dropout, and data augmentation, and for implicit regularizers such as early stopping. This serves as a wake-up call for people who study the performance of neural networks.\nThe most interesting (and PROVABLE) guarantee that the paper contains is the following theorem: There exists a two-layer neural network with ReLU activations and 2n+d weights that can represent any function on a sample of size n in d dimensions. While I will not go into the detailed proof here, it is essentially based on solving the system of linear equations based on the ReLU activation function. For the system to have a solution, the coefficient matrix should be full-ranked, which the authors show is indeed the case. If the proof in the paper is too formal (read: succinct) for you, you can find a more detailed one here. In addition, they also show ways in which we can reduce the width of the network at each layer by increasing its depth.\nMaking neural programming architectures generalize via recursion Understanding this paper from researchers at UC Berkeley requires a little background of neural programmer-interpreter (NPI) architectures, which can be found in the paper as well as in the ICLR ’16 paper in which they were introduced. Basically, an NPI framework consists of a controller (such as an LSTM), which takes as input the environment state and arguments, and returns the next program pointer to be executed. In this way, given a set of sequences that an algorithm must follow to get to the output, the NPI can learn the algorithm itself. In the original paper, the authors learned to perform tasks such as adding, sorting, etc. using NPIs.\nIn this paper, the concept of recursion is added to the existing NPI framework, and this makes it capable of performing much more complex tasks such as quick sort. Formally, a function exhibits recursive behavior when it possesses two properties: (1) Base cases — terminating scenarios that do not use recursion to produce answers; (2) A set of rules that reduces all other problems toward the base cases. In the paper, the author describe how they construct NPI training traces so as to make them contain recursive elements and thus enable NPI to learn recursive programs.\nFurthermore, the authors show provably perfect generalization for their new architecture. The theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn.\nTo prove this, we again go to the notions of base case and recursive case. For example, in the addition task, the base case is always a set of small, fixed size step input sequences during which the LSTM state remains constant. So the base case is trivially true. The key in proving the recursive step is to construct the verification set well, so that the step inputs are neither too large so as to be outside the scope of evaluation, nor too small so that the semantics of the problem are not well defined. The actual verification process is simple and can be read in the paper.\nSemi-supervised knowledge transfer for deep learning from private training data Another paper from Google Brain, this deals with the important subject of building a model which learns from sensitive data while also keeping it private. A new model called PATE (Private Aggregation of Teacher Ensembles) is introduced which basically trains in a 2-step strategy:\n An ensemble of teacher models is trained on disjoint subsets of the sensitive dataset. A student model is trained on the aggregate output of the ensemble.  The aggregation is “private” because the number of times the student can access the teacher model is limited, and the top vote of the ensemble is revealed only after adding random noise. Due to these restrictions, no amount of querying can get hold of the private training data used to train the teacher models. Furthermore, in the Laplacian noise added to the teacher aggregation, the noise parameter can be used to tune the privacy-accuracy tradeoff for the student model. For example, if the noise is large, privacy is high at the cost of reduced accuracy, and vice versa.\nFor the transfer of knowledge from the ensemble to the student model, the authors experimented with various techniques and finally used semi-supervised learning with GANs. The student is trained on nonsensitive data (which may be labeled or unlabeled). The discriminator is a multi-class classifier which is trained such that it classifies the labeled data into the correct class, the unlabeled (true) data into any of the k classes, and the generated data (from the generator) into an extra class.\nAgain, the authors use the notion of “differentiable privacy guarantee” to come up with a lower bound for the privacy guarantee for their model. (The derivation is a little involved and I skipped it since I don’t have the prerequisites of security and privacy.)\nTo sum up, all the papers seem to provide some generalization guarantee rather than just proposing a “good” model. Looks like sunny days for deep learning!\n","date":1508054897,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531124725,"objectID":"91a82d86e370c07dbc8830798c4a9ee9","permalink":"https://desh2608.github.io/post/best-papers-at-iclr-17/","publishdate":"2017-10-15T13:38:17+05:30","relpermalink":"/post/best-papers-at-iclr-17/","section":"post","summary":"The International Conference on Learning Representations (ICLR) has evolved into the deep learning conference over the last few years, and with its open review system, it is not difficult to understand why. I was recently going through some of the papers accepted at this year’s ICLR, especially the 3 that were awarded the Best Paper award. In this article, I will try to summarize these 3 papers in simple words, and hopefully get an idea about what’s hot in deep learning.","tags":["deep learning"],"title":"The Best Papers at ICLR 2017","type":"post"},{"authors":null,"categories":[],"content":" While working on my undergrad thesis on relation classification of biomedical text using deep learning methods, I quickly hacked together models in Tensorflow that combined convolutional and recurrent layers in various combinations. While some of these “network architectures” worked superbly (even surpassing state-of-the-art results), I had no clue what was happening inside the model. To gain such an intuition, I read about 20 recent papers on text classification (starting with the first “CNN for sentence classification” paper by Yoon Kim) over the course of a week. Aside from an obvious enlightenment about why my architecture was working the way it was, I also gained valuable insight into how results are presented by experts like Yann LeCunn and Tommi Jaakkola (which would later help me in getting my CoNLL paper accepted as an undergrad).\nAnyway, so while reading these myriad of text classification papers, I subconsciously began organizing them under different heads, depending upon the kind of approach used. The common objective across each of these approaches was that they all wanted to model the structural information of the sentence into the sentence embedding. All of this was in March, and ever since, I have wanted to organize the notes I made from my readings into a formal article, so that others may benefit from the insights.\nSome background in CNNs and LSTMs is assumed.\nCharacter to sentence level embeddings Using word vectors (conventionally obtained using Word2Vec or GloVe) has been the most popular technique for input feature embedding. Yann LeCunn proposed character-level embeddings in his NIPS 2015 paper1, and the motivation behind this was that language could also be thought of as a signal similar to speech, with each character representing one bit of information. As such, it was reasonable to encode characters rather than words to obtain sentence level structure more efficiently. Although the proposed method was outperformed even by traditional tf-idf approaches for smaller datasets, the most important hypothesis obtained from empirical analyses was that character level CNNs tend to work well with uncurated user-generated data, such as reviews on Amazon. This makes them especially suitable for use in data wherever misspellings or use of exotic characters is frequent, such as in tweets.\nEven before LeCunn’s work, a COLING 2014 paper2 from IBM Research (Brazil) combined embeddings from the character, word, and sentence levels to obtain an amalgamation for the sentence representation. This is done in two convolutional layers as follows:\n In the first layer, vectors are obtained for words using traditional lookup techniques like Word2Vec. At the same time, character-level input vectors corresponding to each word are fed into a convolutional layer and a subsequent max pooling layer, and padding is used so that fixed length outputs are obtained for every word. These convolved features are concatenated with the word-level embeddings to obtain the joint word vector. The rationale behind this is that while word-level embeddings are meant to capture syntactic and semantic information, character-level embeddings capture morphological and shape information.   The second layer to obtain sentence-level vectors is similar to the character level. On applying convolutions and max pooling, we obtain a global feature representation for the sentence.  Embeddings have become a staple in deep learning models for NLP, and the latest trend is to use deep transfer learning to learn entire parameters for word vectors. While the community has mostly stabilized on using word-level vectors for input features, it wasn’t for lack for exploration, as is evident from these early approaches.\nEncoding structural information: parse trees and tensor algebra Again, for accurate text classification, it is imperative to obtain a good sentence representation that effectively captures the structural information and any semantics possible. If we think about Yoon Kim’s original CNN model in this vein, the limitations in the simple “conv+pool” model becomes obvious. While the convolutional layer helps to recognize short phrases, the final max pooling layer completely disregards any word order or structural information in the sentence. Essentially, we can reorder phrases in the sentences, and the representation would still remain the same.\nTo solve these problems, a myriad of techniques have been proposed. Here I will discuss 2 of them — the first involves using syntactic parse trees, and the second turns to good old tensor algebra.\nA 2015 paper3 from Peking University proposed two tree-based CNN models, namely c-TBCNN and d-TBCNN, depending on whether constituency or dependency parse trees were used. I will first outline the model:\n A sentence is first converted to a parse tree, and each node is represented as a distributed, real-valued vector. While the nodes of dependency trees are words themselves, those in constituency trees are not. To solve this problem, constituency tree nodes are pretrained using Socher’s RNN and kept fixed thereafter.   A tree-based convolutional window is defined, which slides over the entire tree to extract structural information of the sentence. The convolutional equation for a window which slides over a parent and its direct children in a constituency tree is given by  $$ y = f(w_p^{( c )}\\cdot p + w_l^{( c )}\\cdot c_l + w_r^{( c )}\\cdot c_r + b^{( c )}). $$\n In a dependency tree, a node can have any number of children. To overcome this, weights in these trees are assigned according to dependency type rather than position, and so the convolution formula becomes  $$ y = f(W_p^{(d)}\\cdot p + \\sum_{i=1}^n W_{r[c_i]}^{(d)}\\cdot c_i + b^{(d)}). $$\nIn empirical evaluation, d-TBCNN was found to outperform c-TBCNN probably due to d-TBCNN being able to exploit structural features more efficiently because of the compact expressiveness of dependency trees. The paper also provides visualizations for understanding the mechanism of the proposed network, and they show that TBCNNs do integrate information about different words in a window.\nA 2015 paper4 from Regina Barzilay and Tommi Jaakkola at MIT used non-linear, non-consecutive convolutions, and turned to tensor algebra to reduce computational complexity. The motivation behind this model is two-fold:\n Conventional CNNs use linear operations on stacked word vectors, which ignores the interesting non-linear interaction between n-grams. Consecutive convolutions misses out on the non-consecutive phrases e.g. \u0026ldquo;not nearly as good\u0026rdquo; etc.  Essentially, they modified the 2 main components of a CNN-based text classification module, namely window-based convolutions, and the linear convolution operation, with 3 novel modifications.\n Stacked n-gram word vectors are replaced by tensor products, and this n-gram tensor can be seen as a generalization of the typical concatenated vector. Since the convolutional filters themselves are high-dimensional tensors (n dimensions corresponding to the size of tensor window, and 1 channel dimension), directly maintaining them as full tensors would lead to parametric explosion. To overcome this, the convolutional tensor is represented using low-rank factorization. Instead of applying convolutions only to consecutive n-grams, all possible n-grams are used. At each position, the aggregate representation is the weighted sum of all n-gram representations ending at that position.  The paper makes use of linear algebra very cleverly to extend simple convolution operations across the whole sentence without making it computationally infeasible. In the results section, the authors have also analyzed the importance of such non-linear and non-consecutive activations empirically.\nRegional (two-view) embeddings In a series of papers (published at NIPS 20155 and ICML 20166), Rie Johnson and Tong Zhang introduced the concept of regional embedding in sentences, which was based on two-view embeddings. Essentially, they wanted to answer the question: Can an unlabeled data be used to augment a CNN/LSTM module in a better way than by simply obtaining pretrained word vectors?\nIn some way, these embeddings are also related to the first section on character to sentence level embeddings. However, I have put it in a separate section since my own network architecture in my CoNLL paper derived hugely from the interpretation given in these papers. (You can say this was when I gained enlightenment!)\nIn an earlier paper, the authors had showed that using high-dimensional one-hot bag-of-words (BOW) vectors rather than pretrained word vectors proved to be better in simpler systems. Their new objective was to learn regional embeddings from unlabeled data and use it as additional input to the supervised CNN.\nBut first, what is a tv-embedding? Essentially, it is a function of a view that preserves everything required to predict another view. (See the paper section 2 for details. The motivation for using tv-embeddings is also explained theoretically in the Appendix 5.)\nIn the papers, the authors used a CNN and an LSTM, respectively, to obtain these tv-embeddings for short regions in the sentences using an unlabeled corpus. They called these as “regional embeddings,” and used them as additional input for the supervised classification task. Furthermore, in their ICML paper, they did away with CNNs entirely, and argued that using bidirectional LSTMs for obtaining the regional embedding and then pooling for the sentence vector gives and adequate sentence representation. However, experimental results showed that using tv-embeddings from networks resulted in the best performing model.\nThis “regional embedding+pooling” logic was what finally provided the necessary intuition for my own relation classification network.\n Zhang, Xiang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classification.” Advances in neural information processing systems. 2015. ^ Dos Santos, Cícero Nogueira, and Maira Gatti. “Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts.” COLING. 2014. ^ Mou, Lili, et al. “Discriminative neural sentence modeling by tree-based convolution.” arXiv preprint arXiv:1504.01106 (2015). ^ Lei, Tao, Regina Barzilay, and Tommi Jaakkola. “Molding CNNs for text: non-linear, non-consecutive convolutions.” arXiv preprint arXiv:1508.04112 (2015). ^ Johnson, Rie, and Tong Zhang. “Semi-supervised convolutional neural networks for text categorization via region embedding.” Advances in neural information processing systems. 2015. ^ Johnson, Rie, and Tong Zhang. “Supervised and semi-supervised text categorization using LSTM for region embeddings.” International Conference on Machine Learning. 2016. ^   ","date":1506928754,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531124725,"objectID":"20e29b6e0cb61e03f87e97221980bd29","permalink":"https://desh2608.github.io/post/last-3-years-in-text-classification/","publishdate":"2017-10-02T12:49:14+05:30","relpermalink":"/post/last-3-years-in-text-classification/","section":"post","summary":"While working on my undergrad thesis on relation classification of biomedical text using deep learning methods, I quickly hacked together models in Tensorflow that combined convolutional and recurrent layers in various combinations. While some of these “network architectures” worked superbly (even surpassing state-of-the-art results), I had no clue what was happening inside the model. To gain such an intuition, I read about 20 recent papers on text classification (starting with the first “CNN for sentence classification” paper by Yoon Kim) over the course of a week.","tags":["natural language processing","deep learning","text classification"],"title":"The Last 3 Years in Text Classification","type":"post"},{"authors":null,"categories":[],"content":" This article is a formal representation of my understanding of vector semantics, from course notes and reading reference papers and chapters from Jurafsky’s SLP book. I will be talking about sparse and dense vector semantics, including SVD, skip-gram, and GloVe. In many places, I will try to explain the ideas in language rather than equations (but I’ll provide links to derivations and stuff wherever it is absolutely essential, which is actually everywhere!).\n “You shall know a word by the company it keeps.”\n In vision, images are represented by the corresponding RGB values (or values obtained from other filters), so they are essentially matrices of integers. Language was more arbitrary because traditionally there was no formal method (or globally accepted standard) for representing words with numerical values. Well, not until word embeddings came into the picture (no pun intended)!\nWhat are embeddings, though? They are called so because words are essentially transformed into vectors by “embedding” them into a vector space. For this, we make use of the hypothesis that words which occur in similar context tend to have similar meaning, i.e., the meaning of a word can be inferred from the distribution around it. For this reason, these methods are also called “distributional” methods.\nWord vectors may be sparse or dense. I’ll begin with sparse vectors and then describe dense ones.\nSparse vectors Term-document and term-term matrix Suppose we have a set of 1000 documents, consisting of a total of 5000 unique words. In a very naive fashion, we can simply count the number of occurrences of each word in every document, and then represent each word by this 1000-dimensional vector of counts. This is exactly what a term-document matrix does.\nSimilarly, consider a large corpus of text with 5000 unique words. Now take a window of some fixed size and for each word pair, we count the number of times it occurs in the window. These counts form a term-term matrix, also called a co-occurrence matrix which in this case will be a 5000x5000 matrix (with most cells 0 if the window size is relatively small).\nPointwise Mutual Information (PMI) The co-occurrence matrix is not the best measure of similarity between 2 words since it is based on the raw frequency, and hence is very skewed. Instead, it would be desirable to have a quantity which measures how much more likely is it for 2 words to occur in a window, compared with pure chance. This is exactly what PMI measures.\n$$ \\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right) $$\nIf PMI is positive, the ($x$,$y$) pair is more likely to occur together than pure chance, and vice versa. However, a negative value is unreliable since it is unlikely to get many co-occurrences of a word pair in a small corpus. To solve this problem, we define a Positive PMI (PPMI) as\n$$ \\text{PPMI}(x,y) = \\max (\\text{PMI}(x,y),0). $$\nTF-IDF (Term frequency — inverse document frequency) This is composed of 2 parts: TF, which denotes the count of the word in a document, and IDF, which is a weight component that gives higher weight to words occurring only in a few documents (and hence are more representative of the documents they are present in, in contrast to words like ‘the’ which are present in large number of documents).\n$$ idf_i = \\log \\left( \\frac{N}{df_i} \\right) $$\nHere, $N$ is the total number of documents and $df_i$ is the number of documents in which word $i$ occurs.\nDense vectors The problem with sparse vectors is the curse of dimensionality, which makes computation and storage infeasible. For this reason, we prefer dense vectors, with real-valued elements. Dense vector semantics fall into 2 categories: matrix factorization, and neural embeddings.\nMatrix Factorization Singular vector decomposition (SVD) This is basically a dimensionality reduction technique where we find the dimensions with the highest variances. Suppose we have the co-occurence matrix A of size $m \\times n$, then it is possible to factorize A into:\n$$ A_{m \\times n} = U_{m\\times r}S_{r\\times r}V_{r\\times n}^T $$\nwhere $r$ is the rank of matrix $A$ (i.e. $r$ = maximum number of linearly independent vectors that can be used to form $A$). Also, $U$ is a matrix of the eigenvectors of $AA^T)$ and $S$ is a diagonal matrix comprising its eigenvalues. If we rearrange the columns in $U$ to correspond with a decreasing order of eigenvalues, we can keep the first $k$ columns which will represent the dimensions in the latent space which have the highest variance. These will give us a $k$-dimensional representation for each of the $m$ words in the vocabulary.\nBut why do we want to perform this truncation?\n First, removing the lower variance dimensions filters the noise component from the word embeddings. More importantly, having a lower number of parameters leads to better generalization. It is found that 300-dimensional word embeddings perform much better than, say, 3000-dimensional ones.  However, this approach is still constrained since the matrix factorization of $A$, which in itself may be a large matrix, is computationally complex.\nNeural embeddings The idea is simple. We can treat each element in the vector as a parameter to be updated while training a neural network model. We start with a randomly initialized vector and update it at each iteration. This update is based on the vectors of the context (window) words. The hypothesis is that such an update would ultimately result in similar words having vectors which are closer to each other in the vector space.\nHere, I will describe the 2 most popular neural models — Word2Vec and GloVe.\nWord2Vec Word2Vec is actually the name of a tool which internally uses skip-gram or CBOW (continuous bag-of-words) with negative sampling. The objectives for both these models are quite similar, except a subtle distinction. In skip-gram, we predict the context words given the target word, and in CBOW, we predict the target word given the context words. In this article, I will limit my discussion to skip-gram with negative sampling (SGNS).\nSuppose we have a context window where $w$ is the target word and $c$ is one of the context words. Then, skip-gram’s objective is to compute $P(c|w)$, which is given as\n$$ p(c|w;\\theta) = \\frac{\\exp(v_c\\cdot v_w)}{\\sum_{c^{\\prime}\\in C}\\exp(v_{c^{\\prime}}\\cdot v_w)}. $$\nBasically, it is just a softmax probability distribution over all the word-context pairs in the corpus, directed by the cosine similarity. However, the denominator term here is very expensive to compute since there may be a very large number of context words. To solve this problem, negative sampling is used.\nGoldberg and Levy have explained the derivation for the objective function in SGNS very clearly in their note. I will try to provide a little intuition here.\nFor the word $w$, we are trying to predict the context word $c$. Since we are using softmax, this is essentially like a multi-class classification problem, where we are trying to classify the next word into one of $N$ classes (where $N$ is the number of words in the dictionary). Since $N$ may be quite large, this is a very difficult problem.\nWhat SGNS does is that it converts this multi-classification problem into binary classification. The new objective is to predict, for any given word-context pair ($w$,$c$), whether the pair is in the window or not. For this, we try to increase the probability of a “positive” pair ($w$,$c$), while at the same time reducing the probability of $k$ randomly chosen “negative samples” ($w$,$s$) where $s$ is a word not found in $w$’s context. This leads to the following objective function which we try to maximize in SGNS:\n$$ J = \\log \\sigma(c\\cdot w) + \\sum_{i=1}^k \\mathbb{E}_{w_i \\sim p(w)}[\\log \\sigma (-w_i \\cdot w)] $$\nGloVe (Global Vectors) One grievance with skip-gram and CBOW is that since they are both window-based models, the co-occurrence statistics of the corpus are not used efficiently, thereby resulting in suboptimal embeddings. The GloVe model proposed by Pennington et al. seeks to solve this problem by formulating an objective function from probability statistics.\nAgain, the original paper is very pleasant to read (section 3 describes their model in detail), and it is interesting to note the derivation for the objective function:\n$$ J = \\sum_{i,j=1}^V f(X_{ij})(w_i^Tw_j + b_i + b_j - \\log X_{ij})^2 $$\nHere, $X_{ij}$ is the count of the word pair ($i$,$j$) in the corpus. The weight function $f(x)$ has 3 requirements:\n $f(0) = 0$, so that the entire term does not tend to $\\infty$. It should be non-decreasing to assign low weights to rare occurrences. It should be relatively small for large values of $x$.  Again, please read the paper for details.\nAlthough the matrix factorization approach and the neural embedding method may initially come off as completely independent, Levy and Goldberg (again!) ingeniously showed in a NIPS 2014 paper that even the SGNS method implicitly factorizes a word-context matrix where the cells are the PMI (pointwise mutual information) of the respective word-context pairs, shifted by a global context. They derive this in Section 3.1 of the paper, and I urge you to go to the link and read it. It’s a delight! The derivation is really simple and I would have done it here, except that I would only be reproducing the exact proof.\nVery recently, Richard Socher’s group at Salesforce Research have proposed a new kind of embeddings called CoVe (Contextualized Word Vectors) in their paper. The idea is again borrowed from vision, where transfer learning has been used for a long time. Basically, models with various objectives are trained on a large dataset such as ImageNet, and then these weights are used to initialize model parameters for various vision tasks. Similarly, CoVe uses parameters trained on a attentional Seq2Seq machine translation task, and then uses it for various other tasks, including question-answering, where it has shown state-of-the-art performance on the SQuAD dataset. I have only skimmed through the paper, but I suppose such a deep transfer learning is naturally the next step towards improving word embeddings.\nAs an aside, there is a series of blog posts by Sanjeev Arora that analyzes the theory of semantic embeddings in great detail. There are 3 posts in the series:\n Semantic word embeddings Word Embeddings: Explaining their properties Linear algebraic structure of word embeddings  These provide great insight into the mathematics behind word vectors, and are beautifully written (which is no surprise since Prof. Arora is one of the authors of the famous and notoriously advanced book on Computational Complexity).\n","date":1506663775,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530685249,"objectID":"80787199bf614dd1e63f9f30b8cd0f1a","permalink":"https://desh2608.github.io/post/understanding-word-vectors/","publishdate":"2017-09-29T11:12:55+05:30","relpermalink":"/post/understanding-word-vectors/","section":"post","summary":"This article is a formal representation of my understanding of vector semantics, from course notes and reading reference papers and chapters from Jurafsky’s SLP book. I will be talking about sparse and dense vector semantics, including SVD, skip-gram, and GloVe. In many places, I will try to explain the ideas in language rather than equations (but I’ll provide links to derivations and stuff wherever it is absolutely essential, which is actually everywhere!","tags":["deep learning","natural language processing","representation learning"],"title":"Understanding Word Vectors","type":"post"},{"authors":null,"categories":[],"content":" In this article, I will try to round up some (mostly neural) approaches for semantic parsing and semantic role labeling (SRL). This is not an extensive review of these methods, but just a collection of my notes on reading some recent research on the subject. However, I do believe it covers most of the latest trends as well as their limitations.\nBut first, what is semantic parsing?\n“Semantic” refers to meaning, and “parsing” means resolving a sentence into its component parts. As such, semantic parsing refers to the task of mapping natural language text to formal representations or abstractions of its meaning. A syntactic parser may generate constituency or dependency trees from a sentence, but a semantic parser may be built depending upon the task for which inference is required.\nFor example, we can build a parser that converts the natural language query “*Who was the first person to walk on the moon?*” to an equivalent (although complex!) SQL query such as “SELECT name FROM Person WHERE moon_walk = true ORDER BY moon_walk_date FETCH first 1 rows only.”\nSemantic parsing is inherently more complicated than syntactic parsing because it requires understanding concepts from different word phrases. For instance, the following sentences (adapted from [4]) should ideally map to the same formal representation.\n Sentence 1: India defeated Australia.\nSentence 2: India secured the victory over the Australian team.\n For this reason, semantic parsing is more about capturing the meaning of the sentence rather than plain rule-based pattern matching.\nSemantic role labeling is a sub-task within the former, where the sentence is parsed into a predicate-argument format. The example given on the Wikipedia page for SRL explains this well. Given a sentence like “Mary sold the book to John,” the task would be to recognize the verb “to sell” as representing the predicate, “Mary” as representing the seller (agent), “the book” as representing the goods (theme), and “John” as representing the recipient. In this sense, SRL is sometimes also called shallow semantic parsing because the structure of the target representation is somewhat known.\nIn this article, I will describe models for both these tasks without explicit differentiation, mostly since the same models are found to work well on either task.\nLearning sentence embeddings using deep neural models Vector semantics have been used extensively in all NLP tasks, especially after word embeddings (Word2Vec, GloVe) were found to represent the synonymy-antonymy relations well in real space.\nSimilar to word embeddings, we can try to obtain dense vectors to represent a sentence, and then find some way to obtain the formal representation from it. Ivan Titov (University of Edinburgh) has recently proposed a couple of models which use LSTMs 1 and Graph CNNs 2 for dependency-based SRL task.\nI will first explain the task. We work on datasets where the predicates are marked in the sentence, and the objective is to identify and label the arguments corresponding to each predicate. For instance, given the sentence “Mary eats an apple,” and the predicate marked as EATS, we need to label the words ‘Mary,’ ‘an,’ and ‘apple’ as agent, NULL, and theme, respectively. Also, since a single sentence may contain multiple predicates, the same word may get different labels for each predicate. Essentially, if we repeat the process once for each predicate, out task effectively reduces to a sequence labeling problem.\nLSTM-based approach 1 : LSTMs (which are a type of RNNs that can preserve memory) have been used to model sequences since they were first introduced. In the first model, the sequence labeling is performed as follows.\n Vectors are obtained from each word by concatenating pre-trained embeddings (Word2Vec), random embeddings, and randomly initialized POS embeddings. The word vector also contains a 1-bit flag to mark whether it is the predicate in that particular training instance. This is done to ensure that the network treats each predicate differently. These are fed into a bi-LSTM layer to obtain the word’s context in the sentence. Finally, to label any word, we take the dot product of its hidden state with the predicate’s hidden state and obtain a softmax classifier over it as follows.  $$ p(r|v_i,v_p) \\propto \\exp(W_r (v_i \\cdot v_p)). $$\n Further, we can have the weight matrix parametrized on the role label $r$ as:  $$ W_{l,r} = ReLU(U(u_l \\cdot v_r)), $$\nwhere the vectors in the dot product correspond to randomly initialized embeddings for the predicate lemma and the role, respectively.\nGCN-based approach 2: In a second model, Graph Convolutional Networks (GCNs) have been used to represent the dependency tree for the sentence. In a very crude sense, a GCN input layer encodes the sentence into an $m X n$ matrix based on its dependency tree, such that each of the $n$ nodes of the tree is represented as an $m$-dimensional vector. Once such a matrix has been obtained, we can perform convolutions on it.\nIt is then evident that a one-layer GCN can capture information only about its immediate neighbor. By stacking GCN layers, one can incorporate higher degree neighborhoods.\nGCNs and LSTMs are complementary. Why? LSTMs capture long-term dependencies well but are not able to represent syntax effectively. On the other hand, GCNs are built directly on top of a syntactic-dependency tree so they capture syntax well, but due to the limitation of fixed-size convolutions, the range of dependency is limited. Therefore, using a GCN layer on top of the hidden states obtained from a bi-LSTM layer would theoretically capture the best of both worlds. This hypothesis has also been corroborated through experimental results.\nEncoder-decoder model 3: In this paper, the task is broadened into formal representation rather than SRL. If we consider the formal representation as a different language, this is similar to a machine translation problem, since both the natural as well as formal representations mean the same. As such, it might be interesting to apply models used for MT to semantic parsing. This paper does exactly this.\nAn encoder converts the input sequence to a vector representation and a decoder obtains the target sequence from this vector.\n The encoder uses a bi-LSTM layer similar to the previous methods to obtain the vector representation of the input sequence. The final hidden state is fed into the decoder layer, which is again a bi-LSTM. The hidden states obtained from this layer is used to predict the corresponding output tokens using a softmax function. Alternatively, we can have a hierarchical decoder to account for the hierarchical structure of logical forms. For this purpose, we simply introduce a non-terminal token, say , which indicates the start of a sub-tree. Other tokens may be used to represent the start/end of a terminal sequence or a non-terminal sequence. To incorporate the tree structure, we concatenate the hidden state of the parent non-terminal with every child. Finally in the decoding step, to better utilize relevant information from the input sequence, we use an attention layer where the context vector is a weighted sum over the hidden vectors in the encoder.  While these models are very inspired and intuitive, they are all supervised. As such, they are constrained due to cost and availability of annotated data, especially since manually labeling semantic parsing output is a time-consuming process. In part 2 of this article, I will talk about some approaches which overcome this issue.\n Marcheggiani, Diego, Anton Frolov, and Ivan Titov. “A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling.” arXiv preprint arXiv:1701.02593 (2017). ^ Marcheggiani, Diego, and Ivan Titov. “Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.” arXiv preprint arXiv:1703.04826 (2017). ^ Dong, Li, and Mirella Lapata. “https://arxiv.org/pdf/1601.01280.pdf.” arXiv preprint arXiv:1601.01280 (2016). ^   ","date":1505882002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530685249,"objectID":"71f682f0570d757edbd2bc650063acb2","permalink":"https://desh2608.github.io/post/trends-in-semantic-parsing-1/","publishdate":"2017-09-20T10:03:22+05:30","relpermalink":"/post/trends-in-semantic-parsing-1/","section":"post","summary":"In this article, I will try to round up some (mostly neural) approaches for semantic parsing and semantic role labeling (SRL). This is not an extensive review of these methods, but just a collection of my notes on reading some recent research on the subject. However, I do believe it covers most of the latest trends as well as their limitations.\nBut first, what is semantic parsing?\n“Semantic” refers to meaning, and “parsing” means resolving a sentence into its component parts.","tags":["natural language processing","semantic parsing"],"title":"Trends in Semantic Parsing - Part 1","type":"post"},{"authors":null,"categories":[],"content":" Simple natural language processing tasks such as sentiment analysis, or even more complex ones like semantic parsing are easy to evaluate since the evaluation simply requires label matching. As such, metrics like F-score (which is the harmonic mean of precision and recall), or even accuracy in uniformly distributed data, are used for such tasks.\nEvaluating natural language generation systems is a much more complex task, however. And for this reason, a number of different metrics have been proposed for tasks such as machine translation or summarization. In this blog, I describe 3 major schemes, namely BLEU, ROUGE, and METEOR.\nThe intuition for evaluating generated text is the same as that for evaluating labels. If candidate text A is a closer match to one of the reference texts than candidate text B, then we want to score A higher than B. As in other schemes, this matching is based on precision (specificity) and recall (sensitivity). To put it simply, A is more precise than B if the % of A that matches a reference text is higher than B. A’s recall is higher if it contains more matching text from a reference than B. For example:\n Reference: I work on machine learning.\nCandidate A: I work.\nCandidate B: He works on machine learning.\n In this toy example, A’s precision is higher than B (100% vs. 60%), but B’s recall is higher (60% vs. 40%). Note that in this example, we perform the matching simply using unigrams, which may not always be the case. In fact, this choice of features for computing precision and recall is essentially what differentiates the 3 schemes for NLG evaluation.\nBLEU (Bilingual Evaluation Understudy) This is by far the most popular metric for evaluating machine translation system. In BLEU, precision and recall are approximated by *modified n-gram precision * and best match length, respectively.\nModified n-gram precision: First, an n-gram precision is the fraction of n-grams in the candidate text which are present in any of the reference texts. From the example above, the unigram precision of A is 100%. However, just using this value presents a problem. For example, consider the two candidates:\n (i) He works on machine learning.\n(ii) He works on on machine machine learning learning.\n Candidate (i) has a unigram precision of 60% while for (ii) it is 75%. However, it is obvious that (ii) is not a better candidate than (i) in any way. To solve this problem, we use a “modified” n-gram precision. It matches the candidate’s n-grams only as many times as they are present in any of the reference texts. So in the above example, (ii)’s unigrams ‘on’, ‘machine’, and ‘learning’ are matched only once, and the unigram precision becomes 37.5%.\nFinally, to include all the n-gram precision scores in our final precision, we take their geometric mean. This is done because it has been found that precision decreases exponentially with n, and as such, we would require logarithmic averaging to represent all values fairly.\n$$ \\text{Precision} = \\exp\\left( \\sum_{i=1}^N w_n \\log p_n \\right), ~~~~ \\text{where}~~ w_n = \\frac{1}{n} $$\nBest match length: While precision calculation was relatively simple, the problem with recall is that there may be many reference texts. So it is difficult to calculate the sensitivity of the candidate with respect to a general reference. However, it is intuitive to think that a longer candidate text is more likely to contain a larger fraction of some reference than a shorter candidate. At the same time, we have already ensured that candidate texts are not arbitrarily long, since then their precision score would be low.\nTherefore, we can introduce recall by just penalizing brevity in candidate texts. This is done by adding a multiplicative factor BP with the modified n-gram precision as follows.\n$$ \\text{BP} = \\begin{cases} 1, \u0026amp;\\text{if} c \u0026gt; r, \\\\\\ \\exp(1-\\frac{r}{c},\u0026amp;\\text{otherwise}).\\end{cases} $$\nHere, $c$ is the total length of candidate translation corpus, and $r$ is the effective reference length of corpus, i.e., average length of all references. The lengths are taken as average over the entire corpus to avoid harshly punishing the length deviations on short sentences. As the candidate length decreases, the ratio $\\frac{r}{c}$ increases, and the BP decreases exponentially.\nROUGE (Recall Oriented Understudy for Gisting Evaluation) As is clear from its name, ROUGE is based only on recall, and is mostly used for summary evaluation. Depending on the feature used for calculating recall, ROUGE may be of many types, namely ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S. Here, we describe the idea behind one of these, and then give a quick run-down of the others.\nROUGE-N: This is based on n-grams. For example, ROUGE-1 counts recall based on matching unigrams, and so on. For any $n$, we count the total number of n-grams across all the reference summaries, and find out how many of them are present in the candidate summary. This fraction is the required metric value.\nROUGE-L/W/S are based on: longest common subsequence (LCS), weighted LCS, and skip-bigram co-occurence statistics, respectively. Instead of using only recall, these use an F-score which is the harmonic mean of precision and recall values. These are in turn, calculated as follows for ROUGE-L.\nSuppose A and B are candidate and reference summaries of lengths $m$ and $n$ respectively. Then, we have\n$$ P = \\frac{LCS(A,B)}{m} ~~\\text{and}~~ R = \\frac{LCS(A,B)}{n}. $$\n$F$ is then calculated as the weighted harmonic mean of P and R, as\n$$ F = \\frac{(1+b^2)RP}{R+b^2P}. $$\nSimilarly, in ROUGE-W, for calculating weighted LCS, we also track the lengths of the consecutive matches, in addition to the length of longest common subsequence (since there may be non-matching words in the middle). In ROUGE-S, a skip-bigram refers to any pair of words in sentence order allowing for arbitrary gaps. The precision and recall, in this case, are computed as a ratio of the total number of possible bigrams, i.e., ${n \\choose 2}$.\nMETEOR (Metric for Evaluation for Translation with Explicit Ordering) METEOR is another metric for machine translation evaluation, and it claims to have better correlation with human judgement.\nSo why do we need a new metric when BLEU is already available? The problem with BLEU is that since the *BP*value uses lengths which are averaged over the entire corpus, so the scores of individual sentences take a hit.\nTo solve this problem, METEOR modifies the precision and recall computations, replacing them with a weighted F-score based on mapping unigrams and a penalty function for incorrect word order.\nWeighted F-score: First, we try to find the largest subset of mappings that can form an alignment between the candidate and reference translations. For this, we look at exact matches, followed by matches after Porter stemming, and finally using WordNet synonymy. After such an alignment is found, suppose $m$ is the number of mapped unigrams between the two texts. Then, precision and recall are given as $\\frac{m}{c}$ and $\\frac{m}{r}$, where $c$ and $r$ are candidate and reference lengths, respectively. F is calculated as\n$$ F = \\frac{PR}{\\alpha P + (1-\\alpha)R}. $$\nPenalty function: To account for the word order in the candidate, we introduce a penalty function as\n$$ p = \\gamma \\left( \\frac{c}{m} \\right)^{\\beta},~~~~ \\text{where}~~ 0 \\leq \\gamma \\leq 1. $$\nHere, $c$ is the number of matching chunks and $m$ is the total number of matches. As such, if most of the matches are contiguous, the number of chunks is lower and the penalty decreases. Finally, the METEOR score is given as $(1-p)F$.\nThe links to the original papers for the methods described here are in the section headings. Readers are advised to refer to them for details. I have tried to outline the main ideas here for a quick review.\n","date":1505533544,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530685249,"objectID":"bb157002b67e1bfe1834ba05d9c25ec6","permalink":"https://desh2608.github.io/post/metrics-for-nlg-evaluation/","publishdate":"2017-09-16T09:15:44+05:30","relpermalink":"/post/metrics-for-nlg-evaluation/","section":"post","summary":"Simple natural language processing tasks such as sentiment analysis, or even more complex ones like semantic parsing are easy to evaluate since the evaluation simply requires label matching. As such, metrics like F-score (which is the harmonic mean of precision and recall), or even accuracy in uniformly distributed data, are used for such tasks.\nEvaluating natural language generation systems is a much more complex task, however. And for this reason, a number of different metrics have been proposed for tasks such as machine translation or summarization.","tags":["machine translation","natural language processing","natural language generation"],"title":"Metrics for NLG Evaluation","type":"post"},{"authors":["**Desh Raj**","Sunil Kumar Sahu","Ashish Anand"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"1ea5244378d365370545a5fa8dfd6a56","permalink":"https://desh2608.github.io/publication/conll-17-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conll-17-learning/","section":"publication","summary":"","tags":null,"title":"Learning local and global context using a convolutional recurrent network model for relation classification in biomedical text","type":"publication"},{"authors":null,"categories":null,"content":"We conjecture that predictability of a text is a viable metric of its readability. By using modern language models as predictors, we believe this metric may provide an automated, fine-grained measure of readability. It also provides a natural mechanism to combine scores from different language models, and hence the ability to generalize to a diverse set of texts. Individual language models encode the specific linguistic background that a reader may have, hence providing customized scores for each type of reader. Our work provides authors with a valuable tool to\n assess the readability of their content for readers with different linguistic backgrounds, and identify pain-points at a word-level granularity in their text in order to improve it.  Our evaluations support our conjecture and show that the resulting scores work across a wide range of scenarios.\n Report Slides  ","date":1493551904,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530628474,"objectID":"6f19bae32134b4b001073c72c8272362","permalink":"https://desh2608.github.io/project/readability/","publishdate":"2017-04-30T17:01:44+05:30","relpermalink":"/project/readability/","section":"project","summary":"Course project under [Prof. Ashish Anand](http://www.iitg.ac.in/anand.ashish/)","tags":["natural language processing"],"title":"Text readability analysis using language modeling","type":"project"},{"authors":null,"categories":null,"content":"The objective of the project was to devise a method for obtaining structured triplets from unstructured clinical records such as journal articles, patient health records etc. Simplifying this objective, I was tasked with creating a neural technique which can classify relations existing between entities in a given sentence, an NLP task known as relation classification.\nThe key insight is that convolutions can capture short-term phrases, while recurrence learns long-term dependencies. Combining both, we proposed the CRNN model which outperformed earlier single and double layer methods on two benchmark datasets: i2b2-2010 and DDI. Details about the method can be found in the publication.\nThis project was done as part of my undergraduate senior thesis.\n","date":1493551824,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530628474,"objectID":"8ef556cdab88c736ba70c14b292d2b23","permalink":"https://desh2608.github.io/project/btp/","publishdate":"2017-04-30T17:00:24+05:30","relpermalink":"/project/btp/","section":"project","summary":"Bachelor Thesis Project under [Prof. Ashish Anand](http://www.iitg.ac.in/anand.ashish/)","tags":["deep learning","natural language processing"],"title":"Relation extraction for clinical text","type":"project"},{"authors":["**Desh Raj**","Aditya Gupta","Bhuvnesh Garg","Kenil Tanna","Frank Chung-Hoon Rhee"],"categories":null,"content":"","date":1491039138,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"16292f4779bcce0e4c967c5bfefc4ec8","permalink":"https://desh2608.github.io/publication/tfs-16-analysis/","publishdate":"2017-04-01T15:02:18+05:30","relpermalink":"/publication/tfs-16-analysis/","section":"publication","summary":"","tags":["fuzzy"],"title":"Analysis of data generated from multidimensional type-1 and type-2 fuzzy membership functions","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://desh2608.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564014817,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://desh2608.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"The objective of the project was to propose guidelines for selecting fuzzy membership functions to represent several data sets. In general, the accuracy of representation increases with increasing complexity, which makes it a trade-off. My contributions are listed below.\n Analyzed various multidimensional fuzzy membership functions and compared similarity of data sets using Wilcoxons nonparametric tests. Established guidelines for selecting appropriate MFs based on data set and application requirements. Recently extended the proposed method for high-dimensional data using dimensionality reduction approaches like PCA, kernel PCA, probabilistic PCA, and t-SNE.  We proposed a new log-time algorithm which makes use of Wilcoxon\u0026rsquo;s nonparametric tests to compare similarity between the original data and the synthetic data generated using the fuzzy MFs. The returned similarity score guides the choice of membership function.\n","date":1437564665,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530628474,"objectID":"5e1ee3285d85d44180e55e039a3ca235","permalink":"https://desh2608.github.io/project/fuzzy-wmbs/","publishdate":"2015-07-22T17:01:05+05:30","relpermalink":"/project/fuzzy-wmbs/","section":"project","summary":"Summer research project under [Prof. Frank Chung-Hoon Rhee](http://fuzzy.hanyang.ac.kr/members_prof.html)","tags":["fuzzy"],"title":"Similarity analysis on multidimensional fuzzy sets","type":"project"}]