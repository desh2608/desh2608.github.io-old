<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.42.2" />
  <meta name="author" content="Desh Raj">

  
  
  
  
    
      
    
  
  <meta name="description" content="Transfer learning is undoubtedly the new (well, relatively anyway) hot thing in deep learning right now. In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks. In NLP, though, transfer learning was mostly limited to the use of pretrained word embeddings (which, to be fair, improved baselines significantly).">

  
  <link rel="alternate" hreflang="en-us" href="https://desh2608.github.io/post/transfer-learning-nlp/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-121781547-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <link rel="alternate" href="https://desh2608.github.io/index.xml" type="application/rss+xml" title="Desh Raj">
  <link rel="feed" href="https://desh2608.github.io/index.xml" type="application/rss+xml" title="Desh Raj">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://desh2608.github.io/post/transfer-learning-nlp/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/raj_desh26">
  <meta property="twitter:creator" content="@https://twitter.com/raj_desh26">
  
  <meta property="og:site_name" content="Desh Raj">
  <meta property="og:url" content="https://desh2608.github.io/post/transfer-learning-nlp/">
  <meta property="og:title" content="Transfer Learning in NLP | Desh Raj">
  <meta property="og:description" content="Transfer learning is undoubtedly the new (well, relatively anyway) hot thing in deep learning right now. In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks. In NLP, though, transfer learning was mostly limited to the use of pretrained word embeddings (which, to be fair, improved baselines significantly).">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-06-15T13:42:18&#43;05:30">
  
  <meta property="article:modified_time" content="2018-06-15T13:42:18&#43;05:30">
  

  
  

  <title>Transfer Learning in NLP | Desh Raj</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Desh Raj</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Transfer Learning in NLP</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-06-15 13:42:18 &#43;0530 IST" itemprop="datePublished dateModified">
      Jun 15, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Desh Raj">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://desh2608.github.io/post/transfer-learning-nlp/#disqus_thread"></a>
  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Transfer%20Learning%20in%20NLP&amp;url=https%3a%2f%2fdesh2608.github.io%2fpost%2ftransfer-learning-nlp%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdesh2608.github.io%2fpost%2ftransfer-learning-nlp%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdesh2608.github.io%2fpost%2ftransfer-learning-nlp%2f&amp;title=Transfer%20Learning%20in%20NLP"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fdesh2608.github.io%2fpost%2ftransfer-learning-nlp%2f&amp;title=Transfer%20Learning%20in%20NLP"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Transfer%20Learning%20in%20NLP&amp;body=https%3a%2f%2fdesh2608.github.io%2fpost%2ftransfer-learning-nlp%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>Transfer learning is undoubtedly the new (well, relatively anyway) hot thing in deep learning right now. In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks. In NLP, though, transfer learning was mostly limited to the use of pretrained word embeddings (which, to be fair, improved baselines significantly). Recently, researchers are moving towards transferring entire models from one task to another, and that is the subject of this post.</p>

<p><a href="http://ruder.io/" target="_blank">Sebastian Ruder</a> (whose biweekly newsletter inspires a lot of my deep learning reading) and <a href="https://www.fast.ai/about/#jeremy" target="_blank">Jeremy Howard</a> were perhaps the first to make transfer learning in NLP exciting through their <a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" target="_blank">ULMFiT method</a> which surpassed all text classification state-of-the-art. This Monday, <a href="https://openai.com/" target="_blank">OpenAI</a> <a href="https://blog.openai.com/language-unsupervised/" target="_blank">extended their idea</a> and outperformed SOTAs on several NLP tasks. At NAACL 2018, the Best Paper award was given to the paper introducing <a href="https://allennlp.org/elmo" target="_blank">ELMo</a>, a new word embedding technique very similar to the idea behind ULMFiT, from researchers at <a href="https://allenai.org/" target="_blank">AllenAI</a> and <a href="https://www.cs.washington.edu/people/faculty/lsz/" target="_blank">Luke Zettlemoyer</a>’s group at UWash (Seattle).</p>

<p>In this article, I will discuss all of these new work and how they are interrelated. Let’s start with Ruder and Howard’s trend-setting architecture.</p>

<hr />

<h3 id="universal-language-model-fine-tuning-for-text-classification-https-arxiv-org-pdf-1801-06146-pdf"><a href="https://arxiv.org/pdf/1801.06146.pdf" target="_blank">Universal Language Model Fine-Tuning for Text Classification</a></h3>

<p>Most datasets for text classification (or any other supervised NLP tasks) are rather small. This makes it very difficult to train deep neural networks, as they would tend to overfit on these small training data and not generalize well in practice.</p>

<p>In computer vision, for a couple of years now, the trend is to pre-train any model on the huge ImageNet corpus. This is much better than a random initialization because the model learns general image features and that learning can then be used in any vision task (say captioning, or detection).</p>

<p>Taking inspiration from this idea, Howard and Ruder propose a bi-LSTM model that is trained on a general language modeling (LM) task and then fine tuned on text classification. This would, in principle, perform well because the model would be able to use its knowledge of the semantics of language acquired from the generative pre-training. Ideally, this transfer can be done from any source task $S$ to a target task $T$. The authors use LM as the source task because:</p>

<ul>
<li>it is able to capture long-term dependencies in language</li>
<li>it effectively incorporates hierarchical relations</li>
<li>it can help the model learn sentiments</li>
<li>large data corpus is easily available for LM</li>
</ul>

<p>Formally, &ldquo;LM introduces a hypothesis space $H$ that should be useful for many other NLP tasks.&rdquo;</p>

<p>For the architecture, they use the then SOTA <a href="https://arxiv.org/pdf/1708.02182.pdf" target="_blank">AWD-LSTM</a> (which is, I suppose, a multi-layer bi-LSTM network without attention, but I would urge you to read the details in the paper from Salesforce Research). The model was trained on the WikiText-103 corpus.</p>

<p>Once the generic LM is trained, it can be used as is for multiple classification tasks, with some fine-tuning. For this fine tuning and subsequent classification, the authors propose 3 implementation tricks.</p>

<p><strong>Discriminative fine tuning:</strong> Different learning rates are used for different layers during the fine-tuning phase of LM (on the target task). This is done because the layers capture different types of information.</p>

<p><strong>Slanted triangular learning rates (STLR):</strong> Learning rates are first increased linearly, and then decreased gradually after a cut, i.e., there is a &ldquo;short increase&rdquo; and a &ldquo;long decay&rdquo;. This is similar to the aggressive cosine annealing learning strategy that is popular now.</p>

<p><img src="/img/20/stlr.png" alt="" /></p>

<p><strong>Gradual unfreezing:</strong> During the classification training, the LM model is gradually unfreezed starting from the last layer. If all the layers are trained from the beginning, the learning from the LM would be forgotten quickly, and so gradual unfreezing is important to make use of the transfer learning.</p>

<p>On the 6 text classification tasks that they evaluated, there was a relative improvement of 18–24% on the majority of tasks. Further, the following was observed:</p>

<ul>
<li>Only 100 labeled samples in classification were sufficient to match the performance of a model trained on 50–100x samples from scratch.</li>
<li>Pretraining is more useful on small and medium sized data.</li>
<li>LM quality affects final classification performance.</li>
</ul>

<p>The analysis in the paper is very thorough, and I would recommend going through it for details, and also to learn how to design experiments for strong empirical results. They suggest some possible future directions as follows:</p>

<ol>
<li>The LM pretraining and fine-tuning can be improved.</li>
<li>The LM can be augmented with other tasks in a multi-task learning setting.</li>
<li>The pretrained model can be evaluated on tasks other than classification.</li>
<li>Further analysis can be done to determine what information is captured during pretraining and changed during fine-tuning.</li>
</ol>

<p>1 and 3 should be noted, in particular, as that makes up the novelty in OpenAI’s new paper discussed below.</p>

<hr />

<h3 id="improving-language-modeling-by-generative-pre-training-https-s3-us-west-2-amazonaws-com-openai-assets-research-covers-language-unsupervised-language-understanding-paper-pdf"><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">Improving Language Modeling by Generative Pre-training</a></h3>

<p>This paper was published on ArXiv this Monday (11 June), and my Twitter feed has been inundated with talk about it since then. Jeremy Howard himself tweeted favorably about it, saying that this was exactly the kind of work he was hoping for in his &ldquo;future directions&rdquo;.</p>

<p><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">This is exactly where we were hoping our ULMFit work would head - really great work from <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a>! 😊<br><br>If you&#39;re doing NLP and haven&#39;t tried language model transfer learning yet, then jump in now, because it&#39;s a Really Big Deal. <a href="https://t.co/0Dj8ChCxvu">https://t.co/0Dj8ChCxvu</a></p>&mdash; Jeremy Howard (@jeremyphoward) <a href="https://twitter.com/jeremyphoward/status/1006262925986652161?ref_src=twsrc%5Etfw">June 11, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>What Alec Radford (the first author) does here is</p>

<ol>
<li>use a Transformer network (explained below in detail) instead of the AWD-LSTM; and</li>
<li>evaluate the LM on a variety of NLP tasks, ranging from textual entailment to question-answering.</li>
</ol>

<p>If you are already aware of the ULMFiT architecture, you only need to know 2 things to understand this paper: (a) how the Transformer works, and (b) how an LM-trained model can be used to evaluate the different NLP tasks.</p>

<h4 id="the-transformer">The Transformer</h4>

<p><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.WyH7hIrhXb0" target="_blank">This blog</a> provides an extensive description of the model, originally proposed in <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">this highly popular paper</a> from last year. Here I will go over the salient features. For details, you can go through the linked blog post or the paper itself.</p>

<p><img src="/img/20/transformer.png" alt="Single layer of Encoder (left) and Decoder (right) that is build out of *N*=6 identical layers." /></p>

<p>The problem with RNN-based seq2seq models is that since they are sequential models, they cannot be parallelized. One possible solution that was proposed to remedy this involved the use of fully convolutional networks with positional embeddings, but it required O(nlogn) time to relate 2 words at some distance in the sentence. The Transformer solves this problem by completely doing away with convolutions or recurrence, and relying entirely upon self-attention.</p>

<p>In a simple <em>scalar dot-product attention</em>, weight is computed by taking the dot product of the query (Q) and key (K). The weighted sum of all values V is then the required output. In contrast, in a <em>multihead attention</em>, the input vector itself is divided into chunks and then the scalar dot-product attention is applied on each chunk in parallel. Finally, we compute the average of all the chunk outputs.</p>

<p><img src="/img/20/multiatt.png" alt="Multi-head attention architecture" /></p>

<p>The final step consists of a position-wise FFN, which itself is a combination of 2 linear transformations and a ReLU for each position. The following GIF explains this process very effectively.</p>

<p><blockquote class="imgur-embed-pub" lang="en" data-id="a/GUwAEe9"><a href="//imgur.com/GUwAEe9">Transformer</a></blockquote><script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script></p>

<h4 id="task-specific-input-transformations">Task-specific input transformations</h4>

<p>The second novelty in the OpenAI paper is how they use the pretrained LM model on several NLP tasks.</p>

<ul>
<li><em>Textual entailment</em>: The text (t) and the hypothesis (h) are cocatenated with a $ in between. This makes it naturally suitable for evaluation on an LM model.</li>
<li><em>Text similarity</em>: Since the order is not important here, the texts are concatenated in both orders and then processed independently and added element-wise.</li>
<li><em>Question-answering and commonsense reasoning</em>: The text, query, and answer option are concatenated with some differentiation symbol in between and each such sample is processed. They are then normalized via softmax to produce output distribution over possible answers.</li>
</ul>

<p>The authors trained the Transformer LM on the Book Corpus dataset, and improved SOTA on 9 of the 12 tasks. While the results are indeed amazing, the analysis is not as extensive as that performed by Howard and Ruder, probably because the training required a month on 8 GPUs. This was even pointed out by <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>.</p>

<p><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Strong empirical results, but I wish there was more focus on a proper comparison / controlled experiments. <br><br>Is the improvement due to LSTM -&gt; Transformer or due to Wiki-1B (single sents) -&gt; BooksCorpus (longer context)? <a href="https://t.co/L3WrJW3z12">https://t.co/L3WrJW3z12</a></p>&mdash; (((λ()(λ() &#39;yoav)))) (@yoavgo) <a href="https://twitter.com/yoavgo/status/1006410113547108354?ref_src=twsrc%5Etfw">June 12, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<hr />

<h3 id="deep-contextualized-word-representations-https-arxiv-org-pdf-1802-05365-pdf"><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank">Deep Contextualized Word Representations</a></h3>

<p>The motivation for this paper, which won the Best Paper award at NAACL’18, is that word embeddings should incorporate both word-level characteristics as well as contextual semantics.</p>

<p>The solution is very simple: instead of taking just the final layer of a deep bi-LSTM language model as the word representation, <strong>obtain the vectors of each of the internal functional states of every layer, and combine them in a weighted fashion</strong> to get the final embeddings.</p>

<p>The intuition is that the higher level states of the bi-LSTM capture context, while the lower level captures syntax well. This is also shown empirically by comparing the performance of 1st layer and 2nd layer embeddings. While the 1st layer performs better on POS tagging, the 2nd layer achieves better accuracy for a word-sense disambiguation task.</p>

<p>For the initial representation, the authors chose to initialize with the embeddings obtained from a character CNN, so as to have character level morphological information incorporated in the embeddings. Finally for an $L$-layer bi-LSTM, $2L+1$ such vectors need to be combined to get the final representation, after performing some layer normalization.</p>

<p>In the empirical evalutation, the use of ELMo resulted in up to 25% relative increase in performance across several NLP tasks. Moreover, it improves sample efficiency considerably.</p>

<p>(Interestingly, a Google search revealed that this paper was first submitted at ICLR’18 but later withdrawn. I wonder why.)</p>

<hr />

<p>As Jeremy Howard says, transfer learning is indeed the Next Big Thing in NLP, and these trend-setting papers demonstrate why. I am sure we will see a lot of development in this area in the days to come.</p>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="https://desh2608.github.io/tags/natural-language-processing/">natural language processing</a>
  
  <a class="btn btn-primary btn-outline" href="https://desh2608.github.io/tags/transfer-learning/">transfer learning</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/project/irony-tweet/">Irony detection in tweets</a></li>
        
        <li><a href="/post/irony-detection-in-tweets/">Irony Detection in Tweets</a></li>
        
        <li><a href="/post/unsupervised-approaches-for-nmt/">Unsupervised Approaches for NMT</a></li>
        
        <li><a href="/post/trends-in-semantic-parsing-2/">Trends in Semantic Parsing - Part 2</a></li>
        
        <li><a href="/post/last-3-years-in-text-classification/">The Last 3 Years in Text Classification</a></li>
        
      </ul>
    </div>
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "desh2608-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//desh2608-github-io.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

