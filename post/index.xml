<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Desh Raj</title>
    <link>https://desh2608.github.io/post/</link>
    <description>Recent content in Posts on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Transfer Learning in NLP</title>
      <link>https://desh2608.github.io/post/transfer-learning-nlp/</link>
      <pubDate>Fri, 15 Jun 2018 13:42:18 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/transfer-learning-nlp/</guid>
      <description>Transfer learning is undoubtedly the new (well, relatively anyway) hot thing in deep learning right now. In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks. In NLP, though, transfer learning was mostly limited to the use of pretrained word embeddings (which, to be fair, improved baselines significantly).</description>
    </item>
    
    <item>
      <title>Evaluation Methods for Dialog Systems</title>
      <link>https://desh2608.github.io/post/evaluation-methods-dialog-systems/</link>
      <pubDate>Wed, 06 Jun 2018 13:42:04 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/evaluation-methods-dialog-systems/</guid>
      <description>Spoken Dialog Systems (SDS) have become very popular recently, especially for goal completion tasks on mobile devices. Also, with the increasing use of IoT devices and their associated assistants like Alexa, Google Home, etc., systems that can converse with users in natural language are set to be the primary mode of human-computer interaction in the coming years.
Early SDSs used to be extremely modular, with components such as automatic speech recognition, natural language understanding, dialog management, response generation, and speech synthesis, each trained separately and then combined.</description>
    </item>
    
    <item>
      <title>The 8 Commandments for Coding Your Research</title>
      <link>https://desh2608.github.io/post/8-commandments-for-coding-research/</link>
      <pubDate>Wed, 23 May 2018 13:41:50 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/8-commandments-for-coding-research/</guid>
      <description>This article is in a different flavor from the other posts in this publication. This is because I have been reading Roman Vershynin’s &amp;ldquo;High Dimensional Probability&amp;rdquo; for the last few days, and between that and visa formalities, I didn’t get a chance to check out new papers. I do plan to write an article on new methods for object detection (such as RCNN, Faster RCNN, and YOLO) sometime next month.</description>
    </item>
    
    <item>
      <title>An Introduction to Speech Recognition using WFSTs</title>
      <link>https://desh2608.github.io/post/intro-speech-recognition-wfst/</link>
      <pubDate>Mon, 23 Apr 2018 13:41:31 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-speech-recognition-wfst/</guid>
      <description>Until now, all of my blog posts have been about deep learning methods or their application to NLP. Since the last couple of weeks, however, I have started learning about Automatic Speech Recognition (ASR)1. Therefore, I will also include speech-related articles in this publication now.
The ASR logic is very simple (it’s just Bayes rule, like most other things in machine learning). Essentially, given a speech waveform, the objective is to transcribe it, i.</description>
    </item>
    
    <item>
      <title>How to Obtain Sentence Vectors</title>
      <link>https://desh2608.github.io/post/how-to-obtain-sentence-vectors/</link>
      <pubDate>Thu, 12 Apr 2018 13:41:14 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/how-to-obtain-sentence-vectors/</guid>
      <description>In several of my previous posts, I have discussed methods for obtaining word embeddings, such as SVD, word2vec, or GloVe. In this post, I will abstract a level higher and talk about 4 different methods that have been proposed to get embeddings for sentences.
But first, some of you may ask why do we even need a different method for obtaining sentence vectors. Since sentences are essentially made up of words, it may be reasonable to argue that simply taking the sum or the average of the constituent word vectors should give a decent sentence representation.</description>
    </item>
    
    <item>
      <title>Online Learning of Word Embeddings</title>
      <link>https://desh2608.github.io/post/online-learning-word-embeddings/</link>
      <pubDate>Wed, 14 Mar 2018 13:40:57 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/online-learning-word-embeddings/</guid>
      <description>Word vectors have become the building blocks for all natural language processing systems. I have earlier written an overview of popular algorithms for learning word embeddings here. One limitation with all these methods (namely SVD, skip-gram, and GloVe) is that they are all “batch” techniques. In this post, I will discuss two recent papers (which are very similar but were developed independently) which aim to provide an online approximation for the skip-gram algorithm.</description>
    </item>
    
    <item>
      <title>Sparsity in Online Learning with Lasso Regularization</title>
      <link>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</link>
      <pubDate>Sat, 24 Feb 2018 13:40:42 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/sparse-online-learning-lasso-regularization/</guid>
      <description>Sparse vectors have become popular recently for 2 reasons:
 Sparse matrices require much less storage since they can be stored using various space-saving methods. Sparse vectors are much more interpretable than dense vectors. For instance, the non-zero non-negative components of a sparse word vector may be taken to denote the weights for certain features. In contrast, there is no interpretation for a value like $-0.1347$.  Sparsity is often induced through the use of L1 (or Lasso) regularization.</description>
    </item>
    
    <item>
      <title>A Short Note on Stochastic Gradient Descent Algorithms</title>
      <link>https://desh2608.github.io/post/short-note-sgd-algorithms/</link>
      <pubDate>Thu, 08 Feb 2018 13:40:25 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/short-note-sgd-algorithms/</guid>
      <description>I just finished reading Sebastian Ruder’s amazing article providing an overview of the most popular algorithms used for optimizing gradient descent. Here I’ll make very short notes on them primarily for purposes of recall.
Momentum The update vector consists of another term which has the previous update vector (weighted by $\gamma$). This helps it to move faster downhill — like a ball.
$$ v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta) $$</description>
    </item>
    
    <item>
      <title>Irony Detection in Tweets</title>
      <link>https://desh2608.github.io/post/irony-detection-in-tweets/</link>
      <pubDate>Wed, 07 Feb 2018 13:40:06 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/irony-detection-in-tweets/</guid>
      <description>There was a SemEval 2018 Shared Task on “irony detection in tweets” that ended recently. As a fun personal project, I thought of giving it a shot, just to implement some new ideas. In this post, I will describe my approach for the problem along with some code.
Problem description The task itself was divided into two subtasks:
 Task A: Binary classification. Given a tweet, detect whether it has irony or not.</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 2</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-2/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:45 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-2/</guid>
      <description>In the first part of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used — Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.</description>
    </item>
    
    <item>
      <title>Introduction to Learning Theory - Part 1</title>
      <link>https://desh2608.github.io/post/intro-learning-theory-1/</link>
      <pubDate>Mon, 15 Jan 2018 13:39:43 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/intro-learning-theory-1/</guid>
      <description>One of the most significant take-aways from NIPS 2017 was the &amp;ldquo;alchemy&amp;rdquo; debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.
One of the most important concepts in this regard is to measure the complexity of a hypothesis class $H$. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set.</description>
    </item>
    
    <item>
      <title>Unsupervised Approaches for NMT</title>
      <link>https://desh2608.github.io/post/unsupervised-approaches-for-nmt/</link>
      <pubDate>Thu, 14 Dec 2017 13:39:30 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/unsupervised-approaches-for-nmt/</guid>
      <description>Translation is one of those tasks in language where the arrival of deep learning systems, and in particular sequence-to-sequence, has been something like a boon. In less than 4 years since the first paper on Neural Machine Translation, software giants such as Google and Microsoft have already announced that their translation systems have almost completely shifted from statistical to neural. Gone are the days when researchers mulled over complex word and phrase alignment techniques, and yet fell short on several language combinations.</description>
    </item>
    
    <item>
      <title>Beyond Euclidean Embeddings</title>
      <link>https://desh2608.github.io/post/beyond-euclidean-embeddings/</link>
      <pubDate>Wed, 06 Dec 2017 13:39:15 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/beyond-euclidean-embeddings/</guid>
      <description>Representation learning, as the name suggests, seeks to learn representations for structures such as images, videos, words, sentencences, graphs, etc., which may then be used for several objectives. Arguably the most important representations used nowadays are word embeddings, usually learnt using the distributional semantics methods such as skip-gram or GloVe. I have previously written about these methods here.
Two assumptions are inherent while using these methods to learn word vectors:</description>
    </item>
    
    <item>
      <title>Deep Learning for Multimodal Systems</title>
      <link>https://desh2608.github.io/post/deep-learning-multimodal-systems/</link>
      <pubDate>Thu, 09 Nov 2017 13:38:58 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-multimodal-systems/</guid>
      <description>When I was browsing through research groups for my grad school applications, I came across some interesting applications of new deep learning methods in a multimodal setting. ‘Multimodal,’ as the name suggests, refers to any system involving two or more modes of input or output. For example, an image captioning system provides images as input and expects a textual output. Similarly, speech-to-text, descriptive art, video summarization, etc., are all examples of multimodal objectives.</description>
    </item>
    
    <item>
      <title>Trends in Semantic Parsing - Part 2</title>
      <link>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</link>
      <pubDate>Wed, 08 Nov 2017 13:38:39 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</guid>
      <description>In Part 1 of this two-part series, I discussed some supervised approaches for the objective. In this part, we will look at some unsupervised or semi-supervised approaches, namely a Bayesian model, and transfer learning.
An unsupervised Bayesian model This paper was published in ACL 20111, back when statistical methods were still being used for NLP tasks. But with the recent forays into generative models, I feel it has again become relevant to understand how such methods worked.</description>
    </item>
    
    <item>
      <title>The Best Papers at ICLR 2017</title>
      <link>https://desh2608.github.io/post/best-papers-at-iclr-17/</link>
      <pubDate>Sun, 15 Oct 2017 13:38:17 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/best-papers-at-iclr-17/</guid>
      <description>The International Conference on Learning Representations (ICLR) has evolved into the deep learning conference over the last few years, and with its open review system, it is not difficult to understand why. I was recently going through some of the papers accepted at this year’s ICLR, especially the 3 that were awarded the Best Paper award. In this article, I will try to summarize these 3 papers in simple words, and hopefully get an idea about what’s hot in deep learning.</description>
    </item>
    
    <item>
      <title>The Last 3 Years in Text Classification</title>
      <link>https://desh2608.github.io/post/last-3-years-in-text-classification/</link>
      <pubDate>Mon, 02 Oct 2017 12:49:14 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/last-3-years-in-text-classification/</guid>
      <description>While working on my undergrad thesis on relation classification of biomedical text using deep learning methods, I quickly hacked together models in Tensorflow that combined convolutional and recurrent layers in various combinations. While some of these “network architectures” worked superbly (even surpassing state-of-the-art results), I had no clue what was happening inside the model. To gain such an intuition, I read about 20 recent papers on text classification (starting with the first “CNN for sentence classification” paper by Yoon Kim) over the course of a week.</description>
    </item>
    
    <item>
      <title>Understanding Word Vectors</title>
      <link>https://desh2608.github.io/post/understanding-word-vectors/</link>
      <pubDate>Fri, 29 Sep 2017 11:12:55 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/understanding-word-vectors/</guid>
      <description>This article is a formal representation of my understanding of vector semantics, from course notes and reading reference papers and chapters from Jurafsky’s SLP book. I will be talking about sparse and dense vector semantics, including SVD, skip-gram, and GloVe. In many places, I will try to explain the ideas in language rather than equations (but I’ll provide links to derivations and stuff wherever it is absolutely essential, which is actually everywhere!</description>
    </item>
    
    <item>
      <title>Trends in Semantic Parsing - Part 1</title>
      <link>https://desh2608.github.io/post/trends-in-semantic-parsing-1/</link>
      <pubDate>Wed, 20 Sep 2017 10:03:22 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/trends-in-semantic-parsing-1/</guid>
      <description>In this article, I will try to round up some (mostly neural) approaches for semantic parsing and semantic role labeling (SRL). This is not an extensive review of these methods, but just a collection of my notes on reading some recent research on the subject. However, I do believe it covers most of the latest trends as well as their limitations.
But first, what is semantic parsing?
“Semantic” refers to meaning, and “parsing” means resolving a sentence into its component parts.</description>
    </item>
    
    <item>
      <title>Metrics for NLG Evaluation</title>
      <link>https://desh2608.github.io/post/metrics-for-nlg-evaluation/</link>
      <pubDate>Sat, 16 Sep 2017 09:15:44 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/metrics-for-nlg-evaluation/</guid>
      <description>Simple natural language processing tasks such as sentiment analysis, or even more complex ones like semantic parsing are easy to evaluate since the evaluation simply requires label matching. As such, metrics like F-score (which is the harmonic mean of precision and recall), or even accuracy in uniformly distributed data, are used for such tasks.
Evaluating natural language generation systems is a much more complex task, however. And for this reason, a number of different metrics have been proposed for tasks such as machine translation or summarization.</description>
    </item>
    
  </channel>
</rss>