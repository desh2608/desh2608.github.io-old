<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Desh Raj</title>
    <link>https://desh2608.github.io/post/</link>
    <description>Recent content in Posts on Desh Raj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0530</lastBuildDate>
    
	<atom:link href="https://desh2608.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning for Multimodal Systems</title>
      <link>https://desh2608.github.io/post/deep-learning-multimodal-systems/</link>
      <pubDate>Thu, 09 Nov 2017 13:38:58 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/deep-learning-multimodal-systems/</guid>
      <description>When I was browsing through research groups for my grad school applications, I came across some interesting applications of new deep learning methods in a multimodal setting. ‘Multimodal,’ as the name suggests, refers to any system involving two or more modes of input or output. For example, an image captioning system provides images as input and expects a textual output. Similarly, speech-to-text, descriptive art, video summarization, etc., are all examples of multimodal objectives.</description>
    </item>
    
    <item>
      <title>Trends in Semantic Parsing - Part 2</title>
      <link>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</link>
      <pubDate>Wed, 08 Nov 2017 13:38:39 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/trends-in-semantic-parsing-2/</guid>
      <description>In Part 1 of this two-part series, I discussed some supervised approaches for the objective. In this part, we will look at some unsupervised or semi-supervised approaches, namely a Bayesian model, and transfer learning.
An unsupervised Bayesian model This paper was published in ACL 20111, back when statistical methods were still being used for NLP tasks. But with the recent forays into generative models, I feel it has again become relevant to understand how such methods worked.</description>
    </item>
    
    <item>
      <title>The Best Papers at ICLR 2017</title>
      <link>https://desh2608.github.io/post/best-papers-at-iclr-17/</link>
      <pubDate>Sun, 15 Oct 2017 13:38:17 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/best-papers-at-iclr-17/</guid>
      <description>The International Conference on Learning Representations (ICLR) has evolved into the deep learning conference over the last few years, and with its open review system, it is not difficult to understand why. I was recently going through some of the papers accepted at this year’s ICLR, especially the 3 that were awarded the Best Paper award. In this article, I will try to summarize these 3 papers in simple words, and hopefully get an idea about what’s hot in deep learning.</description>
    </item>
    
    <item>
      <title>The Last 3 Years in Text Classification</title>
      <link>https://desh2608.github.io/post/last-3-years-in-text-classification/</link>
      <pubDate>Mon, 02 Oct 2017 12:49:14 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/last-3-years-in-text-classification/</guid>
      <description>While working on my undergrad thesis on relation classification of biomedical text using deep learning methods, I quickly hacked together models in Tensorflow that combined convolutional and recurrent layers in various combinations. While some of these “network architectures” worked superbly (even surpassing state-of-the-art results), I had no clue what was happening inside the model. To gain such an intuition, I read about 20 recent papers on text classification (starting with the first “CNN for sentence classification” paper by Yoon Kim) over the course of a week.</description>
    </item>
    
    <item>
      <title>Understanding Word Vectors</title>
      <link>https://desh2608.github.io/post/understanding-word-vectors/</link>
      <pubDate>Fri, 29 Sep 2017 11:12:55 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/understanding-word-vectors/</guid>
      <description>This article is a formal representation of my understanding of vector semantics, from course notes and reading reference papers and chapters from Jurafsky’s SLP book. I will be talking about sparse and dense vector semantics, including SVD, skip-gram, and GloVe. In many places, I will try to explain the ideas in language rather than equations (but I’ll provide links to derivations and stuff wherever it is absolutely essential, which is actually everywhere!</description>
    </item>
    
    <item>
      <title>Trends in Semantic Parsing - Part 1</title>
      <link>https://desh2608.github.io/post/trends-in-semantic-parsing-1/</link>
      <pubDate>Wed, 20 Sep 2017 10:03:22 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/trends-in-semantic-parsing-1/</guid>
      <description>In this article, I will try to round up some (mostly neural) approaches for semantic parsing and semantic role labeling (SRL). This is not an extensive review of these methods, but just a collection of my notes on reading some recent research on the subject. However, I do believe it covers most of the latest trends as well as their limitations.
But first, what is semantic parsing?
“Semantic” refers to meaning, and “parsing” means resolving a sentence into its component parts.</description>
    </item>
    
    <item>
      <title>Metrics for NLG Evaluation</title>
      <link>https://desh2608.github.io/post/metrics-for-nlg-evaluation/</link>
      <pubDate>Sat, 16 Sep 2017 09:15:44 +0530</pubDate>
      
      <guid>https://desh2608.github.io/post/metrics-for-nlg-evaluation/</guid>
      <description>Simple natural language processing tasks such as sentiment analysis, or even more complex ones like semantic parsing are easy to evaluate since the evaluation simply requires label matching. As such, metrics like F-score (which is the harmonic mean of precision and recall), or even accuracy in uniformly distributed data, are used for such tasks.
Evaluating natural language generation systems is a much more complex task, however. And for this reason, a number of different metrics have been proposed for tasks such as machine translation or summarization.</description>
    </item>
    
  </channel>
</rss>